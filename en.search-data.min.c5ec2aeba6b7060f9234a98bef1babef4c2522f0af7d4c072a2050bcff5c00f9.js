'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/sap/abap/','title':"ABAP",'content':"ABAP\r#\r\rSe aglutina todo lo referente al lenguaje de programación de SAP.\nPublicaciones\r#\r\r\r\rBOPF\r\r\rBOPF\r\r\rSentencias \u0026gt;= 7.4\r\r\rNuevas sentencias aparecidas en la versión 7.4\r\r\rALV\r\r\rALV\r\r\rArchivelink\r\r\rArchivelink / GOS / BDS / Etc.\r\r\rBAPIs\r\r\rBAPIs\r\r\rCDS\r\r\rCore Data Services\r\r\rClases\r\r\rClases\r\r\rClases de desarrollo\r\r\rClases de desarrollo\r\r\rFunciones\r\r\rFunciones\r\r\rIDOC\r\r\rIDOC\r\r\rNotas de SAP\r\r\rNotas de SAP\r\r\rProgramas\r\r\rProgramas\r\r\rRecursos humanos\r\r\rRecursos humanos\r\r\rRTTS\r\r\rRuntime Type Services\r\r\rTransacciones\r\r\rTransacciones\r\r\rUtilidades generales\r\r\rUtilidades generales\r\r\r"});index.add({'id':1,'href':'/docs/sap/abap/cds/atajos/','title':"Atajos",'content':"Objetivo\r#\r\rDisponer de pequeños trozos de código que al no usarse mucho me cuestan saber como utilizarlo y siempre tengo que estar buscandolos.\nParámetros\r#\r\rdefine view ZATRON_CDS_DOMAIN_TEXTS with parameters p_domain :DOMNAME, Valores por defecto\r#\r\rEn este ejemplo se usa para pasar por defecto en un parámetro de entrada. Pero la misma variable variable de entorno se puede usar en un WHERE\n@Environment.systemField: #SYSTEM_DATE p_langu :abap.dats "});index.add({'id':2,'href':'/docs/sap/abap/bopf/','title':"BOPF",'content':"Objetivo\r#\r\rBOPF es un framework que permite modelar datos, definir validaciones, definir la persistencia en base de datos, realizar determinaciones, definir consultas, etc..\nEs un herramienta muy útil porque te ahorra toda la gestión que hay que hacer en los modelos de datos propios que se hacen en los proyectos.\nAquí iré recogiendo un poco de todo de lo que voy haciendo y/o aprendiendo sobre los BOPF.\nProyectos personales\r#\r\rEn el siguiente enlace están las utilidades que voy creando para los BOPF.\nPublicaciones\r#\r\r\r\rAcciones\r\r\rTodo lo referente a las acciones\r\r\rAPI desde ABAP\r\r\rComo usar la API de los BOPF desde ABAP\r\r\rCódigo ejemplo dentro del BOPF\r\r\rCódigo ejemplo dentro del BOPF\r\r\rDeterminaciones\r\r\rTodo lo referente a determinaciones\r\r\rNomenclaturas\r\r\rNomenclaturas\r\r\rTablas\r\r\rTablas\r\r\r"});index.add({'id':3,'href':'/docs/python/enlaces_interes/','title':"Enlaces de interes",'content':"Introducción\r#\r\rRecopilación de enlaces que me parece interesante.\nAprendizaje\r#\r\r En Youtube tenemos el canal de AMP Tech que tiene video de como programar en Python hasta como hacer cosas con Machine Learning  "});index.add({'id':4,'href':'/docs/github/enlaces_interesantes/','title':"Enlaces interesantes",'content':"Enlaces\r#\r\r \rComandos en Git \u0026ndash;\u0026gt; https://www.hostinger.es/tutoriales/comandos-de-git  "});index.add({'id':5,'href':'/docs/python/entorno_desarrollo/','title':"Entorno de desarrollo",'content':"Introducción\r#\r\rAquí hasta que no tenga más conocimientos voy ir anotando cosas que me sirven para preparar el entorno para desarrollo\nPython\r#\r\rEl interprete del lenguaje hay que descargarlo de la página oficial.\nIDE de desarrollo\r#\r\rUso el VS Code que tiene una ventaja cuando creas un programa con extensión \u0026ldquo;.py\u0026rdquo; (extensión que indica que es un lenguaje Python) te descarga el plugin para poder ejecutar directamente desde el VSCode.\nAunque en la documentación de VS Code explica como instalar los plugins necesarios.\nJupyter Notebooks\r#\r\rHay muchos cursos donde explican en lenguaje con Jupyter Notebooks. Esto es un proyecto opensource que permite ejecutar código Python como si fuese un lenguaje estructurado. La ayuda oficial de como usarlo dentro del VS Code es esta\nPara utilizarlo hay que pulsar CTRL+SHIFT+P para activar la ventana de comandos y escribir lo Create Blank:\n\rAparece el editor y ya lo podremos usar:\n\rPara probar lo que vamos escribiendo hay que poner en marcha el servidor de Jupyter notebook. Los pasos que he hecho han sido ir de nuevo la ventana de comandos y escribir jupyter server:\n\rHa preguntado si el servidor sería local o remote, he escogido remoto:\n\rLuego ha preguntado que interprete de Python:\n\ry a continuación ha comenzado hacer una instalación en local.\nNOTA: Como tengo instalado de antemano Anaconda, y al tener dos entornos configurados que a la postre son como tener dos interpretes de Python independientes, me ha salido la ventana a escoger el interprete: el oficial de Python y los datos de Anaconda.\nEl interprete puede ser cambiando abriendo la ventana de comandos:\n\rUna vez instalado todo en la parte superior derecha del VS Code se verá el servidor activo:\n\rCuando volvamos abrir de nuevo el VSCode y abrir un editor del Jupyter Notebook el servidor arrancará automáticamente.\n** AVISO **\nSi se esta usando Anaconda para la gestión de los paquetes es necesario tener instalado la libería ipykernel en el entorno seleccionado. En caso contrario dará un error que falta instalarlo.\nEn el ejemplo de arriba no se explica porque en el entorno de Anaconda de testeo que se creo se instalo la funcionalidad Jupyter Notebooks que lo instala de manera indirecta.\nAnaconda\r#\r\r\rAnaconda es un gestor de paquetes para poder utilizar los productos de Machine Learning como TensorFlow.\nYo me he instalado la versión 64-Bit Grapical Installer.\nUna vez instalado lo abriremos el menú de inicio de Windows y escribiremos Anaconda:\n\rUna vez arrancado sale la siguiente pantalla:\n\rEn la imagen se ve que estoy en un ambiente llamado test que he creado para hacer pruebas. Los ambientes, creo que se permiten hasta cinco, permites tener librerias distintas según el tipo de proyecto que se use.\nDesde este menu lanzo el VS Code. Cuando lo lance desde esta opción se me instalo, eso creo, la extensión Anaconda Extension Pack:\n\rEsta extensión usará la configuración de paquetes o librerias que tengas instalada en el ambiente que se tenga seleccionado.\nPero toda la parte de uso Anaconda lo iré poniendo en un artículo aparte.\n"});index.add({'id':6,'href':'/docs/python/sentencias/entrada_datos/','title':"Entrada de datos",'content':"Entrada de datos\r#\r\rPara pedir datos existe la sentencia *input\u0026rdquo;, esta sentencia en el VSCODE abre una ventana, como la de los comandos, donde te pide los datos.\nEjemplo:\nnombre = input(\u0026#34;¿dime tu nombre?\u0026#34;) print(nombre) Resultado:\nivan\r"});index.add({'id':7,'href':'/docs/python/extensiones/','title':"Extensiones",'content':"Introducción\r#\r\rExtensiones, sobretodo para VS Code, que son útiles para desarrollar\nPython-autopep8\r#\r\r¿Qué és?\r#\r\rExtensión que formatea en bonito y siguiendo best-practices el código que se desarrolla.\nInstalación\r#\r\rSe instala directamente desde las extensiones del visual code\nPylint\r#\r\r¿Qué és?\r#\r\r\rLibrería que verifica sintacticamente que el código sea correcto.\nInstalación\r#\r\rSe hace con Anaconda Power Shell en el entorno adecuado con lo siguiente:\nconda activate \u0026lt;entorno\u0026gt; pip install pylint Pylint\r#\r\r¿Qué és?\r#\r\r\rLibrería que verifica sintacticamente que el código sea correcto cuando se usa la librería Django\nInstalación\r#\r\rSe hace con Anaconda Power Shell en el entorno adecuado con lo siguiente:\nconda activate \u0026lt;entorno\u0026gt; pip install pylpylint-django Luego hay que ir a VSCode y pulsar CTRL+SHIFT+P para abrir el command palette y escribir Preferences: Configure Language Specific Settings. Saldrá un desplegable con el lenguaje a seleccionar, se escoge Python. Y en la configuración añadir el siguiente código:\n\u0026#34;python.linting.pylintArgs\u0026#34;: [ \u0026#34;--load-plugins=pylint_django\u0026#34;, ] "});index.add({'id':8,'href':'/docs/python/framework/','title':"Frameworks",'content':"ABAP\r#\r\rLos frameworks son librerías que nos permite realizar, de una manera simplifica, que de otra manera seria muy costoso de desarrollar.\nAquí se ira recopilando los que voy usando.\nPublicaciones\r#\r\r\r\rDjango\r\r\rDjando y librerías asociadas\r\r\r"});index.add({'id':9,'href':'/docs/github/generar_rsa_windows/','title':"Generar clave RSA",'content':"Motivo\r#\r\rEl motivo de crear una clave RSA es poder hacer deploy a Github mediante consola y evitar el siguiente error:\n\rDigo consola porque es el único momento que lo he necesitado, ya que usando VS Code o la propia aplicación de Github no lo he necesitado.\nPasos a seguir\r#\r\r Abrir el git Bash. Para eso hay que tener el instalado el Git para windos y al pulsar la tecla win y al escribir git aparecerá la opción:  \rEn el terminal que aparecerá escribir: ssh-key -t rsa -b 4096 -C \u0026ldquo;mail que nos hemos registrado en github\u0026rdquo; y al pulsar enter nos aparecerá esto:  \rPulsaremos Enter\nSe nos pedirá introducir una frase:\n\rLa frase se dejará en blanco para no tener que introducirla en todos los sitios que se haga push.\nAhora hay que añadir la clave generada al ssh-agent. Pero para eso primero hay que valir que este en marcha. Para ello hay que introducir: eval $(ssh-agent –s):  \rY con la siguiente instrucción se añade la clave al agente: ssh-add ~/.ssh/id_rsa nos pedirá la frase introducida en el paso 2 y si es correcta la añadirá.  \rAhora hay que obtener la key para poderla incluirla en nuestra cuenta github, para eso hay que escribir el siguiente comando:c cat ~/.ssh/id_rsa.pub  \rNota: Imagen cortada por seguridad. Todo lo que nos aparezca hay que copiarlo en un notepad para luego asociarlo a nuestra cuenta.\nAsociar clave a nuestra cuenta Github\r#\r\rAhora hay que ir al github y acceder a los Settings de nuestro usuario e ir a la opción SSH and GPG keys:\n\rY pulsar sobre el botón New SSH Key:\n\rEl en título se puede poner lo que sea, ejemplo el ordenador donde has generado la clave, y la clave generada es la rista que hemos obtenido del comando cat ~/.ssh/id_rsa.pub. Una vez introducida nos aparecerá lo siguiente:\n\rBibliografía\r#\r\rPara poder este artículo me he bsado en el siguiente artículo.\n"});index.add({'id':10,'href':'/docs/github/','title':"Github",'content':"Introducción\r#\r\rEn esta sección iré poniendo todo lo que vaya aprendido sobre Github.\nPublicaciones\r#\r\r\r\rEnlaces interesantes\r\r\rEnlaces interesantes sobre Github\r\r\rGenerar clave RSA\r\r\rGenerar clave RSA en Windows\r\r\r"});index.add({'id':11,'href':'/docs/hugo/','title':"Hugo",'content':"Introducción\r#\r\rEn esta sección iré poniendo todo lo que vaya aprendido sobre Hugo. Hugo para quien no lo sepa permite crear páginas web estáticas que se pueden subir a un repositorio de Github.\nPublicaciones\r#\r\r\r\rPublicar Web Github\r\r\rComo publicar una web en Github\r\r\r"});index.add({'id':12,'href':'/docs/python/machine_learning/librerias/','title':"Librerias y paquetes",'content':"Introducción\r#\r\rLibrerias y pawuetes para poder usar el Machine Learning.\nLos pasos para instalar las librerías esta explicado en la página de Anaconda.\nTensor Flow\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: TensorFlow es una biblioteca de código abierto para aprendizaje automático a través de un rango de tareas, y desarrollado por Google para satisfacer sus necesidades de sistemas capaces de construir y entrenar redes neuronales para detectar y descifrar patrones y correlaciones, análogos al aprendizaje y razonamiento usados por los humanos.\nInstalación\r#\r\rLa librería no esta instalada por defecto, lo más fácil es instalarla a través del anaconda navigator:\n\rComo es la librería que permite realizar más cosas con Machine Learning se ha instalado tanto versión CPU, como GPU. GPU porque las tarjetas gráficas tienen más procesadores lo que permite hacen calculos más rapidos que si solo se usará la CPU.\nEn la imagen hay librerías secundarías que se instalan al instalarse la principal.\nNumpy\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: NumPy es una extensión de Python, que le agrega mayor soporte para vectores y matrices, constituyendo una biblioteca de funciones matemáticas de alto nivel para operar con esos vectores o matrices.\nInstalación\r#\r\rNo se instala porque viene incluido con Anaconda:\n\rScikit learn(Sklearn)\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: Scikit-learn es una biblioteca para aprendizaje automático de software libre para el lenguaje de programación Python.​Incluye varios algoritmos de clasificación, regresión y análisis de grupos entre los cuales están máquinas de vectores de soporte, bosques aleatorios, Gradient boosting, K-means y DBSCAN.\nInstalación\r#\r\rPor defecto no se encuentra instalada. En la página que habla Anaconda en el apartado de instalación de paquetes mediante entorno gráfico explica como se hace la instalación.\nGraphviz\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: Graphviz es un conjunto de herramientas de software para el diseño de diagramas definido en el lenguaje descriptivo DOT.​ Fue desarrollado por AT\u0026amp;T Labs​ y liberado como software libre con licencie tipo Eclipse.​\nInstalación\r#\r\rLa instalación más sencilla es hacerlo a través de la interface gráfica de Anaconda:\n\rY el paquete:\n\rPara que funcione correctamente la libreria.\nMatplotlib\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: Matplotlib es una biblioteca para la generación de gráficos a partir de datos contenidos en listas o arrays en el lenguaje de programación Python y su extensión matemática NumPy. Proporciona una API, pylab, diseñada para recordar a la de MATLAB.\nInstalación\r#\r\rEsta libreria ya viene instalada por defecto con Anaconda y no es necesario realizar ningún instalación\nSVM (Support Vector Machine / Maquina de vectores de soporte)\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: Las máquinas de vectores de soporte o máquinas de vector soporte (Support Vector Machines, SVMs) son un conjunto de algoritmos de aprendizaje supervisado desarrollados por Vladimir Vapnik y su equipo en los laboratorios AT\u0026amp;T. Estos métodos están propiamente relacionados con problemas de clasificación y regresión. Dado un conjunto de ejemplos de entrenamiento (de muestras) podemos etiquetar las clases y entrenar una SVM para construir un modelo que prediga la clase de una nueva muestra.\nInstalación\r#\r\rViene incluida en la librería Scikit learn(Sklearn)\nNLTK\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: El kit de herramientas de lenguaje natural, o más comúnmente NLTK, es un conjunto de bibliotecas y programas para el procesamiento del lenguaje natural (PLN) simbólico y estadísticos para el lenguaje de programación Python.\nLo que hace el NLTK es eliminar lo que se llaman stop words. stop words son palabras mudas, o que no aportan valor para los algoritmos. Palabras como: en, la, el, una, etc. Son palabras que normalmente los algoritmos de búsqueda suelen ignorar.\nInstalación\r#\r\rLa librería no esta instalada por defecto, lo más fácil es instalarla a través del anaconda navigator:\n\rKMeans\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: Es un método de agrupamiento, que tiene como objetivo la partición de un conjunto de n observaciones en k grupos en el que cada observación pertenece al grupo cuyo valor medio es más cercano.\nKMens se utiliza en algoritmo de aprendizaje no supervisado.\nInstalación\r#\r\rViene incluida en la librería Scikit learn(Sklearn)\nPCA(Principal component Analysis)\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: En estadística, el análisis de componentes principales (en español ACP, en inglés, PCA) es una técnica utilizada para describir un conjunto de datos en términos de nuevas variables («componentes») no correlacionadas. Los componentes se ordenan por la cantidad de varianza original que describen, por lo que la técnica es útil para reducir la dimensionalidad de un conjunto de datos.\nEsta librería se usa para reducir ruido de los datos que se van a procesar y ver los datos de multiples dimension en dos dimensiones para poder analizarlos.\nInstalación\r#\r\rViene incluida en la librería Scikit learn(Sklearn)\nMglearn\r#\r\r¿Qué és?\r#\r\rRealmente no es una librería, es una paquete que tiene funciones de ayuda para el libro Introduction to Machine Learning with Python. En la siguiente dirección esta el detalle de dicho paquete\nInstalación\r#\r\rLa instalación hay que hacerla a través del paquete PIP, que ya viene instalado. Para hacerlo ha hacer lo siguiente:\nconda activate \u0026lt;nombre entorno\u0026gt; pip install mglearn Con esto ya se puede usar la librería o paquete.\nMglearn\r#\r\r¿Qué és?\r#\r\rExplicación extráido de Bioinformatics at COMAV\nPandas es un paquete de Python que proporciona estructuras de datos similares a los dataframes de R. Pandas depende de Numpy, la librería que añade un potente tipo matricial a Python. Los principales tipos de datos que pueden representarse con pandas son:\n Datos tabulares con columnas de tipo heterogéneo con etiquetas en columnas y filas. Series temporales.  Pandas proporciona herramientas que permiten:\n leer y escribir datos en diferentes formatos: CSV, Microsoft Excel, bases SQL y formato HDF5 seleccionar y filtrar de manera sencilla tablas de datos en función de posición, valor o etiquetas fusionar y unir datos transformar datos aplicando funciones tanto en global como por ventanas manipulación de series temporales hacer gráficas  En pandas existen tres tipos básicos de objetos todos ellos basados a su vez en Numpy:\n Series (listas, 1D), DataFrame (tablas, 2D) y Panels (tablas 3D).  Instalación\r#\r\rViene incluído en la librería Numpy\nMglearn\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: TensorFlow es una biblioteca de código abierto para aprendizaje automático a través de un rango de tareas, y desarrollado por Google para satisfacer sus necesidades de sistemas capaces de construir y entrenar redes neuronales para detectar y descifrar patrones y correlaciones, análogos al aprendizaje y razonamiento usados por los humanos.\nInstalación\r#\r\rTensor flow no viene instalado por defecto pero justo en el página de Anaconda en la sección de instalación de librerías, se explica como instalar Tensor Flow a modo de ejemplo.\nKeras\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: Keras es una biblioteca de Redes Neuronales de Código Abierto escrita en Python. Es capaz de ejecutarse sobre TensorFlow, Microsoft Cognitive Toolkit o Theano. Está especialmente diseñada para posibilitar la experimentación en más o menos poco tiempo con redes de Aprendizaje Profundo. Sus fuertes se centran en ser amigable para el usuario, modular y extensible.\nInstalación\r#\r\rLa librería no esta instalada por defecto, lo más fácil es instalarla a través del anaconda navigator:\n\rPylint\r#\r\r¿Qué és?\r#\r\rEs una libreria que analiza la calidad del código cuando se desarrolla programas en Python. Más información aquí\nInstalación\r#\r\rLa librería no esta instalada por defecto, lo más fácil es instalarla a través del anaconda navigator:\n\rPillow\r#\r\r¿Qué és?\r#\r\rEs una libreria que añade capacidad de procesamiento de imagenes\nInstalación\r#\r\rLa librería no esta instalada por defecto, lo más fácil es instalarla a través del anaconda navigator:\n\ripykernel\r#\r\r¿Qué és?\r#\r\rEs una libreria que usa VS Code para poder conectarse a los entornos de anaconda.\nInstalación\r#\r\rLa librería no esta instalada por defecto, lo más fácil es instalarla a través del anaconda navigator:\n\rOpenCV\r#\r\r¿Qué és?\r#\r\rSegún la Wikipedia: es una biblioteca libre de visión artificial originalmente desarrollada por Intel. OpenCV significa Open Computer Vision (Visión Artificial Abierta). Desde que apareció su primera versión alfa en el mes de enero de 1999, se ha utilizado en una gran cantidad de aplicaciones, y hasta 2020 se la sigue mencionando como la biblioteca más popular de visión artificial.1​ Detección de movimiento, reconocimiento de objetos, reconstrucción 3D a partir de imágenes, son sólo algunos ejemplos de aplicaciones de OpenCV.\nInstalación\r#\r\rLa librería no esta instalada por defecto, lo más fácil es instalarla a través del anaconda navigator:\n\rTesseract\r#\r\r¿Qué és?\r#\r\rNo es ni librería ni paquete, es un porgrama que se instala en practicamente cualquier sistema operativo y que permite reconocer texto en imagenes. Es un programa que esta bajo licencia Apache v2 con lo que permite ser usado por cualquiera libremente. Actualmente es uno de los mejores OCR open-source.\nInstalación\r#\r\rComo es un programa cuyas versiones van variando lo mejor es consultar dos sitios:\n Su página web en github Su repositorio en github  A día 11/08/2020 se ha instalado la versión estable 4.1. Aunque esta disponible la versión 5.0 en versión Alpha\nPytesseract\r#\r\r¿Qué és?\r#\r\rEs una librería que encapsula las llamadas a Tesseract simplificando su uso.\nInstalación\r#\r\rLa instalación se tiene que realizar a través del instalador de Python, PIP usando el powershell de Anaconda. Los comandos son:\nconda activate \u0026lt;entorno\u0026gt; pip install pytesseract pip3 install pytesseract Rope\r#\r\r¿Qué és?\r#\r\rEs una librería que permite realizar refactoring en el código\nInstalación\r#\r\rEn caso de intentar hacer un refactoring con VSCode sin tenerlo instalado el propio editor te pregunta si lo quieres instalar, haciendo el proceso de manera automática. Si se quiere hacer manualmente hay que hacerlo con el PiP\nconda activate \u0026lt;entorno\u0026gt; pip install rope "});index.add({'id':13,'href':'/docs/react/nwb/','title':"nwb",'content':"nwb\r#\r\rComo se ha dicho la introducción React necesita de librerías adicionales, ya que solo renderiza, para hacerlo un framework completo. Esta librerías cada una tiene su propia configuración, instalación, etc..\nLo que permite nwb es tener un entorno de desarrollo que tenga todas esos componentes, cuando se necesitan, y que no se tenga que configurar nada, hasta que sea necesario. De esta manera uno se centra más en el código que en la configuración.\nInstalación\r#\r\rPara que funcione hay tener instalado nodejs y npm. Este último se instala con nodejs\nLa instalación es muy simple hay que hacer:\nnpm install -g nwb Como me ha dado varios warning por versiones deprecated lo que he hecho después ha sido:\nnpm update -g Para actualizar las posibles librerías.\nPublicaciones\r#\r\r\r\r"});index.add({'id':14,'href':'/docs/hugo/publicar_web_github/','title':"Publicar Web Github",'content':"Motivo\r#\r\rPara aquellos que no quieren gastarse dinero en productos como Wordpress o no necesitan páginas dinámicas, la opción de crear una de manera gratuíta y usando Hugo es una buena elección.\nPersonalmente la clave es escoger un tema que se nos adapte a lo que necesitamos, porque cada tema tiene ciertos comportamientos que no tiene otros. Por ejemplo, inicialmente había escogido el tema npp-hugo, cuando empece a montar la estructura que quería vi que no era tan flexible en la organización por carpetas como me esperaba (al menos es lo que he visto probandolo). Por ello finalmente y después de revisar unos cuantos temas he optado por el tema book que se ajustaba más lo que buscaba.\nPor ello creo que para usar Hugo es básico escoger con calma el tema que se quiera ya que funcionalidad que tiene un tema no existen en otro. Por ejmeplo, hay páginas template, mardownsyntax.md del tema npp-hugo no son compatibles con el tema book que uso. En mi caso me he dado cuenta mientras hacía pruebas con este tema.\nAparte he visto que la documentación oficial parece echa para construir template porque realmente lo que me ha servido para construir la página han sido los ejemplos que vienen incluídos en la carpeta exampleSite que suele vernir en cada tema.\nPasos\r#\r\rPasos previo\r#\r\rAntes de iniciar el proceso se aconseja instalar la clave RSA en local de Github siguiendo los pasos de este artículo\nInstalar Hugo\r#\r\rHugo se puede descargar de la siguiente página web. Una vez instalado es aconsejable poner el directorio donde se ha instalado en el PATH de window para un mejor acceso:\n\rCrear los repositorios en el Github\r#\r\rEl sitio que se va a crear se tiene que sincronizar con dos repositorios que se han de crear previamente. Estos sitios son:\n  será el encargado de tener los ficheros del sitio que estamos construyendo. El que contendrá los ficheros .md .github.io será el encargado guardar las páginas renderizadas por Hugo en base al contenido en el sitio anterior. Es importante poner el nombre del usuario ya que si no, no funcionara. En mi caso el repositorio creado es: irodrigob.github.io  Crer el sitio en local\r#\r\rHay que abrir la consola de Windows e ir al directorio donde vamos crear el sitio web. Una vez en el directorio hay que escribir: hugo new site nombre_sitio:\n\rAñadir el tema\r#\r\rEn la página web de los temas se pueden encontrar multitud de temas para descargar. Para este sitio se ha utilizado el tema book\n NOTA: Tanto los pantallazos como en las explicación sale el tema npp-hugo, esto es debido a que use incialmente dicho tema para crear el sitio.*  Para instalarlo hay que hacerlo desde la consola hay que hacer los siguientes pasos:\n git init, Crea los archivos para poder sincronizar el sitio con Git git submodule add https://github.com/saadnpq/npq-hugo themes/npqq-hugo, Añade como submodulo el tema y lo descargará en el directorio themes/npqq-hugo Editar el fichero config.toml que esta en el directorio raíz del sitio y poner el contenido del mismo fichero, que hay en el directorio themes/npqq-hugo/example-config.toml. Una vez copiado se ajustarán los datos de configuración básicos.  En consola se irá viendo lo siguiente:\n\rCon esto ya se tiene un primer esbozo del sitio, para ver cómo queda en la consola hay que escribir: hugo server -D\nSi abrimos una navegador e introducimos la url: http://localhost:1313/ veremos como queda el sitio.\nSincronizar el sitio local con Gihub\r#\r\rEsta es la parte que mas me ha costado por no estar acostumbrado a trabajar con Git. Esta es la parte es la que he aprendido que hacer que hacerlo en consola, ya que aunque puedes hacer los push con la aplicación de escritorio de Github o con el VS Code(que es el que uso para crear las páginas) he descubierto, seguramente por desconocimiento, que no funciona igual y se jode de tal manera los archivos de Git que he tenido que borrar sitio y repositorios más de una vez. Ahora, todo lo hago con la consola y ya no tengo errores.\nPersonalmente creo que si los submodulos (solo hemos creado el primero de ellos, el del tema) se hubiesen sincronizado con el Github Desktop no hubiese habido problemas. Pero ahora mismo no tengo ganas de más experimentos.\nLos dos sitios del github los hemos creado en el paso anterior, aunque también es posible crearlo justo en este momento, pero así es como lo he hecho yo. Una vez creados hay que seguir los siguientes pasos para sincronizalos en local.\nDesde la consola de Windows y estando en el directorio del sitio hay que ejecutar los siguientes comandos git:\n git remote add origin git@github.com:irodrigob/navi-developer.git Añade el repositorio que guardará las páginas que vamos creando git add . Le decimos que queremos añdir todas las carpetas del sitio en el git git commit -m \u0026quot;Commit inicial del sitio\u0026quot; Guarda los ficheros añadidos en el paso anterior en el git local.  A nivel de consolo iremos viendo algo parecido a esto:\n\rAhora hay que añadir el repositorio donde se guardarán las páginas renderizadas por Hugo:\n git submodule add git@github.com:irodrigob/irodrigob.github.io.git En este sentencia aparecerá la pregunta de la frase de la clave RSA que hemos hecho previamente para poder subir a Github:  \rAunque en la imagen no se vé, hay una pregunta que pide poner: Yes/No/Phrase. Como no se ha puesto frase, para ser más comodo las subidas, hay que poner Yes para confirmar la clave y así no volverá a realizar la misma pregunta.\nUna vez añadido hay que hacer los siguientes pasos:\n git add . git commit -m \u0026quot;Commit inicial\u0026quot; git push –u origin master Este sube los archivos del repositorio local al remoto  A nivel de consola deberá aparecer algo parecido a esto:\n\rHasta ahora solo hemos subido datos del repositorio que hemos creado para el sitio. Ahora hay que cambiar la configuración para que Hugo publique las páginas en nuestro repositorio de github.io\nPublicando en github.io\r#\r\rEl archivo de configuración config.toml hay que realizar los siguientes cambios:\n Al parámetro baseURL se le tiene que poner nuestra dirección del repositorio creado con el nombre github.io Se añade el parámetro publisDir con el nombre de la carpeta que se ha creado al crear el submodulo del repositorio *github.io\u0026rdquo;. Este directorio es donde Hugo rendizara las páginas markdow a HTML  Así es como quedaría(Nota: Ejemplo del fichero que estoy usando con tema que estoy usando actualmente):\nbaseURL = \u0026quot;https://irodrigob.github.io/\u0026quot;\rlanguageCode = \u0026quot;es-es\u0026quot;\rtitle = \u0026quot;Navi-developer\u0026quot;\rtheme = \u0026quot;hugo-book\u0026quot;\rpublishDir = \u0026quot;irodrigob.github.io\u0026quot; Ahora de nuevo vía consola, y en el directorio del sitio, con el comando hugo se creará la página Web en el directorio del submodulo github.io\nAhora y antes de hacer el pull para subir los archivos al repositorio *github.io\u0026rdquo; vamos a comprobar que la configuración sea la correcta. Para ello de nuevo desde la consola se escribirá: git remote -v:\n\rSi el resultado es como el de la imagen es que todo es correcto.\nAhora para poder subir los archivos generados a nuestro repositorio github.io hay que ir al directorio de dicho repositorio y lanzar los siguientes comandos en la consola de windows:\ngit add .\rgit commit -m \u0026quot;Primera subida!!\u0026quot;\rgit push origin master\rEn consola aparecerá algo parecido a esto:\n\rY ahora si se accede la siguiente página: http://irodrigob.github.io/ se verá el contenido publicado.\nBibliografía\r#\r\rPara poder este artículo me he bsado en el siguiente artículo.\n"});index.add({'id':15,'href':'/docs/python/','title':"Python",'content':"Introducción\r#\r\rPython es un lenaguaje de programación muy usado para el machine learning. Aquí iré recogiendo todo lo que vaya aprendiendo.\nSecciones\r#\r\rLas sección de Python son las siguientes:\n\r\rEnlaces de interes\r\r\rEnlaces de interes\r\r\rEntorno de desarrollo\r\r\rEntorno de desarrollo\r\r\rExtensiones\r\r\rExtensiones VS Code\r\r\rFrameworks\r\r\rFrameworks\r\r\rAnaconda\r\r\rAnaconda\r\r\rConfiguración Pylint\r\r\rConfiguración Pylint\r\r\rMachine learning\r\r\rMachine learning\r\r\rSentencias\r\r\rSentencias\r\r\r"});index.add({'id':16,'href':'/docs/react/','title':"React",'content':"React\r#\r\r\rReact , o React JS/ReactJS, es un framework que permite la realización de interfaces de usuario para web, móvil, etc. Este framework solo se encagará de la renderización de las páginas web, pero para otras cosas necesarias requiere de librerías adicionales para completar su funcionalidad.\nPublicaciones\r#\r\r\r\rnwb\r\r\rnwb\r\r\r"});index.add({'id':17,'href':'/docs/sap/','title':"SAP",'content':"Introducción\r#\r\rEn esta sección será posiblemente la principal de la página ya que es el sistema que llevo desarrollando durante más de 20 años.\nSecciones\r#\r\rLas sección de SAP son las siguientes:\n\r\rABAP\r\r\rLenguaje de desarrollo ABAP\r\r\rBW\r\r\rBW\r\r\rGateway\r\r\rGateway\r\r\rUI5\r\r\rSAP UI5\r\r\rVentas\r\r\rVentas (SD)\r\r\r"});index.add({'id':18,'href':'/docs/sap/abap/sentencias_74/','title':"Sentencias \u003e= 7.4",'content':"Objetivo\r#\r\rA partir de la versión 7.4 de ABAP aparecerán nuevas sentencia, que se demoninan in-line, que permiten una mayor flexibilidad a la hora de desarrollar. Se demoninan in-line porque se puede combinar varias sentencias en una sola línea, incluso, se puede poner dichas sentencias a la hora de pasar parámetros en clases. Lamentablemente, las funciones no permiten dicha funcionalidad.\nHay que destacar que estas sentencias van mejorando en cada versión, en la versión 7.5 apareciendo nuevas funcionalidades.\nAunque tengo que reconocer que usar estas sentencias son muy guays y parece que sabes, muchas veces su legibilidad queda en entredicho. Y hay veces que hay que usar un mix de sentencias de \u0026ldquo;siempre\u0026rdquo; son más legibles tanto para uno propio, como para la persona que lo va a mantener.\nIré creando páginas para ir clasificando los distintos tipo de sentencias.\nPublicaciones\r#\r\r\r\rBucles\r\r\rSentencias sobre proceso de tablas internas\r\r\rCondiciones\r\r\rSentencias sobre condiciones\r\r\rConversiones\r\r\rSentencias sobre conversión de datos\r\r\rFiltros\r\r\rSentencias sobre el filtrado de datos\r\r\rLectura tablas del diccionario\r\r\rFormas nuevas de leer en las tablas del diccionario\r\r\rMapeo de campos\r\r\rSentencias sobre mapeo de campos\r\r\rRelleno de valores\r\r\rSentencias para informar tablas internas\r\r\rString template\r\r\rSentencias sobre el tratamiento de string\r\r\r"});index.add({'id':19,'href':'/docs/python/sentencias/variables/','title':"Variables",'content':"Variables\r#\r\rExisten tres tipos de variables, al menos por lo que he visto que son: string, enteras y flotantes.\n# Esta es integer a=23 # Esta es float b=23.456 # Esta es string c=\u0026#34;hola mundo\u0026#34; Para saber el tipo de variable en cualquier momento se puede usar la siguiente sentencia:\ntype(c) Convertir variables\r#\r\rSi queremos convertir el formato de una variable a otra. Como hay tres tipos de \u0026ldquo;tipos\u0026rdquo;, hay tres funciones que lo hace: INT, FLOAT, STR\nTenemos este ejemplo:\nh=\u0026#34;12\u0026#34; print(\u0026#34;Suma: \u0026#34;,a+h) Al ejecutarlo nos dará esto:\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\rPero si hacemos cambiamos el código:\nh=\u0026#34;12\u0026#34; print(\u0026#34;Suma: \u0026#34;,a+int(h)) Al ejecutarlo nos dará esto:\nSuma: 24\rNo hay que decir que para poderlo el ejemplo anterior en la variable h tiene que haber un número. Si tuviese el valor 12r daría un error porque hay un carácter que no es númerico.\nBorrar contenido de variables\r#\r\rPara borrar el contenido de una variable se usa la sentencia DEL ejemplo:\ndel KNN "});index.add({'id':20,'href':'/docs/sap/abap/bopf/acciones/','title':"Acciones",'content':"Objetivo\r#\r\rLas acciones se usan para llamar a procesos externos del BOPF. Por ejemplo yo los uso para crear un pedido, lanzar otro programa, etc..\nYo suelo usar una clase general para agrupar acciones de un mismo nodo. En algunos artículos y el propio BOPF aconseja hacer una clase por acción. Pero lo veo una manera de generar clases que a lo mejor solo tiene cuatro líneas de código. En mi caso con una sola clase he tenido suficiente, ya que muchas veces según la complejidad tengo una clase que gestiona dicho proceso.\nCase para ir llamando a las determinaciones\r#\r\rPongo el case siguiente en el método principal que se llama en la acción. Y a partir de ese case voy llamando a métodos distintos.\nCASE is_ctx-act_key. WHEN zif_sat_orders_c=\u0026gt;sc_action-\u0026lt;nodo\u0026gt;-\u0026lt;nombre determinacion\u0026gt; ENCASE Parámetros de un método de la deteminación\r#\r\rEl método de la determinación siempre tiene los mismo parámetros que el método estándar, cuya firma es la siguiente:\nIMPORTING !is_ctx TYPE /bobf/s_frw_ctx_act !it_key TYPE /bobf/t_frw_key !io_read TYPE REF TO /bobf/if_frw_read !io_modify TYPE REF TO /bobf/if_frw_modify !is_parameters TYPE REF TO data EXPORTING !eo_message TYPE REF TO /bobf/if_frw_message !et_failed_key TYPE /bobf/t_frw_key !ev_static_action_failed TYPE abap_bool !et_data TYPE INDEX TABLE RAISING /bobf/cx_frw . "});index.add({'id':21,'href':'/docs/python/machine_learning/algoritmo_aprendizaje_no_supervisado/','title':"Algoritmos de aprendizaje no supervisado",'content':"Introducción\r#\r\rHasta ahora los ejemplos usando el set de datos de sklearn era con algoritmo de aprendizaje supervisado, es decir, los set de datos tienen características y etiquetas y nos devuelve una predicción. El caso del set de datos de iris tenemos que las características son las mediciones del petalo y cepalo de una planta y las etiquetas a que especie pertenecen.\nPero no siempre vamos a tener datos tan bien construidos, en la mayoria de casos solo se tendrá las características. Para este caso lo que hay que utilizar son algoritmo de aprendizaje no supervisado. Estos algoritmo se especializan en que no necesitan las etiquetas para poder aprender. En el caso del set de datosd e iris solo se le pasarían las mediciones del petalo y cepalo, y con ello que encuentre un patron para poder clasificar bien las flores.\nUna librería que nos permite usar este tipo de algoritmos es KMeans. Esta librería permite agrupar los datos en grupos, también llamado K, según las características que tiene.\nComo funciona el algoritmo\r#\r\rLa explicación que se habrá aquí pertenece al video Machine Learning episodio 8. KMeans. Se pondrán imagens obtenidas del video para explicar mejor el funcionamiento tal como lo hace en el video.\nImaginemos que tenemos los siguientes datos\n\rEn la imagen queda claro que hay dos grupos diferencias pero esto el algoritmo no lo sabe. Entonces al algoritmo hay que indicarle cuantos grupos o cluster queremos que agrupe los datos. En este caso serán 2 o como se ve en las explicaciones: K=2.\nEntonces lo que hace el algoritmo es crear un puntos aleatorios llamados centroides:\n\rEntonces lo que hace el algorimto es ver que puntos están mas cercanos a los centroides. Con ello sacará el promedio de los puntos que están más cercanos a los centroides, en la siguiente iteracción se va a mover el centroide al promedio que saco en el paso anterior.\nEn la siguiente imagen se mostraría que datos estarían más cercanos para cada centroide.\n\rAhora con el promedio obtenido reajuste los centroides:\n\rEstas iteracciones las va ir repitiendo hasta que los centroides no se muevan o se llege al final de las iteracciones. Lo ideal sería que una ha terminado todas las iteracciones los centroides quedarán en las siguientes posiciones:\n\rCon lo cual los grupos se crearían de la siguiente manera:\n\rComo los datos no siempre van estar en esta posición ideal hay que tener los datos estén correctos y hacer muchas pruebas de ensaño y error para ir ajustando el número de grupos a clasificar.\n\rSecciones\r#\r\rLas secciones son las siguientes:\n\r\rEjemplo KMeans\r\r\rEjemplo Kmeans\r\r\r"});index.add({'id':22,'href':'/docs/sap/abap/alv/','title':"ALV",'content':"Objetivo\r#\r\rCajón de desastre de cosas interesante de los ALV. No descarto que alguna cosa este duplicada en alguna otra sección como clases o funciones.\nProyectos personales\r#\r\rEn el siguiente enlace hay una clase que permite simplificar la creación de ALV. Esta clase encapsula las clases de los SALV.\nMostrar un ALV como si fuera una dynpro pero sin dynpro\r#\r\r CL_GUI_CONTAINER=\u0026gt;SCREEN0 \u0026ndash;\u0026gt; Permite sacar un ALV como si fuera dynpro si tener una o configurar container. En los nuevos SALV hay que poner después del display un WRITE: \u0026lsquo;X\u0026rsquo;.  Recuperar contenido mostrado por un ALV\r#\r\rcl_salv_bs_runtime_info=\u0026gt;set( EXPORTING display = abap_false metadata = abap_true data = abap_true ). Para evitar que un ALV se muestre y asi poder recoger su catalogo,filtros, etc.. . Eso si, el truco del SCREEN0 provoca que no funcione el DISPLAY con este truco\n"});index.add({'id':23,'href':'/docs/python/anaconda/','title':"Anaconda",'content':"Introducción\r#\r\rTal como se ha explicado en la página de entorno de desarrollo, Anaconda permite la instalación de paquetes para usar el machine learning en Python o cualquier otro paquete para proyectos en Pytjon..\nEntornos\r#\r\rLos entornos, creo que hasta tres entornos se pueden usar, permiten tener instalaciones separadas de paquetes según lo que se quiera hacer.\nPara hacer pruebas me he creado un ambiente llamado test que lo he creado con el Anaconda navigator yendo a la pestaña de Enviroments:\n\rY luego pulsando el botón de Create que es la parte inferior de la lista de ambientes.\n\rTe pide los datos del entorno :\n\rY al pulsar el botón de Create comenzar a instalar los paquetes necesarios y algunas librerias.\nActivar entorno\r#\r\rPor defecto esta el entorno base activado. Para cambiar de entorno se puede hacer de dos maneras.\n Es mediante el Anaconda Navigator, ir a la pestaña de Environments y seleccionar el entorno:  \rTarda unos segundos en activarse.\nA través de la consola. Abrir el menu de inicio de Windows, escribir Anaconda y seleccionar el modo consola:  \rY escribir:\nconda activate \u0026lt;nombre entorno\u0026gt; Como en la imagen:\n\rA la derecha de la consola nos pone el entorno por defecto. Al escribir y pulsar Enter\n\rY veremos que a la derecha sale el entorno seleccionado.\nInstalar libreria\r#\r\rVía consola\r#\r\rLa libería de TensorFlow no viene instalado. Vamos a instalar en el entorno de test y vía consola.\nconda installa tensorflow Sale información sobre lo que se va a instalr_\n\rImportante en la imagen se ve que va instalar TensorFlow en el entorno test, el activo. Si tenemos otro entorno que lo queramos instalar hay que activar dicho entorno e instalarlo.\nSe le pulsa que y y comenzará a realizar la instalación. Nos irá diciendo del progreso de instalación pero no tarda mucho. Con eso ya se puede usar el TensorFlow.\nVía entorno gráfico\r#\r\rVamos a instalar la librería Scikit lear(sklearn) para ello primero hay activar el entorno donde la queremos, ir a la pestaña de Environments, seleccionar los paquetes no instaldos y filtrar por scikit:\n\rEn la imagen ya se ha marcado el paquete que nos interesa y pulsar el botón de Apply situado en la parte inferior derecha. NOTA: Al marca los tres paquetes de scifi no hay manera que se instalen por eso solo marco uno\n\rSi hay dependencias con otros paquetes nos pedirá que confirmemos los otros paquetes:\n\rSe pulsar el botón Apply para iniciar la instalación. Una vez instalado el paquete desaparecerá de la lista de paquetes no instalados.\nTambién he instalado el paquete scikit-image. El otro paquete asociado scikit-rf da error al instalarlo.\n"});index.add({'id':24,'href':'/docs/sap/abap/bopf/api_abap/','title':"API desde ABAP",'content':"Objetivo\r#\r\rLa API de acceso al ABAP es siempre la misma por eso voy a poner los ejemplos de cada tipo de operación que se quiera hacer.\nYo para estos casos me suelo crear un helper para simplificar las llamadas entre las clases del proceso y del BOPF.\nInstanciar variables para acceder al BOPF\r#\r\rLo primero que se suele hacer es instanciar las variables necesarias para operar con los BOPF.\nDATA mo_svc_mngr TYPE REF TO /bobf/if_tra_service_manager. DATA mo_txn_mngr TYPE REF TO /bobf/if_tra_transaction_mgr. DATA mo_conf_mngr TYPE REF TO /bobf/if_frw_configuration. TRY. \u0026#34; Inicialización del gestor transaccional actualizaciones, bloqueos, etc.. mo_txn_mngr = /bobf/cl_tra_trans_mgr_factory=\u0026gt;get_transaction_manager( ). \u0026#34; Creación del el gestor de servicios del BOPF. Permite realizar las llamadas al BOPF para ejecutar validaciones, acciones, añadir, etc.. \u0026#34; Es la clase más importante ya que toda la gestión CRUD se realiza en esta clase mo_svc_mngr = /bobf/cl_tra_serv_mgr_factory=\u0026gt;get_service_manager( iv_bo_key ). \u0026#34; Creación de la configuración del BOPF, permite obtener los metadas del BOPF mo_conf_mngr = /bobf/cl_frw_factory=\u0026gt;get_configuration( iv_bo_key ). CATCH /bobf/cx_frw. \u0026#34;TODO: Error handling... ENDTRY. Añadir registro\r#\r\rCuando se añaden hay que tener en cuenta si se insertados datos dependiente de un nodo superior, o no. En un BOPF todos los nodos dependen del ROOT, al menos es lo que he visto, por lo tanto añadir en el nodo ROOT es distinto que en un nodo \u0026ldquo;hijo\u0026rdquo;. Se muestra los ejemplos en ambos\nNodo root\r#\r\rA la tabla donde se guarda los datos que se van a enviar siempre se guarda la estructura de datos. Esa estructura es siempre la combinada, la que tiene los datos persistentes y transitorios. El motivo es que esa estructura tiene el campo donde le indicaremos la clave del registro.\nDATA lt_mod TYPE /bobf/t_frw_modification. DATA(lo_header) = NEW zcar_bo_sc_header( ). ASSIGN lo_header-\u0026gt;* TO FIELD-SYMBOL(\u0026lt;ls_header\u0026gt;). \u0026lt;ls_header\u0026gt; = CORRESPONDING #( is_header ). \u0026lt;ls_header\u0026gt;-vbeln = iv_vbeln. \u0026lt;ls_header\u0026gt;-key = /bobf/cl_frw_factory=\u0026gt;get_new_key( ). INSERT VALUE #( node = zif_car_bo_orders_c=\u0026gt;sc_node-root change_mode = /bobf/if_frw_c=\u0026gt;sc_modify_create key = lo_header-\u0026gt;key data = lo_header ) INTO TABLE lt_mod. Subnodo del ROOT\r#\r\rEste ejemplo es cuando se añaden datos que dependen del root, además es un buen ejemplo para ver como se añaden datos de una tabla interna. Aquí de nuevo se usa la estructura combinada, que se instancia por cada nuevo registro. Si no lo hicieramos insertaríamos siempre el último registro debido a los punteros de memoria.\nLOOP AT it_positions ASSIGNING FIELD-SYMBOL(\u0026lt;ls_positions\u0026gt;). DATA(lv_tabix) = sy-tabix. DATA(lo_positions) = NEW zcar_bo_sc_positions( ). ASSIGN lo_positions-\u0026gt;* TO FIELD-SYMBOL(\u0026lt;ls_bo_positions\u0026gt;). \u0026lt;ls_bo_positions\u0026gt; = CORRESPONDING #( \u0026lt;ls_positions\u0026gt; ). \u0026lt;ls_bo_positions\u0026gt;-posnr = lv_tabix. \u0026lt;ls_bo_positions\u0026gt;-key = /bobf/cl_frw_factory=\u0026gt;get_new_key( ). INSERT VALUE #( node = zif_car_bo_orders_c=\u0026gt;sc_node-positions change_mode = /bobf/if_frw_c=\u0026gt;sc_modify_create key = \u0026lt;ls_bo_positions\u0026gt;-key data = lo_positions source_node = zif_car_bo_orders_c=\u0026gt;sc_node-root association = zif_car_bo_orders_c=\u0026gt;sc_association-root-positions source_key = lo_header-\u0026gt;key ) INTO TABLE lt_mod. ENDLOOP. Modificar registro\r#\r\rEste ejemplo es como se haría en el nodo ROOT pero se puede extrapolar como se haría con el ejemplo de añadir en un nodo dependiente del ROOT.\nCuando se modificar siempre hay que leer los datos previamente a través de la querys del BOPF. Una vez leído los datos se puede modificar.\nINSERT VALUE #( node = zif_sat_orders_c=\u0026gt;sc_node-root change_mode = /bobf/if_frw_c=\u0026gt;sc_modify_update key = lo_header-\u0026gt;key data = lo_header ) INTO TABLE et_mod. Modificar datos en BOPF\r#\r\rPara que los datos del BOPF se graben primero se lanza el proceso de modificación, en este proceso es donde saltarían las validaciones propias.\nEn la clase de utilidades del BOPF que se esta indicada en el índice de la sección todo este proceso esta encapsulado para simplificar la operativa.\nmo_svc_mngr-\u0026gt;modify( EXPORTING it_modification = it_mod IMPORTING eo_change = DATA(lo_change) eo_message = eo_message ). Grabar datos en BOPF\r#\r\rUna vez la modificación no devuelve errores es cuando se pueden grabar.\nEn la clase de utilidades del BOPF que se esta indicada en el índice de la sección todo este proceso esta encapsulado para simplificar la operativa.\nmo_txn_mngr-\u0026gt;save( IMPORTING ev_rejected = ev_rejected eo_message = eo_message ). Consulta de datos\r#\r\rCuando se tiene la clave del registro del nodo se tiene que usar el método retrieve o retrieve_by_association.\nCuando se tiene el ID del registro\r#\r\rLT_HEADER es la tabla interna que se indica en Combinated table type\nmo_svc_mngr-\u0026gt;retrieve( EXPORTING iv_node_key = zif_atron_file_engine_c=\u0026gt;sc_node-root it_key = VALUE #( ( key = iv_id_file ) ) iv_fill_data = abap_false IMPORTING et_data = lt_header ). Los datos de un subnodo\r#\r\rEn node_key se informa el nodo donde pertenece la clave que se le pasará IT_KEY. En asociación se indica del nodo de la key a que nodo se quiere recuperar valores. En el ejemplo la key pertenece al nodo root y queremos que recuperar los datos asociados a dicha clave en el nodo content.\nmo_svc_mngr-\u0026gt;retrieve_by_association( EXPORTING iv_node_key = zif_atron_file_engine_c=\u0026gt;sc_node-root it_key = VALUE #( ( key = iv_id_file ) ) iv_association = zif_atron_file_engine_c=\u0026gt;sc_association-root-content iv_fill_data = abap_true IMPORTING et_data = lt_content ). Hacer consultas\r#\r\rAl BOPF se puede hacer búsqueda por cualquier campo de los nodos que tenga. Para eso el nodo tiene que tener un tipo de búsqueda de tipo *Select by elements\u0026rdquo; en la pestaña Query.\nLos parámetros de búsqueda se pasan en el tipo tabla /BOBF/T_FRW_QUERY_SELPARAM y el método ha usar es el QUERY. Ejemplo:\nDATA lt_header_selparams TYPE /bobf/t_frw_query_selparam. INSERT VALUE #( attribute_name = zif_car_bo_orders_c=\u0026gt;sc_node_attribute-root-vkorg sign = \u0026#39;I\u0026#39; option = \u0026#39;EQ\u0026#39; low = p_vkorg ) INTO TABLE lt_header_selparams. INSERT VALUE #( attribute_name = zif_car_bo_orders_c=\u0026gt;sc_node_attribute-root-vtweg sign = \u0026#39;I\u0026#39; option = \u0026#39;EQ\u0026#39; low = p_vtweg ) INTO TABLE lt_header_selparams. INSERT VALUE #( attribute_name = zif_car_bo_orders_c=\u0026gt;sc_node_attribute-root-spart sign = \u0026#39;I\u0026#39; option = \u0026#39;EQ\u0026#39; low = p_spart ) INTO TABLE lt_header_selparams. mo_svc_mngr-\u0026gt;query( EXPORTING iv_query_key = zif_car_bo_orders_c=\u0026gt;sc_query-root-select_by_elements it_selection_parameters = lt_header_selparams iv_fill_data = abap_true IMPORTING et_data = et_header ). El parámetro IV_FILL_DATA tiene que estar TRUE para que nos devuelva datos en el parámetro ET_DATA. La tabla interna donde se almacenan los datos debe ser del tipo Combinated table type que hay definida en el nodo. Si solo queremos los ID de los registros se haría de la siguiente manera:\nmo_svc_mngr-\u0026gt;query( EXPORTING iv_query_key = zif_car_bo_orders_c=\u0026gt;sc_query-positions-select_by_elements it_selection_parameters = it_positions_selparams iv_fill_data = abap_false IMPORTING et_key = DATA(lt_key_pos) ). El parámetro de entrada IS_QUERY_OPTIONS se le puede indicar los campos de ordenación, número de registros y opciones de páginación.\n"});index.add({'id':25,'href':'/docs/python/machine_learning/arbol_decisiones/','title':"Arbol de decisiones",'content':"Introducción\r#\r\rTodo sobre arboles de decisiones\nPara oider usar los algoritmos de regresion líneas en el ambiente de anaconda tiene que estar instalado la librería Scikit learn se puede obtener más información en la página que habla sobre las librerías\nSecciones\r#\r\rLas secciones son las siguientes:\n\r\rEjemplo básico\r\r\rEjemplo básico de entrenamiento y visualización del arbol de decisiones\r\r\rExplicación sobreajuste\r\r\rEjemplo sobreajuste\r\r\r"});index.add({'id':26,'href':'/docs/sap/abap/archivelink/','title':"Archivelink",'content':"Objetivo\r#\r\rRecopilar información útil sobre los archivelink\nProyectos personales\r#\r\rEn el siguiente enlace hay una clase que permite la gestión de documentos en Archivelink, GOS y BDS. Permite leer, subir y borrar documentos.\nPublicaciones\r#\r\r\r\rInformación general\r\r\rInformación general\r\r\r"});index.add({'id':27,'href':'/docs/python/framework/django/autentificacion_session/','title':"Autentificación por sesión",'content':"Introducción\r#\r\rA través de Django se puede autentificar a los servicios de varias maneras. No es el objetivo de explicarlos aquí, pero el que he configurado es el de Session. E\nEste tipo de autentificación hay un primer servicio que se le pasa el usuario y password y si es valido te genera una cookie de sesión que en los servicios siguientes no sea necesario pasar las credencilas. Se puede indicar el tiempo de expiración de la sessión para que de manera automática haga el logout.\nComo la documentación de Django y Django Rest FrameWork es muy teoríca y poco práctica, al final gracias a un ejemplo he conseguido hacerlo funcionar. Por ello voy a poner aquí los pasos.\nConfiguración\r#\r\rHay que ir al archivo settings.py que esta dentro de las dos carpetas del mismo nombre que se genera el inicio del todo. Cualquier duda recomiendo al índice de Django para ver los primeros pasos.\nEn este archivo haremos las siguientes tareas.\n Se añadira el tipo de autentificación en la variable REST_FRAMEWORK, bueno esta variable sirve para muchas cosas entre ellas la autentificación:  REST_FRAMEWORK = { # Modo de autentificación global. Será por sesión \u0026#39;DEFAULT_AUTHENTICATION_CLASSES\u0026#39;: [ \u0026#39;rest_framework.authentication.SessionAuthentication\u0026#39; ], # Acceso a los servicios solo si están autentificados. \u0026#39;DEFAULT_PERMISSION_CLASSES\u0026#39;: ( \u0026#39;rest_framework.permissions.IsAuthenticated\u0026#39;, ) } A modo informativo si no queremos que se haga ningún tipo de autentificación solo habría que poner:\nREST_FRAMEWORK = { \u0026#39;DEFAULT_AUTHENTICATION_CLASSES\u0026#39;: [], \u0026#39;DEFAULT_PERMISSION_CLASSES\u0026#39;: [], \u0026#39;UNAUTHENTICATED_USER\u0026#39;: None } Al final del archivo he puesto las siguiente variables:\n# Evita que se puede acceder a la cookie mediante javascript # Más info: https://docs.djangoproject.com/en/dev/ref/settings/#session-cookie-httponly SESSION_COOKIE_HTTPONLY = True # Cuando se cierra el navegador la sesión caduca SESSION_EXPIRE_AT_BROWSER_CLOSE = True # Tiempo de validez de la session. Son en segundos. Le he puesto 60 minutos. SESSION_COOKIE_AGE = 60*60 Los comentarios son autosuficientes para entender para que se usan.\nSerializadores\r#\r\rEl serializador, o el controlador como yo lo llamo, es el que nos va gestionar si el usuario introducido existe. Yo en este caso he creado a nivel de proyecto un serializador porque al final el login/logout afecta a todos los servicios de toda las aplicaciones del proyecto.\nComo el fichero serializers.py no existia lo he creado en la caperta donde tenemos el settings.py poniendo el siguiente código:\nfrom django.contrib.auth import authenticate from rest_framework import serializers \u0026#34;\u0026#34;\u0026#34; Clase que se encarga de validar que el usuario y password existen \u0026#34;\u0026#34;\u0026#34; class LoginSerializer(serializers.Serializer): user = serializers.CharField() password = serializers.CharField() def validate(self, attrs): user = authenticate( username=attrs[\u0026#39;user\u0026#39;], password=attrs[\u0026#39;password\u0026#39;]) if not user: raise serializers.ValidationError(\u0026#39;Incorrect user or password.\u0026#39;) if not user.is_active: raise serializers.ValidationError(\u0026#39;User is disabled.\u0026#39;) return {\u0026#39;user\u0026#39;: user} Lo bueno de los serializers en Django Rest Framework es que pueden detener su propio modelo sin tener que declararlo en base de datos.\nVistas\r#\r\rLas vistas es el endpoint donde accede el servicio. De nuevo, he creado el fichero views.py en la misma carpeta que el serializer.\nLas vistas usadas son las genéricas, las APIView. Supongo que usan ese tipo de vistas porque tienen que declarar los distintos métodos que quieres usar. Si llamas al servicio con un método POST pero solo tienes definido el GET te va a dar un error que el método usado no es válido.\nSi se usará las vistas ViewSet no se puede hacer porque automimplementan ellas mismas todos los métodos.\nEl código es el siguiente:\nfrom django.contrib.auth import login, logout from rest_framework.views import APIView from rest_framework import permissions from rest_framework import response from rest_framework import status from rest_framework import authentication from . import serializers \u0026#34;\u0026#34;\u0026#34; Solo se usa para el login y logout y desactivan la validación Csrf para poder hacer generar la sesión o cancelarla. \u0026#34;\u0026#34;\u0026#34; class CsrfExemptSessionAuthentication(authentication.SessionAuthentication): def enforce_csrf(self, request): return \u0026#34;\u0026#34;\u0026#34; Se encarga del login, es el que hará la cookie de sesión. Se usa el tipo de visto APIView, porque es la que se tiene que definir cada método para que funcione. Ejemplo, tanto el login como logout cualquier llamada que no sea post devolverá un método no permitido. \u0026#34;\u0026#34;\u0026#34; class LoginView(APIView): permission_classes = (permissions.AllowAny,) authentication_classes = (CsrfExemptSessionAuthentication,) def post(self, request): serializer = serializers.LoginSerializer(data=request.data) serializer.is_valid(raise_exception=True) user = serializer.validated_data[\u0026#39;user\u0026#39;] login(request, user) return response.Response(status=status.HTTP_200_OK) class LogoutView(APIView): def get(self, request): logout(request) return response.Response() En la clase de LoginView hay varias cosas interesantes:\n permission_classes indica que o tiene permisos o no tiene. Porque hay permisos que indica si no tiene permisos solo puedes acceder a solo lectura. Aquí nada. authentication_classes se le pasa una clase para que no valide el Csrf token que es obligatorio la autentificación por sesión. Este token se generara en la sentencia login  En la clase LogoutView se habilitado el método HTTP GET porque no es necesario pasar nada en el body para hacerlo el proceso.\nURLs\r#\r\rA nivel de proyecto hay que añadir el acceso a las dos vistas creadas. Para ello hay que modificar el archivo urls.py, que este se genera al crear el proyecto, y se añaden las siguientes líneas:\nurlpatterns = [ path(\u0026#39;admin/\u0026#39;, admin.site.urls), path(r\u0026#39;login\u0026#39;, views.LoginView.as_view(), name=\u0026#39;login\u0026#39;), path(r\u0026#39;logout\u0026#39;, views.LogoutView.as_view(), name=\u0026#39;logout\u0026#39;) ] El path de admin ya viene de serie y se añade las dos vistas de login y logout.\nFuncionamiento\r#\r\rLas pruebas se han hecho con el programa POSTMAN. Para hacer el login és con la siguiente url: http://localhost:8000/login\nAl llamarlo a nivel de cabecera veremos las dos cookies que genera:\n\rPara las siguiente llamadas hay que tener en cuenta que si son de tipo POST habrá que pasar informar en la cabecera de la llamada la variable X-CSRFToken con el valor devuelto en la cookie que tiene en el campo el literal csrftoken=, hay que coger todo la cadena de número hasta el ;, sin incluirlo. Tal que así:\n\rPara los métodos GET no es necesario.\n"});index.add({'id':28,'href':'/docs/sap/abap/bapis/','title':"BAPIs",'content':"Introducción\r#\r\rLas BAPIs son objetos de negocio (pedidos, documentos financieros, empleado, etc.) que aglutinan un conjunto de funciones que permiten crear, modificar o consultar datos de negocio. Por ejemplo, el el objeto de negocio de pedido de compras tendríamos las opciones de: crear. modificar, liberar, etc..\nEsas funcionalidad son funciones, de tipo RFC, que permiten crear desde pedidos, contabilizar documentos financieros, etc. Por eso coloquialmente cuando se habla de BAPI se refiere a una función, porque dichas funciones normalmente comiezan por *BAPI_*.\nLa transacción donde podemos ver los objetos de negocio es la transacción BAPI. Aunque muchas veces hay funciones que no están inventaridas en dicha transacción, por ello, para funciones de módulos pequeños o acciones muy específicas lo mejor es buscar las funciones que comiencen por *BAPI_*.\nEjemplos de BAPIs\r#\r\rLos ejemplos de BAPIs se van a ir organizando por módulos para que quede mejor organizado.\n\r\rBAPIs del módulo comercial o ventas\r\r\rBAPIs del módulo comercial o ventas\r\r\r"});index.add({'id':29,'href':'/docs/sap/abap/bapis/ventas/','title':"BAPIs del módulo comercial o ventas",'content':"Introducción\r#\r\rEjemplos de las BAPIs del módulo de ventas\nEjemplos\r#\r\r\r\rCreación pedido de ventas\r\r\rEjemplo básico de creación pedido de ventas\r\r\r"});index.add({'id':30,'href':'/docs/sap/abap/bopf/tablas/basic_component/','title':"Basic Component",'content':"Objetivo\r#\r\rGeneralmente las tablas que generales a todo SAP son las interesantes ya que permiten cosas que de otra manera no se podría saber: ya sea porque no se conoce la transacción o falta de permisos para acceder a determinadas transacciones.\nLista\r#\r\rGenerales\r#\r\r   Función Descripción     SXC_CLASS Contiene que clase esta utilizando una implementacion de BADI   DWINACTIV Objetos desactivados   SSM_CUST Variables del session manager (ejemplo imagen en pantalla de inicio)   SSM_USR Tabla para controlar a que menus tiene acceso el usuario en el session manager   USERS_SSM Lo mismo que la de antes pero valida en la ECC6.0 EHP4   SE16N_CD_KEY Change Documents – Header   SE16N_CD_DATA Change Documents – Data   DBTABLOG Log de modificacion de tablas   TLOCK Tabla de bloqueos de objetos en ordenes.   BALHDR Tabla de cabecera con logs que se generan con la SLG0 y SLG1    Desarrollo ABAP\r#\r\r   Función Descripción     DEVACCESS Tabla con la clave desarrollador de los usuarios   TFDIR Include y programa de una funcion.    Formularios\r#\r\r   Función Descripción     STXFTXT Tabla de textos de formularios    Archivelink / GOS / BDS / Etc.\r#\r\r   Función Descripción     SRGBTBREL Tabla con los adjuntos del GOS. Es como la TOA01 del archivelink   SGOSATTR Tabla que sirve para configurar acciones que salen en el boton del GOS de SAP   SDOKFEXT_C Tabla de cliente para poner los mimetype que faltan para que pueda funcionar el GOS    "});index.add({'id':31,'href':'/docs/sap/abap/funciones/basic_component/','title':"Basic Component",'content':"Objetivo\r#\r\rFunciones del módulo basic component, es decir, funciones que son generales a todo SAP.\nAl final hay una sección de ejemplos para algunas funciones. No todas las funciones tienen ejemplo.\nLista\r#\r\rGenerales\r#\r\r   Función Descripción     SXO_IMPL_DELETE Borra la implementación de una BADI   SCP_REPLACE_STRANGE_CHARS Quita acentos, ç, ¨, etc… de una char   SELECTION_TEXTS_MODIFY_DTEL Permite cambiar el texto de un campo de selección. Util para en enhacements en programas estándar.   SAPGUI_SET_FUNCTIONCODE Permite simular que se pulsa un código de función.   SEO_METHOD_SIGNATURE_GET De una clase y método te devuelve sus parametros y tipos de datos   C14Z_MESSAGES_SHOW_AS_POPUP Saca un popup con los mensajes (formato similar a bapiret2) con sus iconos y demas   F4IF_INT_TABLE_VALUE_REQUEST Permite hacer ayudas para búsqueda en base a una tabla interna   RS_REFRESH_FROM_SELECTOPTIONS Recupera los valores de los parametros de una pantalla de selección. Ideal para usarlo despues con el WITH SELECTION-TABLE   SYSTEM_CALLSTACK Funciones para ver la pilas de llamada de un progama   BP_JOB_READ Lee los datos de un job. El OPCODE para leerlo todo es el 36.    Diccionario de datos\r#\r\r   Función Descripción     DOMAIN_VALUE_GET Se le pone el dominio y valor, y devuelve su descripcion   DDUT_DOMVALUE_TEXT_GET Recupera el texto de un valor fijo de un dominio   OIUP_REFRESH_TABLE_BUFFER Sincroniza el buffer de una tabla    SPOOL\r#\r\r   Función Descripción     RSPO_RETURN_SPOOLJOB Devuelve el contenido binario de una orden de spool (para listados da dump). Si en el campo DESIRED_TYPE le pones \u0026ldquo;RAW\u0026rdquo; devuelve el contenido en formato legible. En este caso no da dump si es un ALF.   RSPO_DOWNLOAD_SPOOLJOB Funcion que descarga un spool a fichero plano    Conversión\r#\r\r   Función Descripción     UNIT_CONVERSION_SIMPLE Convierte valores de unidades medidas estándar (de horas a minutos, etc..)   HRHAP_CONVERT_STR_TO_TABLE Pasa de un string a un tline respetando saltos de lineas    Dirección\r#\r\r   Función Descripción     ADDR_GET Devuelve los datos de direccion de cualqueir tipo de ADRNR   ADDR_GET_COMPLETE Te devuelve todos los datos de direccion: direccion, mails, telefonos, etc..   ADDRESS_INTO_PRINTFORM De una direccion te lo pinta en formato direccion de smartform. En el campo ADDRESS_TYPE hay que poner el valor \u0026lsquo;1\u0026rsquo;.    Smartform\r#\r\r   Función Descripción     SSFRT_READ_TEXTMODULE Devuele el texto creado por la transaccion SMARTFORMS    ALV\r#\r\r   Función Descripción     LT_DBDATA_READ_FROM_LTDX Permite leer los datos de una variante de ALV: catalogo, ordenaciones, etc..    Ejemplos\r#\r\rSELECTION_TEXTS_MODIFY_DTEL\r#\r\rDATA: lt_zsel_dtel TYPE rsseldtel OCCURS 0, ls_zsel_dtel TYPE rsseldtel. ls_zsel_dtel-name = \u0026#39;S_PRODPH\u0026#39;. ls_zsel_dtel-kind = \u0026#39;S\u0026#39;. ls_zsel_dtel-datenelment = \u0026#39;ZZE_PRODPH\u0026#39;. APPEND ls_zsel_dtel TO lt_zsel_dtel. CALL FUNCTION \u0026#39;SELECTION_TEXTS_MODIFY_DTEL\u0026#39; EXPORTING PROGRAM = sy-repid TABLES sel_dtel = lt_zsel_dtel EXCEPTIONS program_not_found = 1 program_cannot_be_generated = 2 OTHERS = 3. LT_DBDATA_READ_FROM_LTDX\r#\r\rCALL FUNCTION \u0026#39;LT_DBDATA_READ_FROM_LTDX\u0026#39; EXPORTING I_TOOL = R_TOOL \u0026#34; LT IS_VARKEY = LS_VARKEY TABLES T_DBFIELDCAT = LT_DBFIELDCAT T_DBSORTINFO = LT_DBSORTINFO T_DBFILTER = LT_DBFILTER T_DBLAYOUT = LT_DBLAYOUT EXCEPTIONS NOT_FOUND = 1 WRONG_RELID = 2 OTHERS = 3. REPORT ZMML0088 HANDLE LOG_GROUP USERNAME JTORRE VARIANT TEST_ENZ TYPE F "});index.add({'id':32,'href':'/docs/sap/abap/clases/basic_component/','title':"Basic component",'content':"Lista\r#\r\r   Clase Descripción     CALL METHOD cl_gui_cfw=\u0026gt;set_new_ok_code( ‘YOUR_OK_CODE’ ). Permite simular que se pulsa un código de función.   CL_BCS_CONVERT Convierte de txt a raw, a solix, viceversa, etc…   CL_ABAP_EXCEPTIONAL_VALUES Con el método get_max_value devuelve el valor máximo de una variable. Si es numc03 da 999.   CL_SYSTEM_TRANSACTION_STATE Para saber si esta en un proceso commit, o saber el numero de LUW, etc..   CL_ABAP_DATFM Funcion que te devuelve el formato de fecha de un usuario o pais. Y de ese formato tiene otro método que te dice la mascara   CL_SALV_BS_RUNTIME_INFO Clase para desactivar la salida del ALV y poder recuperar los datos    "});index.add({'id':33,'href':'/docs/sap/abap/sentencias_74/bucles/','title':"Bucles",'content':"Introducción\r#\r\rEn este grupo se irán poniendo las distintas sentencias que nos permiten procesar tablas internas. Este artículo esta muy relacionado con el del relleno de valores ya que se usán las mismas sentencias para procesar tablas internas. Aún asi, lo separo en dos artículos según la finalidad en que se use.\nEstas sentencias se pueden utilizar para crear nuevas variables o ponerlas como entrada de parámetro en clases.\nEjemplo 1\r#\r\rEl siguiente ejemplo se usa la cláusula GROUP BY de los LOOP que sustituye a los AT NEW. En el primer loop nos devuelve la clave de registros únicos que se va agrupando y que informa en el field-symbol . Y el loop at group  nos devuelve los registros que conforman dicha clave.\nLOOP AT mt_level_agr_type ASSIGNING FIELD-SYMBOL(\u0026lt;ls_level_agr_type\u0026gt;) WHERE id_nivel1 = \u0026#39;OFF\u0026#39; GROUP BY ( id_nivel1 = \u0026lt;ls_level_agr_type\u0026gt;-id_nivel1 id_nivel_inf = \u0026lt;ls_level_agr_type\u0026gt;-id_nivel_inf boart = \u0026lt;ls_level_agr_type\u0026gt;-boart size = GROUP SIZE index = GROUP INDEX ) ASSIGNING FIELD-SYMBOL(\u0026lt;group\u0026gt;). . write:/ \u0026lt;group\u0026gt;-id_nivel1, \u0026lt;group\u0026gt;-id_nivel_inf, \u0026lt;group\u0026gt;-boart. LOOP AT GROUP \u0026lt;group\u0026gt; ASSIGNING FIELD-SYMBOL(\u0026lt;ls_level_agr_type_row\u0026gt;). WRITE:/ \u0026lt;ls_level_agr_type_row\u0026gt;-boart_agr. ENDLOOP. ENDLOOP. Ejemplo 2\r#\r\rEn este ejemplo es similar al anterior la diferencia es que dentro del loop ya tendremos los registros únicos según lo que se indique en el GROUP BY.\nLOOP AT mt_key_field_text_cond ASSIGNING FIELD-SYMBOL(\u0026lt;ls_key_field_text_cond_group\u0026gt;) WHERE kschl = \u0026lt;ls_kschl\u0026gt;-kschl AND field_cust_is_hier = zcl_rtn_customer_data=\u0026gt;is_customer_hierarchy( iv_customer ) AND field_cust IS NOT INITIAL GROUP BY \u0026lt;ls_key_field_text_cond_group\u0026gt;-kotab. En la clausula GROUP BY se indica los campos que se quiere hacer la agrupación\nEjemplo 3\r#\r\rEjemplo encontrado de como hacer un collect:\nactual_collect = VALUE #( FOR GROUPS group OF \u0026lt;flight\u0026gt; IN original_table GROUP BY ( carrid = \u0026lt;flight\u0026gt;-carrid connid = \u0026lt;flight\u0026gt;-connid ) ( REDUCE #( INIT sum_aux TYPE sflight FOR line IN GROUP group NEXT sum_aux = VALUE #( BASE sum_aux carrid = line-carrid connid = line-connid price = sum_aux-price + line-price seatsmax = sum_aux-seatsmax + line-seatsmax seatsocc = sum_aux-seatsocc + line-seatsocc paymentsum = sum_aux-paymentsum + line-paymentsum seatsmax_b = sum_aux-seatsmax_b + line-seatsmax_b seatsocc_b = sum_aux-seatsocc_b + line-seatsocc_b seatsmax_f = sum_aux-seatsmax_f + line-seatsmax_f seatsocc_f = sum_aux-seatsocc_f + line-seatsocc_f ) ) ) ). Ejemplo collect\r#\r\rEn este ejemplo es propio y permite saber cuantas repeticiones que hay de las price list(PLTYP)\nTYPES: BEGIN OF ts_pltyp_count, pltyp TYPE knvv-pltyp, count TYPE i, END OF ts_pltyp_count. TYPES: tt_pltyp_count TYPE STANDARD TABLE OF ts_pltyp_count WITH EMPTY KEY. \u0026#34; Se cuenta cuantas price list hay repetidas DATA(lt_count) = VALUE tt_pltyp_count( FOR GROUPS lt_group OF \u0026lt;wa1\u0026gt; IN lt_customer_sales GROUP BY ( pltyp = \u0026lt;wa1\u0026gt;-pltyp ) ( REDUCE #( INIT ls_count TYPE ts_pltyp_count FOR ls_group IN GROUP lt_group NEXT ls_count = VALUE #( BASE ls_count pltyp = ls_group-pltyp count = ls_count-count + 1 ) ) ) ). Este es un caso, cuya legibilidad no queda clara y tengas que leer la sentencias varias veces para entenderla. Sobretodo porque son cosas que, al menos a mi, no se usan muy a menudo.\nEjemplo de un DELETE ADJACENT DUPLICATES con la sentencia REDUCE\r#\r\rTYPES: BEGIN OF ts_dates_intervals, datab TYPE datab, datbi TYPE datab, END OF ts_dates_intervals. TYPES: tt_dates_intervals TYPE STANDARD TABLE OF ts_dates_intervals WITH EMPTY KEY. DATA(lt_dates_intervals) = VALUE tt_dates_intervals( ( datab = \u0026#39;20200101\u0026#39; datbi = \u0026#39;20201231\u0026#39; ) ( datab = \u0026#39;20200101\u0026#39; datbi = \u0026#39;20201231\u0026#39; ) ( datab = \u0026#39;20200101\u0026#39; datbi = \u0026#39;20211231\u0026#39; ) ( datab = \u0026#39;2190301\u0026#39; datbi = \u0026#39;20211231\u0026#39; ) ( datab = \u0026#39;2190301\u0026#39; datbi = \u0026#39;20200830\u0026#39; ) ). DATA(lt_dates) = VALUE tt_dates_intervals( FOR GROUPS lt_group OF \u0026lt;wa\u0026gt; IN lt_dates_intervals GROUP BY ( datab = \u0026lt;wa\u0026gt;-datab datbi = \u0026lt;wa\u0026gt;-datbi ) ( REDUCE #( INIT ls_dates TYPE ts_dates_intervals FOR ls_group IN GROUP lt_group NEXT ls_dates = VALUE #( BASE ls_dates datab = ls_group-datab datbi = ls_group-datbi ) ) ) ). Otra sentencia que usando las sentences de siempre quedan más claras para cualquiera que las lea.\nContar el valor de un campo en una tabla interna\r#\r\rEjemplo que cuenta los registros que cumplen unas determinas condiciones:\ncs_layout_options-number_columns_fixed = REDUCE #( INIT x = 0 FOR \u0026lt;fcat_ui5\u0026gt; IN it_fieldcatalog_ui5 WHERE ( fixed_column = abap_true ) NEXT x = x + 1 ). Concatenar todos los valores de un campo de una tabla separandolos por un cáracter separador\r#\r\rSe puede hacer de tres formas:\n Ejemplo 1  DATA(lv_sql) = REDUCE string( INIT sql TYPE string FOR \u0026lt;ls_fields\u0026gt; IN lt_fields NEXT sql = sql \u0026amp;\u0026amp; COND #( LET sep = \u0026#39;,\u0026#39; IN WHEN sql IS NOT INITIAL THEN |{ sep } { \u0026lt;ls_fields\u0026gt; }| ELSE |{ \u0026lt;ls_fields\u0026gt; }| ) ). Ejemplo 2  DATA(lv_sql) = REDUCE string( INIT sql TYPE string FOR \u0026lt;ls_fields\u0026gt; IN lt_fields NEXT sql = sql \u0026amp;\u0026amp; COND #( WHEN sql IS NOT INITIAL THEN |, { \u0026lt;ls_fields\u0026gt; }| ELSE |{ \u0026lt;ls_fields\u0026gt; }| ) ). Ejemplo 3  DATA(lv_sql) = REDUCE string( INIT sql TYPE string FOR \u0026lt;ls_fields\u0026gt; IN lt_fields NEXT sql = sql \u0026amp;\u0026amp; CONV string( LET sep = COND #( WHEN sql IS NOT INITIAL THEN \u0026#39;,\u0026#39; ELSE \u0026#39;\u0026#39; ) IN |{ sep } { \u0026lt;ls_fields\u0026gt; }| ) ). Todas ellas casi funcionan de la misma manera, la diferencia es como se concatena los campos. Casi, porque el ejemplo 3 pone un espacio en blanco antes del primer campo.\nComo nota personal el código que finalmente use para el desarrollo que estaba haciendo es el número 2.\n"});index.add({'id':34,'href':'/docs/sap/BW/','title':"BW",'content':"Objetivo\r#\r\rBW es Business Warehouse es la parte analítica de SAP. Normalmente el BW en SAP esta en sistemas separados. Pero ahora con la opciones como los CDS es relativamente sencillo montarse un cubo a partir de CDS. COn lo cual se puede montar en cualquier sitio un sistema analítico.\nPublicaciones\r#\r\r\r\rInformación general\r\r\rInformación general\r\r\r"});index.add({'id':35,'href':'/docs/python/sentencias/cadenas/','title':"Cadenas",'content':"Cadenas\r#\r\rLas cadenas son secuencias de carácteres. Es decir, una variable de tipo de string puedes acceder a una posición concreta. Es como acceder a una posición de un array o lista. Ejemplo:\ntexto = \u0026#34;hola\u0026#34; print(\u0026#34;Segunda letra del texto es:\u0026#34;, texto[1]) Resultado:\nSegunda letra del texto es: o\rOJO: Que aquí los arrays también empiezan por 0.\nCon la sentencia LEN se puede saber el tamaño de la cadena. Ejemplo:\ntexto = \u0026#34;hola\u0026#34; print(\u0026#34;Primera letra del texto es: \u0026#34;, texto[0]) longitud = len(texto) print(\u0026#34;Última letra del texto es: \u0026#34;,texto[longitud-1]) Resultado:\nPrimera letra del texto es: h\rÚltima letra del texto es: a\rComo en Javascript la longitud hay que restarle uno para poder acceder al último elemento, o para hacer bucles.\nHay que tener en cuenta que las cadenas no se puede modificar como si se puede hacer en otros lenguajes, solo podemos consultarla. Es decir, esto:\ntexto=\u0026#34;hola\u0026#34; texto[2]=\u0026#34;C\u0026#34; Da el siguiente error:\nTypeError Traceback (most recent call last)\rin 1 texto=\u0026quot;hola\u0026quot;\r----\u0026gt; 2 texto[2]=\u0026quot;C\u0026quot;\rTypeError: 'str' object does not support item assignment\rBucles\r#\r\rA las cadenas también se puede acceder haciendo iteracciones. Ejemplo:\nEjemplo:\ntexto=\u0026#34;hola\u0026#34; n=0 while n \u0026lt; len(texto): print(\u0026#34;Letra: \u0026#34;,texto[n]) n = n + 1 Resultado:\nLetra: h\rLetra: o\rLetra: l\rLetra: a\rEl ejemplo anterior pero usando FOR\ntexto=\u0026#34;hola\u0026#34; for letra in texto: print(\u0026#34;Letra: \u0026#34;, letra) Dando el mismo resultado.\nAccediendo a las partes de la cadena\r#\r\rTambién se puede acceder a trozos de una cadena:\ntexto=\u0026#34;hola mundo\u0026#34; print(\u0026#34;Primera parte: \u0026#34;, texto[0:4]) print(\u0026#34;Segunda parte: \u0026#34;, texto[5:10]) Resultado:\nPrimera parte: hola\rSegunda parte: mundo\rOperadores en condiciones\r#\r\rEl operador IN se menciona en la página de condiciones pero pongo el ejemplo aquí que es su sitio:\ntexto=\u0026#34;hola\u0026#34; if \u0026#34;o\u0026#34; in texto: print(\u0026#34;Existe la O\u0026#34;) if \u0026#34;j\u0026#34; in texto: pass else: print(\u0026#34;No existe la J\u0026#34;) Resultado:\nExiste la O\rNo existe la J\rTambién se puede usar los \u0026lt; o \u0026gt; para hacer comparaciones por orden alfabético.\nMétodos de cadenas\r#\r\rLas cadenas son también objetos que por defecto tienen su propios métodos. Para saber que métodos tiene un cadena hay que hacer lo siguiente:\ntexto=\u0026#34;hola\u0026#34; dir(texto) El resultado no lo pongo porque es muy largo y supongo que los métodos variarán segun la versión de Python usada. Un ejemplo de como se usaría:\ntexto=\u0026#34;hola\u0026#34; print(\u0026#34;texto en minúsculas\u0026#34;, texto) print(\u0026#34;texto en mayñusculas\u0026#34;, texto.upper()) Resultado:\ntexto en minúsculas hola\rtexto en mayñusculas HOLA\rPara saber como funciona mejor es irse a la docu oficial de Python.\nFormatos\r#\r\rComo cadenas se les puede aplicar formatos. El carácters para aplicar formatos es % y a continuación el tipo de valor que se va formtear:\n d \u0026ndash;\u0026gt; Decimal g \u0026ndash;\u0026gt; Flotante s \u0026ndash;\u0026gt; String  Ejemplo:\ntexto=\u0026#34;ivan\u0026#34; print(\u0026#34;hola %s\u0026#34; % texto) Resultado:\nhola ivan\rSe puede sustituir varios valores pero hay que definir una tupla. Ejemplo:\nprint(\u0026#34;hola %s, hoy es dia %d y son las %g\u0026#34; % (\u0026#34;ivan\u0026#34;, 5, 16.32)) Resultado:\nhola ivan, hoy es dia 5 y son las 16.32\rEl numero de formatos en la cadena debe coincidir con el número de valores en una tupla para que no de error.\nString sin carácteres de escape\r#\r\rSi queremos inicializar una variable con un ruta de nuestro disco duro como esto:\npath = \u0026#34;c:\\archivos de programa\\test\\test.exe\u0026#34; No se almacene bien porque python por defecto usa el carácter ** como carácter de escape. Si queremos decirle que lo trate como un raw string, es decir, literal sin tranformaciones hay que poner una r delante del literal, tal que así:\npath = r\u0026#34;c:\\archivos de programa\\test\\test.exe\u0026#34; "});index.add({'id':36,'href':'/docs/python/sentencias/captura_excepciones/','title':"Captura de excepciones",'content':"Captura excepciones\r#\r\rPara captura excepciones y que no salgan error raros al hacer operaciones tenemos la sentencia TRY..EXCEPT\nEn el siguiente ejemplo extráido de un manual hay una formula para pasar de grados Farenheit a Celsius:\nent = input(\u0026#39;Introduzca la Temperatura Fahrenheit:\u0026#39;) try: fahr = float(ent) cel = (fahr - 32.0) * 5.0 / 9.0 print(\u0026#34;Grados celsius\u0026#34;:cel) except: print(\u0026#39;Por favor, introduzca un número\u0026#39;) NOTA: Aquí también tiene que haber una identación en el código dentro del TRY y del EXCEPT\nResultado:\nGrados celsius: 7.222222222222222\rSi se pone un valor no válido, como un string: Resultado:\nPor favor, introduzca un número\r"});index.add({'id':37,'href':'/docs/sap/abap/cds/','title':"CDS",'content':"Objetivo\r#\r\rLos de CDS que aparecen, si no recuerdo mal, en la versión 7.4 de ABAP pero es en la versión 7.5, donde creo, tiene ya una funcionalidad madura que permite usarse en multitud de sitios.\nLos CDS están pensados para y por HANA, aunque no es necesario tener HANA para usarlo pero según que funcionalidad de que uses pueden perder rendimiento, ya que disponen de funcionalidades que permite que el trabajo \u0026ldquo;sucio\u0026rdquo; lo haga la base de datos. Además, los CDS permiten un Open SQL extendido, que se asemeja cada vez más al standard SQL, que permiten realizar operaciones sin necesidad de tratar a posterior los datos en ABAP.\nY punto importante como los CDS se ejecutan directamente en base de datos son más óptimos que hacer un SELECT tradicional a la hora de buscar y/o procesar un volumen alto de datos.\nPublicaciones\r#\r\r\r\rAtajos\r\r\rCódigo para ser usado\r\r\r"});index.add({'id':38,'href':'/docs/sap/abap/clases/','title':"Clases",'content':"Objetivo\r#\r\rRecopilación de clases que son interesantes\nPublicaciones\r#\r\r\r\rBasic component\r\r\rBasic component\r\r\r"});index.add({'id':39,'href':'/docs/sap/abap/clases_desarrollo/','title':"Clases de desarrollo",'content':"Objetivo\r#\r\rRecopilar clases de desarrollo que son interesante para tener ejemplos\nLista\r#\r\r   Clase desarrollo Descripción     SST_DEMO Ejemplos de hojas de transformación simple   SABAPDEMOS Ejemplos de todo tipo sobre ABAP. Desde rendimiento, CDS, push, etc..    "});index.add({'id':40,'href':'/docs/python/machine_learning/redes_neuronales/como_funcionan_redes_neuronales/','title':"Como funcionan las redes neuronales",'content':"Introducción\r#\r\rEjemplo extraído de ¿Como funcionan las redes neuronales?. Se pondrán imagens obtenidas del video para explicar mejor el funcionamiento tal como lo hace en el video.\nExplicación\r#\r\rComo se explica en el índice de la página las redes neuronales se basan en la neuronas del cerebro humano. Una neurona esta interconectada con otras neuronas y responde cuando recibe un estimulo eléctrico. Cuando lo recibe decide si tiene que activarse o no. Si se activa, esa neurona enviará un impulso eléctrico a las neuronas que esta conectada. Si no se activa, no enviará información.\nLas redes neuronales lo que hacer es seguir el mismo principio.\nEn el siguiente ejemplo basíco de funcionamiento de una red neuronal:\n\rEs una diagrama que tienes tres capas:\n La capa que esta la izquierda del todo es la que recibe los datos. Será la capa que reciba los datos de entrenamiento. Laa capa que esta al derecha del todo es la que devuelve los resultados. Es la que recibe los datos de las capas ocultas y a partir de esos datos da un resultado. La capa en medio de ambas es la oculta, aunque en ejemplo solo hay una puede haber miles pero seguirán siendo la capa oculta. Serán las capas ocultas las encargadas de realizar todo el aprendizaje.  Las neuronas de las capaz se conectan a las neuronas de la siguiente capa tal como se en la siguiente imagen:\n\rEl proceso de entrenamiento es un proceso que irá enviando datos al algoritmo, este lo procesará y arrojará un resultado. Si el resultado contiene errores se ajustará los parámetros y se volverá a empezar. Si no hay errores entonces el algoritmo irá bien.\nEl funciona a nivel de neurona se puede ver en la siguiente imagen Una neurona recibe de otras neuronas lo siguientes datos:\n x \u0026ndash;\u0026gt; Es valor a clasificar w \u0026ndash;\u0026gt; El peso de ese valor. El peso permite es que dar más importancia a una conexión que a otra. Es decir, si tenemos X=4 y w=30 y también recibe x=3 y w=40. Tendrá más importancia el valor de x=3.  La neurona también tiene a nivel el valor b. b es el sesgo o bias(ingles). El sesgo indicará con que facilidad se activa la neurona sobre otras.\nPara que una neurona se active y envie información a otra debe cumplirse una formula de activación que es la siguiente:\n\rEn esta formula se hace el sumatario de (valor * Peso) y al resultado se le suma el sesgo, el resultado se guarda en la variable y. Es el valor de esta variable si supera cierto valor entonces la neurona se activará y enviará el valor y, que será el valor x de entrada de la siguientes neuronas que esta conectada.\nEl proceso de entrenamiento es un proceso repetitivo donde se va repitiendo hasta que el resultado sea el esperado. En caso de volverse a repetir se ajustarán los valor de w y b. Para saber como de grande es el error se usará esta ecuación:\n\rLa ecuación se llama Error cuadrático medio, según entiendo del video.\nEn esta formulario se hace el sumario de y(es el resultado que devuelve la neurona) menos a, valor que se espera, elevado al cuadrado. El resultado lo vamos a multiplicar por 1/2*n. Donde n es el número de elementos que se le mando a entrenar.\nDebajo hay una tabla donde tenemos el valor que devuelve, el valor que se espera y el resultado de la formula. El resultado de todos los valores es de 31. Este resultado se le aplicaría la siguiente formula: 31 * ( 1 / 2*n ). n en este caso es 4, porque son los valores que se le ha pasado para el entrenamiento, las filas de la tabla. Al final la formula sería 31 * (1/8) = 3,87. 3,87 es el error que se obtiene.\nAhora que sabemos el error en la siguiente iteracción hay que cambiar el valor de w y/o b para que el error se reduzca.\nPara reducirlo se va utilizar el algoritmo gradiente descendiente\n\rEn este diagrama tenemos 3 ejes:\n w2 \u0026ndash;\u0026gt; Es el equivalente a b, sesgo. w1 \u0026ndash;\u0026gt; Es el equivalente a w, peso e \u0026ndash;\u0026gt; Es el error que devuelve la formuala Error cuadrático medio  **NOTA: El video indica que modifica \u0026ldquo;x\u0026rdquo; pero no es correcto porque el valor de entrada no se modifica.\nDonde se quiere llegar es hasta el punto más bajo donde el error esta minimizado.\nEl punto negro de la imagen supongamos que es donde esta el valor e que se ha obtenido en la formula. Ahora la formula, o algoritmo, va a derivar en ese punto la pendiente. Con el valor de la pendiente el algoritmo sabe donde se tiene que mover para llegar al mínimo global. Para ello irá ajustando los valores de b y/o w y relanzar de nuevo toda la red neuronal para comprobar el error que da.\nUna vez que se llega al mínimo de error, la red ya esta entrada. En ese momento se le puede volver a pasar más datos pero ya no volverá a realizar ecuaciones cuadráticas ni grandientes descendientes porque cada elemento de la red ya tiene su valor de peso, w y b en cada neurona.\nCuando se entrena una red nosotros indicamos el número de iteraciones tiene. Esas iteracciones son las que se aplicarán la ecuación de error y gradientes descendiente. Si las iteracciones son bajas la corrección de error será baja y no hará buenas clasificaciones. Y si tenemos un número de iteracciones muy alto se corre el riesgo de sobreajuste. Con lo cual hay que ir cambiando el número de iteracciones, número de capas, etc. para ir ajustando los valores. Es decir, ir haciendo ensayo y error.\n\r"});index.add({'id':41,'href':'/docs/python/framework/django/usar_react/','title':"Como utilizar React como frontend",'content':"Introducción\r#\r\rLo primero es tener nwb instalado en la maquina.\nSegún he léido en articulos hay como tres posiblidad de hacerlo:\n Tener la aplicación React y django por separado pero habría que usar JWT para poderse comunicar. Aunque yo no tengo tan claro si uno tiene el montaje de sesión como hice en este artículo. Ya que el primer servicio que lanzas en React es para generar el token de sesión y luego lo vas usando para las distintas llamadas. Este indican que la configuración es complicada, y me lo creo con lo que me costo entender y hacer la de sesión Tener React dentro de django. Esta ponen que la dificultad es media. Usar componententes React en Django. Esta indican que la dificultad es fácil.  Yo de momento voy a optar por la solución 2. De esta manera espero ahorrarme tener dos servidores corriendo.\nReact dentro de Django\r#\r\rTodos los pasos que he hecho ha sido dentro de la pestaña de terminal del VS Code con el proyecto abierto.\nCreación de la aplicación Rest\r#\r\rLa aplicación de React hay que hacerla dentro del proyecto de Django mediante el siguiente comando:\nnwb react-app reactjs En la consola veremos los directorios y archivos que se generán\nAjustando el .gitignore\r#\r\rEste punto no lo he encontrado en ningún sitio. Pero si nos fijamos en el resultado de la consola el comando anterior crear el .gitignore para los archivos que no se quieren subit a Git. El mótivo es que este comando crea la aplicación como su fuese standalone. Entonces lo que he hecho es añadir los archivos del archivo al .gitignore de directorio principal del proyecto en VS Code. Quedando así:\n**/dist **/node_modules **npm-debug.log* Le he puesto \u0026ldquo;**\u0026rdquo; delante para que los ignore independietemente del nivel de directorio donde este. De esta manera me evito poner todo el path\nConfigurando los templates en Django\r#\r\rLas plantillas en Django permite crear páginas Web y que se renderizen en las vistas. Lo que se quiere conseguir es que la plantilla arranque la aplicación React.\nPara ello dentro del directorio del proyecto de Django se creará la carpeta templates y a continuación desde el VS Code se creará el archivo index.html con el el texto\nPunto de acceso a React\rEsto es para probar que la configuración inicial funciona.\nAhora hay que indicaré a Django donde están los templates. Para ello hay que modificar el archivo del proyecto Django llamado settings.py y buscar la variable TEMPLATES y ajustarla para que quede tal que así:\nTEMPLATES = [ { \u0026#39;BACKEND\u0026#39;: \u0026#39;django.template.backends.django.DjangoTemplates\u0026#39;, \u0026#39;DIRS\u0026#39;: [ os.path.join(BASE_DIR, \u0026#39;templates\u0026#39;) ], \u0026#39;APP_DIRS\u0026#39;: True, \u0026#39;OPTIONS\u0026#39;: { \u0026#39;context_processors\u0026#39;: [ \u0026#39;django.template.context_processors.debug\u0026#39;, \u0026#39;django.template.context_processors.request\u0026#39;, \u0026#39;django.contrib.auth.context_processors.auth\u0026#39;, \u0026#39;django.contrib.messages.context_processors.messages\u0026#39;, ], }, }, ] Lo único que se añade es esta línea os.path.join(BASE_DIR, \u0026lsquo;templates\u0026rsquo;) ya que el resto ya viene informado al crear el proyecto.\nEn la misma carpeta que están la configuración del proyecto hay que modificar las URLs del proyecto en el archivo urls.py añadiendo lo siguiente:\nfrom django.views.generic import TemplateView path(r\u0026#39;\u0026#39;, TemplateView.as_view(template_name=\u0026#34;index.html\u0026#34;)), Esto indica que cuando se acceda al path inicial del servidor se redirigira al archivo index.html\nAhora hay que arrancar el servidor de Django y acceder a la URL inicial y si vemos el texto que se ha puesto en el fichero HTML es que la configuración es correcta.\nEnlanzar React con Django\r#\r\rEl último paso es poder enlazar React con Django. Para ello se instalará a nivel local el plugin \rhtml-webpack-plugin y el \rhtml-webpack-harddisk-plugin. Esto permite hacer uso de Webpack que permite encapsular todo el código en unos archivos que entienda Django. El segun plugin amplia la funcionalidad del primero para que siempre escriba en disco.\nPara instalarlo hay que ir al directorio de la aplicación React creada dentro del proyecto de Django y ejecutar el siguiente comando:\nnpm install --save-dev html-webpack-plugin npm install --save-dev html-webpack-harddisk-plugin Dara unos avisos de dependencias que hay instalar manualmente pero no hay que hacer nada porque ya estan incluidas con nwb\nAhora hay que editar el fichero nwb.config.js que esta en el directorio de la aplicación React y añadir lo siguiente:\nconst HtmlWebpackHarddiskPlugin = require(\u0026#34;html-webpack-harddisk-plugin\u0026#34;); const path = require(\u0026#34;path\u0026#34;); const isPro = process.env.NODE_ENV === \u0026#34;production\u0026#34;; module.exports = { type: \u0026#34;react-app\u0026#34;, webpack: { // dont forget delete src/index.html to use this config: mountid, title, favicon html: { mountId: \u0026#34;app\u0026#34;, title: \u0026#34;OCR Invoices\u0026#34;, // favicon: \u0026#39;src/favicon.ico\u0026#39; //this setting is required for HtmlWebpackHarddiskPlugin to work alwaysWriteToDisk: true, filename: \u0026#34;index.html\u0026#34;, }, publicPath: isPro ? \u0026#34;/static/\u0026#34; : \u0026#34;http://localhost:3000/\u0026#34;, extra: { plugins: [ // this will copy an `index.html` for django to use new HtmlWebpackHarddiskPlugin({ outputPath: path.resolve(__dirname + \u0026#34;/../\u0026#34;, \u0026#34;templates\u0026#34;), }), ], }, config: function (config) { if (!isPro) { config.entry = [ \u0026#34;webpack-dev-server/client?http://localhost:3000\u0026#34;, \u0026#34;webpack/hot/only-dev-server\u0026#34;, \u0026#34;./src/index.js\u0026#34;, ]; } return config; }, }, devServer: { // allow django host, in case you use custom domain for django app allowedHosts: [\u0026#34;localhost\u0026#34;], }, }; Arrancar\r#\r\rPara arrancar React a nivel de la carpeta de la aplicación de React hay que lanzar el siguiente comando:\nnpm start Esto arranca el servidor React en el puerto 3000. Y lo que va hacer es sobreescribir el fichero index.html de la carpeta de templates para que se vea lo que hay en el servidor de React.\nEs decir. Que si accedemos a http://localhost:8000/ veremos lo mismo que si accedemos a http://localhost:3000/\n** NOTA IMPORTANTE INSTALACIÓN **\nEsto tal cual da un error porque el módulo html-webpack-plugin requiere del webpack instalado al mismo nivel que el plugin, es decir, a nivel de proyecto. Pero he hecho tantas instalaciones a nivel global, como local, que ahora no sé como me funciona.\nPaso para desplegar en producción\r#\r\rEsta explicado en este artículo pero pongo aquí el texto por si el artículo se pierde cuando lo necesite:\nnpm run build All js files should be ready in reactjs/dist. Check templates/react.html to see the changes. Now we need to tell django go to collectstatic Edit settings.py 1 2 3 4 STATIC_ROOT = os.path.join(BASE_DIR, \u0026#39;static_dist\u0026#39;) STATICFILES_DIRS = [ os.path.join(BASE_DIR, \u0026#39;reactjs/dist\u0026#39;), ] Restart django app and check again http://127.0.0.1:8000/, react js is served by django static commit dist or not! it doesn’t matter We will want to fix reactjs/.gitignore if we want to commit dist, otherwise our CI server must do yarn build first, then collectstatic Now we have an app with react at frontend, backend by django, next step is add django rest framework,…etc Github: https://github.com/tamhv/djangoreact Nota: he cambiado el modo es que se producen los archivos para su distribución\n"});index.add({'id':42,'href':'/docs/python/sentencias/condiciones/','title':"Condiciones",'content':"Condiciones\r#\r\rLas condiciones son como en la mayoria de lenguajes\n x == y \u0026ndash;\u0026gt; x es igual a y x != y \u0026ndash;\u0026gt; x es distinto de y x \u0026gt; y \u0026ndash;\u0026gt; x es mayor que y x \u0026lt; y \u0026ndash;\u0026gt; x es menor que y x \u0026gt;= y \u0026ndash;\u0026gt; x es mayor o igual que y x \u0026lt;= y \u0026ndash;\u0026gt; x es menor o igual que y x is y \u0026ndash;\u0026gt; x es lo mismo que y x is not y \u0026ndash;\u0026gt; x no es lo mismo que y Operadores lógicos son: and(y), or(o) y not(no) IN Devuelve si un cadena esta dentro de otra. El ejemplo de este operador se encuentra en la página de cadenas  Para hacer bucles condiciones esta la sentencia IF. Ejemplo\nEjemplo sencillo:\nif 34 == 45: print(\u0026#34;Son iguales\u0026#34;) else: print(\u0026#34;No son iguales\u0026#34;) Resultado:\nNo son iguales\rHay una sentencia que es PASS que se pondría después del IF o del ELSE para que no de error. Ejemplo:\nif 34 \u0026lt;= 45: pass No hace nada. Útil cuando todavía no saber que poner. En otros lenguaje se pondría un comentario pero aquí esto da error:\nif 34 \u0026lt;= 45: #no hago nada Da este error::\nFile \u0026quot;\u0026quot;, line 2\r#no hago nada\r^\rSyntaxError: unexpected EOF while parsing\rTambién están los *ELSE IF\u0026rdquo; que se escriben con ELIF:\nif 34 == 45: print(\u0026#34;Son iguales\u0026#34;) elif 34 \u0026gt; 45: print(\u0026#34;Es mayor\u0026#34;) else: print (\u0026#34;Es menor\u0026#34;) Resultado:\nEs menor\r"});index.add({'id':43,'href':'/docs/sap/abap/sentencias_74/condiciones/','title':"Condiciones",'content':"Introducción\r#\r\rEn este grupo se irán poniendo las distintas sentencias que nos permiten hacer condiciones.\nEstas sentencias se pueden utilizar para crear nuevas variables o ponerlas como entrada de parámetro en clases.\nEjemplo 1\r#\r\rPermite que una variable se inicialice segun una condicion determinada:\nDATA(lv_updkz) = COND #( WHEN is_header-negotiation_code IS INITIAL THEN zif_rtn_data=\u0026gt;cv_updkz_insert WHEN is_header-has_changed = abap_true THEN zif_rtn_data=\u0026gt;cv_updkz_edit ). Ejemplo 2\r#\r\rUso de SWITCH haciendo una conversion de variable;\n\u0026lt;ls_cond_crud\u0026gt;-value = SWITCH #( \u0026lt;ls_conditions_values_field\u0026gt;-field_type WHEN zif_rtn_negotiation_data=\u0026gt;cs_field_types-flex_group THEN \u0026lt;field\u0026gt; LSE CONV string( \u0026lt;field\u0026gt; ) ). Ejemplo 3\r#\r\rDATA lv_valor TYPE string. DO 10 TIMES. lv_valor = COND #( LET x = sy-index MOD 2 IN WHEN x = 0 THEN |par| ELSE |impar| ). WRITE:/ lv_valor. ENDDO. "});index.add({'id':44,'href':'/docs/python/configuracion_pylint/','title':"Configuración Pylint",'content':"Introducción\r#\r\rPylint es una librería que se usa para verificiar que el código de Python que estamos escribiendo, para el entorno que tengamos activado, sea correcto.\nPero hay veces que hay que tocar la configuración de Pylint para afinarlo.\nEvitar errores de librerías que no existen\r#\r\rHay casos que Pylint dice que una librería no existe pero realmente si que existe. En mi caso me ha ocurrido al usar librerías creadas en el raíz del proyecto de VS Code, dentro de un proyecto de Django. Pero luego, en tiempo de ejecución no da problemas.\nEl motivo es que esa librería propia no esta en las variables de entorno y esto provoca que Pylint piense que no existe.\nLa solución es modificar el fichero .pylintrc que esta en el raíz del proyecto de VS Code. Si no existiera se puede crear mediante el siguiente comando:\npylint --generate-rcfile \u0026gt; .pylintrc\rUna vez en el archivo hay que localizar la variable init-hook y añadir lo siguiente:\ninit-hook='import sys; sys.path.append(\u0026quot;K:\\github\\ocr_invoices\u0026quot;)'\rDonde se le añade el directorio del proyecto a las variables de entorno. De esta manera desaparecen ese tipo de errores.\n"});index.add({'id':45,'href':'/docs/python/framework/django/configuracion_url_aplicacion/','title':"Configuración URLs aplicación",'content':"Introducción\r#\r\rCada aplicación dentro de un proyecto en Django tiene su propio archivo de URLs, urls.py donde se van incluyendo las vistas que se van añadiendo. Aquí explicaremos como configurar dicho archivo tanto a nivel de proyecto como de aplicación.\nHay que saber que el punto de acceso de los servicios será el archivo urls.py a nivel de proyecto. Por ello yo lo que hago es:\n Uso el fichero urls.py a nivel de aplicación para indicar sus vistas Dentro del fichero urls.py a nivel de proyecto incluyo el fichero homólogo a nivel de aplicación.  Esta solución la veo la más limpia y estructurada.\nURLs a nivel de aplicación\r#\r\rSi se usan vistas de tipo ViewSet la manera más simple de hacerlo es a través de los routers:\nfrom rest_framework import routers from . import views router = routers.DefaultRouter() router.register(r\u0026#39;invoice/image\u0026#39;, views.invoiceImageViewSet) URls a nivel de proyecto\r#\r\rEl fichero actual que tengo es este:\nfrom django.contrib import admin from django.urls import path, include from invoices import urls from django.conf import settings from django.conf.urls.static import static from . import views urlpatterns = [ path(\u0026#39;admin/\u0026#39;, admin.site.urls), path(r\u0026#39;api/\u0026#39;, include(urls.router.urls)), path(r\u0026#39;login\u0026#39;, views.LoginView.as_view(), name=\u0026#39;login\u0026#39;), path(r\u0026#39;logout\u0026#39;, views.LogoutView.as_view(), name=\u0026#39;logout\u0026#39;) ] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) el path de admin ya viene por defecto y le he añadido:\n en el path api será el punto de acceso a las vistas de las distintas aplicaciones que pueda ir creando Los servicios de login y logout al ser globales al proyecto están definidos directamente en el proyecto. Se le añade los path para los archivos estáticos.  URL de llamada\r#\r\rLa URL para llamar a un servicio de la aplicación como el configurado sería:\nhttp://localhost:8000/api/invoice/image/\rPara llamar a un servicio del proyecto sería:\nhttp://localhost:8000/login\r"});index.add({'id':46,'href':'/docs/python/sentencias/constantes_internas/','title':"Constantes internas",'content':"Iteracciones\r#\r\rLas constantes internas, son eso, constantes propias de Python que tienen un valor fijo y que pueden servir para inicializar valores.\n None \u0026ndash;\u0026gt; Sirve para indicar que una variable esta vacia.  "});index.add({'id':47,'href':'/docs/sap/abap/sentencias_74/conversiones/','title':"Conversiones",'content':"Introducción\r#\r\rEn este grupo se irán poniendo las distintas sentencias que nos permiten convertir valores de un campos a otro.\nEstas sentencias se pueden utilizar para crear nuevas variables o ponerlas como entrada de parámetro en clases.\nEjemplo 1\r#\r\rLo que antes se hacia:\nDATA lo_elemdesc type ref to cl_abap_elemdescr. Lo_elemdesc ?= cl_abap_typedescr=\u0026gt;describe_by_data( iv_abap_value. Ahora se puede hacer en una sola línea: DATA(lo_elemdesc) = CAST cl_abap_elemdescr( cl_abap_typedescr=\u0026gt;describe_by_data( iv_abap_value ) ). Ejemplo 2\r#\r\rConvertir los valores de un select para adaptarlos a la variable donde se van a guardar:\nSELECT SINGLE CAST( months_past AS INT2 ), CAST( months_future AS INT2 ) INTO (@ev_months_past, @ev_months_future) FROM zrtn_t_0037 WHERE vkorg = @iv_vkorg AND vtweg = @iv_vtweg AND spart = @iv_spart. "});index.add({'id':48,'href':'/docs/sap/abap/bapis/ventas/crear_pedido_ventas_basico/','title':"Creación pedido de ventas",'content':"Ejemplo\r#\r\rEste es un ejemplo muy básico de creación de un pedidos ventas.\n\u0026#34; Tipos de datos TYPES: tt_bapi_item TYPE STANDARD TABLE OF bapisditm WITH EMPTY KEY. TYPES: tt_bapi_itemx TYPE STANDARD TABLE OF bapisditmx WITH EMPTY KEY. TYPES: tt_bapi_partners TYPE STANDARD TABLE OF bapiparnr WITH EMPTY KEY. TYPES: tt_bapi_schedules TYPE STANDARD TABLE OF bapischdl WITH EMPTY KEY. TYPES: tt_bapi_schedulesx TYPE STANDARD TABLE OF bapischdlx WITH EMPTY KEY. DATA lv_ebeln TYPE vbak-vbeln. \u0026#34; Estructuras de la BAPI DATA ms_bapi_header TYPE bapisdhd1. DATA ms_bapi_headerx TYPE bapisdhd1x. DATA mt_bapi_item TYPE tt_bapi_item. DATA mt_bapi_itemx TYPE tt_bapi_itemx. DATA mt_bapi_partners TYPE tt_bapi_partners. DATA mt_bapi_schedules TYPE tt_bapi_schedules. DATA mt_bapi_schedulesx TYPE tt_bapi_schedulesx. DATA mt_bapi_return TYPE bapiret2_t. \u0026#34; Datos de cabecera del pedido ms_bapi_header-doc_type = \u0026#39;ZP06\u0026#39;. \u0026#34; CLase pedido ms_bapi_headerx-doc_type = abap_true. ms_bapi_header-po_method = \u0026#39;CSTK\u0026#39;. \u0026#34; Motivo de pedido ms_bapi_headerx-po_method = abap_true. ms_bapi_header-sales_org = \u0026#39;US01\u0026#39;. \u0026#34; Org. ventas ms_bapi_headerx-sales_org = abap_true. ms_bapi_header-distr_chan = \u0026#39;10\u0026#39;. \u0026#34; Canal ms_bapi_headerx-distr_chan = abap_true. ms_bapi_header-division = \u0026#39;PP\u0026#39;. \u0026#34; Sector ms_bapi_headerx-division = abap_true. ms_bapi_header-req_date_h = sy-datum. \u0026#34; Fecha preferente entrega ms_bapi_headerx-req_date_h = abap_true. ms_bapi_header-war_date = sy-datum. \u0026#34; Fecha de garantia. Es opcional ms_bapi_headerx-war_date = abap_true. ms_bapi_header-purch_no_c = \u0026#39;Pedido cliente\u0026#39;. ms_bapi_headerx-purch_no_c = abap_true. ms_bapi_headerx-updateflag = \u0026#39;I\u0026#39;. \u0026#34; Se va insertar el pedido \u0026#34; Posiciones del pedido DATA(lv_cont) = 1. APPEND INITIAL LINE TO mt_bapi_item ASSIGNING FIELD-SYMBOL(\u0026lt;ls_bapi_items\u0026gt;). APPEND INITIAL LINE TO mt_bapi_itemx ASSIGNING FIELD-SYMBOL(\u0026lt;ls_bapi_itemsx\u0026gt;). \u0026lt;ls_bapi_items\u0026gt;-itm_number = lv_cont. \u0026#34; Posición del pedido \u0026lt;ls_bapi_itemsx\u0026gt;-itm_number = lv_cont. \u0026lt;ls_bapi_itemsx\u0026gt;-updateflag = \u0026#39;I\u0026#39;. \u0026#34; Se va insertar la posición \u0026lt;ls_bapi_items\u0026gt;-material = \u0026#39;000000000065032815\u0026#39;. \u0026lt;ls_bapi_itemsx\u0026gt;-material = abap_true. \u0026lt;ls_bapi_items\u0026gt;-target_qty = \u0026#39;12.000\u0026#39;. \u0026lt;ls_bapi_itemsx\u0026gt;-target_qty = abap_true. \u0026lt;ls_bapi_items\u0026gt;-target_qu = \u0026#39;ST\u0026#39;. \u0026lt;ls_bapi_itemsx\u0026gt;-target_qu = abap_true. \u0026#34; La cantidad que se informa en la posición hay que indicarla también en el reparto APPEND INITIAL LINE TO mt_bapi_schedules ASSIGNING FIELD-SYMBOL(\u0026lt;ls_bapi_schedules\u0026gt;). APPEND INITIAL LINE TO mt_bapi_schedulesx ASSIGNING FIELD-SYMBOL(\u0026lt;ls_bapi_schedulesx\u0026gt;). \u0026lt;ls_bapi_schedules\u0026gt;-itm_number = \u0026lt;ls_bapi_items\u0026gt;-itm_number. \u0026lt;ls_bapi_schedulesx\u0026gt;-itm_number = \u0026lt;ls_bapi_items\u0026gt;-itm_number. \u0026lt;ls_bapi_schedules\u0026gt;-req_qty = \u0026lt;ls_bapi_items\u0026gt;-target_qty. \u0026lt;ls_bapi_schedulesx\u0026gt;-req_qty = abap_true. \u0026#34; Partners \u0026#34; Destinatario APPEND INITIAL LINE TO mt_bapi_partners ASSIGNING FIELD-SYMBOL(\u0026lt;ls_partner\u0026gt;). \u0026lt;ls_partner\u0026gt;-partn_role = \u0026#39;WE\u0026#39;. \u0026lt;ls_partner\u0026gt;-partn_numb = \u0026#39;0003331395\u0026#39;. \u0026#34; Solicitante APPEND INITIAL LINE TO mt_bapi_partners ASSIGNING \u0026lt;ls_partner\u0026gt;. \u0026lt;ls_partner\u0026gt;-partn_role = \u0026#39;AG\u0026#39;. \u0026lt;ls_partner\u0026gt;-partn_numb = \u0026#39;0003331395\u0026#39;. CALL FUNCTION \u0026#39;BAPI_SALESORDER_CREATEFROMDAT2\u0026#39; EXPORTING order_header_in = ms_bapi_header order_header_inx = ms_bapi_headerx IMPORTING salesdocument = lv_ebeln TABLES return = mt_bapi_return[] order_items_in = mt_bapi_item order_items_inx = mt_bapi_itemx order_partners = mt_bapi_partners order_schedules_in = mt_bapi_schedules order_schedules_inx = mt_bapi_schedulesx. \u0026#34; Nota la clase de mensajes \u0026#39;V4\u0026#39; es propia de la BAPI, se recomienda excluirlos para saber \u0026#34; no tener mensajes que no aportan nada READ TABLE mt_bapi_return ASSIGNING FIELD-SYMBOL(\u0026lt;ls_return\u0026gt;) WITH KEY type = \u0026#39;E\u0026#39;. IF sy-subrc NE 0. CALL FUNCTION \u0026#39;BAPI_TRANSACTION_COMMIT\u0026#39;. WRITE:/ \u0026#39;Pedido creado: \u0026#39;, lv_ebeln. ELSE. CALL FUNCTION \u0026#39;BAPI_TRANSACTION_ROLLBACK\u0026#39;. WRITE:/ \u0026#39;Se ha producido el error:\u0026#39;, \u0026lt;ls_return\u0026gt;-message. ENDIF. "});index.add({'id':49,'href':'/docs/python/sentencias/crear_paquetes_librerias/','title':"Crear paquetes y librerías",'content':"Introducción\r#\r\rEn la siguiente pagina se explica como usar librerías o paquetes. Aquí se explica como crearnos nuestras propías librerías y/o paquetes para utilizarlos.\nPara ello voy a usar un ejemplo de una aplicación que estoy montando. No se si será el mejor ejemplo pero leyendo de varios sitios creo que es una buena estructura.\nEjemplo\r#\r\rLa aplicación, que esta en una fase embrionaria, he creado una paquete cuya carpeta principal es lib donde estarán todas las librerias que se usarán en la aplicación. Actualmente tiene los siguientes niveles:\nlib\r|--- __init__.py\r|--- image\r|--- __init__.py\r|--- constants.py\r|--- image.py\r|--- imagePreProcessing.py\rHe optado por crear una carpeta llama lib y dentro de ella ir creando carpetas por cada tipo distinto de librería, quedando esta manera todo queda más organizado. Si hubiese creado cada librería en el directorio raíz de la aplcación quedaría desorganizado y no sabrías para que sirve cada carpeta.\nEn los paquetes en cada carpeta tiene que crearse un archivo init.py para que cuando hagamos el import, python sepa que es una librería. Este fichero puede estar vacio no es necesario introducir nada.\nEn mi caso los tengo informado de la siguiente manera:\nPara el init.py del directorio raíz:\nfrom lib.image import * Para el init.py del directorio lib:\nfrom . import image from . import imagePreProcessing from . import constants Esto nos permite simplificar los import en los programa. Ejemplo, gracías al init.py del directorio raíz se puede hacer esto directamente:\nimport lib . . oImg = lib.image.clImage() Si dicho fichero estuviera vacio el código habría que cambiarlo por esto:\nfrom lib.image import image . . oImg = image.clImage() Incluso es posible delimitar que librerías pueden usarse. Si el init.py del directorio ráiz solo tenemos esto:\nfrom lib.image import imagePreProcessing Este trozo de código nos daría error al no encontrar la librería:\nimport lib . . oImg = lib.image.clImage() Aunque, podríamos solucionarlo accedediendo directamente a la librería que queremos: from lib.image import image\nEn librerías pequeñas simplificar las llamadas pueden ser irrelevante porque mirando los directorios podemos encontrar la librería que nos interesa. Pero en libererías más complejas(solo hay que ver la de numpy) esto no es una opción, y es más comodo informar en los archivos init.py que librerías se importan de manera automática al llamar a la librería. De esta manera se simplifica, y mucho las llamadas.\n"});index.add({'id':50,'href':'/docs/sap/abap/bopf/plantillas_codigo_interno/','title':"Código ejemplo dentro del BOPF",'content':"Objetivo\r#\r\rAquí están las plantillas de ejemplo ABAP que se usarán dentro de las clases que se usarán en validaciones, determinaciones, etc..\nInstanciar clase de mensajes\r#\r\rLos mensajes de los BOPF se recogen en una clase generica que pueda ser usada en las determinaciones, validaciones, etc..\nIF eo_message IS NOT BOUND. \u0026#34; Se instancia la clase de mensajes en caso de no estarlo eo_message = /bobf/cl_frw_factory=\u0026gt;get_message( ). ENDIF. Template código usado en acciones, validaciones y determinaciones\r#\r\rEn cada método de una determinación, validación o acciones siempre hay los siguientes pasos, más o menos:\n Leer los datos según la clave que se recibe por párametro. Este paso siempre hay que hacerlo da igual lo que se haga Procesar los datos Actualizar los datos en el nodo. Este paso es opcional según lo que estemos haciendo  Lectura de datos\r#\r\rDATA lt_data TYPE zatron_bo_i_file_eng_header. \u0026#34; Lectura de datos io_read-\u0026gt;retrieve( EXPORTING iv_node = is_ctx-node_key it_key = it_key IMPORTING et_data = lt_data ). Actualización\r#\r\rLOOP AT lt_data REFERENCE INTO DATA(lo_data). \u0026#34; Actualización io_modify-\u0026gt;update( iv_node = is_ctx-node_key iv_key = lo_data-\u0026gt;key is_data = lo_data ). ENDLOOP. "});index.add({'id':51,'href':'/docs/sap/ui5/desarrollar_vscode/','title':"Desarrollo con VS Code",'content':"Objetivo\r#\r\rPara desarrollar en UI5 he probado tres editores: WebStorm, Sublime y Visual Studio Code.\nWebStorm lo use hace años y me fue tan bien que pague por la licencia. El tema de reconocer configurar, eso hace años, las librerias era con una extensión pero no esta la última versión.\nSublime lo empece a usar hace poco para mi último proyecto y fue el único que era capaz de navegar entre las distintas funcionaes de javascript que iba creando. Pero su entorno no me terminaba de encajar. Y la extensión de librerias de UI5 esta más desfasada.\nVisual Studio Code. Es el que uso ahora, el único pero gordo es que no puedes navegar entre funcionens que esten entre distintos archivos. Esto hace que uses mucho los buscadores o tener buena memoria. Además, tiene extensiones para generar las vistas XML. El entorno lo veo más amigable y es el que me siento a gusto. Además, creo que es el que tiene más recorrido por todo su ecosistema de extensiones.\nEn este artículo explicaría sin mucho detalle, a menos de momento, como he configurado mi entorno para desarrollar en UI5 con VS Code.\nPreparar entorno en VS Code\r#\r\rPrerequisitos\r#\r\rHay que tener instalado el Visual Studio Code y Node.js.\nPaquetes del node.js\r#\r\r Mediante la consola de windows instalar el paquete \u0026ldquo;generator-easy-ui5\u0026rdquo; para simplificar la creación de proyectos nuevos, como vistas, etc.. Se instala con la sentencia:  npm install -g yo generator-easy-ui5 Configuración del VS Code\r#\r\r Instalar la extensión SAPUI5 Extension de VS Code. Esta extensión te permite autocompletar código en vistas XML y controladores. Yo ahora mismo no lo tengo ni instalado porque he ido haciendo las cosas a base de ejemplos de las propias página de UI5 y su API. Si lo instalamos hay que modificar la configuración de las extensión para:   Cambiar el nombre de la carpeta donde esta el codigo fuente. Como se va usar el \u0026ldquo;easy-ui5\u0026rdquo; se cambia el valor \u0026ldquo;src\u0026rdquo; por \u0026ldquo;webapp\u0026rdquo;. Hay que indicarla versión de SAPUI5 que se va a usar, con la versión 1.73.1 funciona sin problemas  Instalar la extensión XML Tools para que te haga pretty pinter el as vistas XML. Si queremos los atributos en cada línea, y no todo en una línea que es la opción por defecto, hay que hay ir a la configuración de las extensión, es la que se llama XML Tools Configuration, y marcar el flag que se llama Split attributes on Format  Preparar el proyecto\r#\r\r  Abrir la consola de Windows e ir a la carpeta donde se alojara el proyecto. IMPORTANTE: No es necesario crear la carpeta del proyecto*\n  Ahora ejecutar el comando:\n  yo easy-ui5 Para hacer la configuración inicial del proyecto. Seguir las preguntas del asistente. Este paso sobrescribe aspectos de la configuración del paso 2. A día de hoy no tengo claro si se puede obviar el paso 2 ya que a lo mejor este paso crea el package.json. Las opciones para mis proyectos personales han sido:\n\rEn las preguntas tenemos la posibilidad de crear la carpeta donde estará el proyecto. Además instalará las dependencias necesarias.\nCosas a tener en cuenta:\n El nombre del namespace se concatena al nombre de la carpeta. Creo que en su momento yo puse un namespace pero le indique que no creará la carpeta.  Como en mi caso voy a llamar a servicios que están en un sistema SAP hay que instalar un proxy para evitar el cross domain. Para eso hay que hacer lo siguiente:   Desde la consola de Windows ejecutar el siguiente comando:  npm install ui5-middleware-simpleproxy --save-dev  Dentro de la carpeta del proyecto hay que modificar el archivo ui5.yaml y poner algo parecido(digo parecido porque cada uno tendrá un servidor distinto):  Srver: server: customMiddleware: - name: ui5-middleware-simpleproxy afterMiddleware: compression mountPath: /sap/opu/odata/sap configuration: baseUri: \u0026#34;http://vhcalnplci.dummy.nodomain:8000/sap/opu/odata/sap\u0026#34; Esto lo que hace que cualquier petición local a /sap/opu/odata/sap se redirigia a la url indicada en baseUri. El nombre del servidor es el SAP pone a sus ABAP Trial Version.\n En el fichero *package.json\u0026rdquo; hacer lo siguiente: Sección devDependencies modificar:  \u0026#34;ui5-middleware-simpleproxy\u0026#34;: \u0026#34;*\u0026#34; Se pone el asterisco en la versión para que sea válido para cualquier versión del paquete.\n Añadir la siguiente sección, yo la he puesto justo después de devDependencies:  \u0026#34;ui5\u0026#34;: { \u0026#34;dependencies\u0026#34;: [ \u0026#34;ui5-middleware-simpleproxy\u0026#34; ] } Ahora abrimos el VS Code y se abre la carpeta del proyecto. Ahora en la parte inferior hay varias pestañas, ir a la de Terminal y pondremos:  npm start Esto arrancará el servidor web con la aplicación si todo esta bien arranca la aplicación template que se ha creado con el yo easy-ui5.\n"});index.add({'id':52,'href':'/docs/sap/abap/bopf/determinaciones/','title':"Determinaciones",'content':"Objetivo\r#\r\rLas determinaciones se usan para rellenar atributos transitorios, son aquellos campos que se definen en la estructura transitoria. Yo los uso para completar descripciones y hacer cálculos.\nYo suelo usar una clase general para agrupar determinaciones de un mismo nodo. En algunos artículos y el propio BOPF aconseja hacer una clase por determinación. Pero lo veo una manera de generar clases que a lo mejor solo tiene cuatro líneas de código. En mi caso con una sola clase he tenido suficiente, ya que muchas veces según la complejidad tengo una clase que gestiona dicho proceso.\nCase para ir llamando a las determinaciones\r#\r\rPongo el case siguiente en el método principal que se llama en la determinación. Y a partir de ese case voy llamando a métodos distintos.\nCASE is_ctx-det_key. WHEN zif_sat_orders_c=\u0026gt;sc_determination-\u0026lt;nodo\u0026gt;-\u0026lt;nombre determinacion\u0026gt; ENCASE Parámetros de un método de la deteminación\r#\r\rEl método de la determinación siempre tiene los mismo parámetros que el método estándar, cuya firma es la siguiente:\nIMPORTING is_ctx TYPE /bobf/s_frw_ctx_det it_key TYPE /bobf/t_frw_key io_read TYPE REF TO /bobf/if_frw_read io_modify TYPE REF TO /bobf/if_frw_modify EXPORTING eo_message TYPE REF TO /bobf/if_frw_message et_failed_key TYPE /bobf/t_frw_key RAISING /bobf/cx_frw. "});index.add({'id':53,'href':'/docs/python/sentencias/diccionario/','title':"Diccionario",'content':"Introducción\r#\r\rSon como las listas o tuplas pero al acceso a sus valores no se accede por un índice, sino que se accede por la clave asociada al valor.\nPor lo que he visto hay diccionarios simples que es una relación clave\u0026lt;-\u0026gt;valor y otros más complejos. De momento explicaré los simples para tener una base\nSimples\r#\r\rCrear un diccionario es muy simple tan sólo hay que usar la siguiente sentencia:\neng2sp = dict() Añadir valores\r#\r\rMétodo simple\r#\r\reng2sp[\u0026#39;one\u0026#39;] = \u0026#39;uno\u0026#39; print(eng2sp) Resultado:\n{'one': 'uno'}\rCon múltiples valores:\neng2sp = {\u0026#39;one\u0026#39;: \u0026#39;uno\u0026#39;, \u0026#39;two\u0026#39;: \u0026#39;dos\u0026#39;, \u0026#39;three\u0026#39;: \u0026#39;tres\u0026#39;} print(eng2sp) Resultado:\n{'one': 'uno', 'two': 'dos', 'three': 'tres'}\rPara ver como se llenaría dinámicamente vamos a contar las letras de una palabra:\npalabra = \u0026#39;ejemplo\u0026#39; d = dict() for c in palabra: if c not in d: d[c] = 1 else: d[c] = d[c] + 1 print(d) Resultado:\n{\u0026#39;e\u0026#39;: 2, \u0026#39;j\u0026#39;: 1, \u0026#39;m\u0026#39;: 1, \u0026#39;p\u0026#39;: 1, \u0026#39;l\u0026#39;: 1, \u0026#39;o\u0026#39;: 1} Usando setdefault\r#\r\rEl setdefault es una propiedad que permite añadir una clave con un valor por defecto en caso de no existir. Es ideal para inicializar un diccionario a partir de una lista:\nblock_num_index = {} for index, value in enumerate(data[\u0026#39;block_num\u0026#39;]): block_num_index.setdefault(value,[]).append(index) Obtener valores\r#\r\rPara obtener valores se puede acceder directamente por índice:\neng2sp = {\u0026#39;one\u0026#39;: \u0026#39;uno\u0026#39;, \u0026#39;two\u0026#39;: \u0026#39;dos\u0026#39;, \u0026#39;three\u0026#39;: \u0026#39;tres\u0026#39;} print(eng2sp[\u0026#39;two\u0026#39;]) Resultado\ndos\rO con la método get que es implicito del objeto dict\neng2sp = {\u0026#39;one\u0026#39;: \u0026#39;uno\u0026#39;, \u0026#39;two\u0026#39;: \u0026#39;dos\u0026#39;, \u0026#39;three\u0026#39;: \u0026#39;tres\u0026#39;} print(eng2sp.get(\u0026#39;two\u0026#39;)) Resultado\ndos\rSi se informa un valor que no existe, ejemplo:\neng2sp = {\u0026#39;one\u0026#39;: \u0026#39;uno\u0026#39;, \u0026#39;two\u0026#39;: \u0026#39;dos\u0026#39;, \u0026#39;three\u0026#39;: \u0026#39;tres\u0026#39;} print(eng2sp.get(\u0026#39;four\u0026#39;)) El resultado es:\nNone\rSi queremos que devuelva un valor en caso de no existe podemos hacer lo siguiente:\neng2sp = {\u0026#39;one\u0026#39;: \u0026#39;uno\u0026#39;, \u0026#39;two\u0026#39;: \u0026#39;dos\u0026#39;, \u0026#39;three\u0026#39;: \u0026#39;tres\u0026#39;} print(eng2sp.get(\u0026#39;four\u0026#39;,0)) El resultado es:\n0\rBuscar datos\r#\r\rPara buscar datos podemos usar el operador in pero este operador nos dirá si existe, o no, una clave. Ejemplo:\neng2sp = {\u0026#39;one\u0026#39;: \u0026#39;uno\u0026#39;, \u0026#39;two\u0026#39;: \u0026#39;dos\u0026#39;, \u0026#39;three\u0026#39;: \u0026#39;tres\u0026#39;} if \u0026#39;one\u0026#39; in eng2sp: print(\u0026#34;existe\u0026#34;) else: print(\u0026#34;No existe\u0026#34;) Resultado:\nexiste\rSi se quiere buscar por valores primero hay que recuperar una lista:\neng2sp = {\u0026#39;one\u0026#39;: \u0026#39;uno\u0026#39;, \u0026#39;two\u0026#39;: \u0026#39;dos\u0026#39;, \u0026#39;three\u0026#39;: \u0026#39;tres\u0026#39;} valores = list(eng2sp.values()) print(valores) Resultado:\n['uno', 'dos', 'tres']\rHay que usar la sentencia list que nos convierte la lista de tipo dict a una lista que se puede usar el in para buscar.\nRecorrer los datos\r#\r\rUn dict se puede recorre usando un for pero hay que recordar que nos irá devolviendo la clave:\neng2sp = {\u0026#39;one\u0026#39;: \u0026#39;uno\u0026#39;, \u0026#39;two\u0026#39;: \u0026#39;dos\u0026#39;, \u0026#39;three\u0026#39;: \u0026#39;tres\u0026#39;} for clave in eng2sp: print(f\u0026#34;Clave: {clave} - Valor: {eng2sp.get(clave)}\u0026#34;) Resultado:\nClave: one - Valor: uno Clave: two - Valor: dos Clave: three - Valor: tres Itertools\r#\r\rPython tiene una librería llamada Itertools que simplifica las operaciones con diccionarios. Para poderlas utilizar tan solo hay que declararlas en nuestro código de la siguiente manerA:\nimport itertools as it En el ejemplo le pongo un alias para que simplificar sus llamadas.\n"});index.add({'id':54,'href':'/docs/python/framework/django/','title':"Django",'content':"Introducción\r#\r\r\rDjango es un framework que permite realizar aplicaciones web de una manera sencilla y rápida.\nA esto hay que sumarle Django Rest Framework, de aquí en adelante DRF, que permite crear servicios REST a través de Django\nEn mi caso quiero usar Django + Django Rest Framework porque me permite publicar servicios web y guardar determinada información en una base de datos (inicialmente estoy usando la que viene por defecto que es SQLite3) de una manera bastante sencilla. Ya que usando las vistas de tipo ViewSet DRF se encarga de gestionar todo el CRUD de un modelo de datos sin añadir nada de código. Además DRF usa la arquitectura MVC. Donde la M son los modelos que se crean en Django. La V son las Views de DRF. Y la C es el serializer de DRF. Esto hace posible crear servicios sin modelo usando solo las Views y Serializers con DRF. Que no sé si esto con Django se puede hacer.\nA nivel de usarlo en nuestras aplicaciones Django se estructura en proyectos y aplicaciones. Y estos se creán dentro del directorio de nuestro proyecto.\nMuchos pasos hay que realizarlos desde el terminal de Windows. Que se puede usar tanto el Anaconda PowerShell como el VSCode si lo hemos abiertos a través del Anaconda Navigator con el entorno que queremos ya preseleccionado. El resultado en ambos casos es el mismo\nInstalar las librerías\r#\r\rDjango se puede instalar a través de Anaconda:\n\rDjango Rest Framework hay que instalarlo a través del PiP:\nconda activate \u0026lt;entorno\u0026gt; pip install djangorestframework Creación del proyecto\r#\r\rDesde el terminal, yo he usado el del VSCode con lo cual lo primero que he hecho es:\nconda activate \u0026lt;entorno\u0026gt; Aunque tecnicamente estaba en el entorno, ya que lo habia abierto desde el Anaconda, con esto me curo en salud.\nIMPORTANTE: Los siguientes pasos hay que hacerlos estando en el directorio de la aplicación. Como he usado el VSCode ya estaba en dicho directorio\nAhora se crea el proyecto:\nD:\\Users\\ivan\\anaconda3\\envs\\ocr\\Lib\\site-packages\\django\\bin\\django-admin.py startproject rest_api # Importante hay que poner la ruta completa donde esta el django-admin.py ** AVISO: Como estaba en el directorio de la aplicación he tenido que poner la ruta completa donde esta el script: djando-admin.py**\nEl nombre del proyecto es rest_api. Con esto nos ha creado un directorio rest_api y a su vez dentro habrá otro directorio con el mismo nombre al cual tenemos que acceder para:\n Modificar el fichero settings.py que se tiene que modificar:   Localizar la variable INSTALLED_APPS y añadir el valor \u0026lsquo;rest_framework\u0026rsquo; \u0026ndash;\u0026gt; Esto nos permite usar la librería para crear servicios Rest Localizar la variable LANGUAGE_CODE y cambiar el valor por \u0026lsquo;es-es\u0026rsquo; Localizar la variable TIME_ZONE y cambiar el valor por Europe/Berlin  Vamos a crear la base de datos inicial. Para ellos hay que volver al directorio del proyecto creado rest_api y lanzar el comando  manage.py migrate El fichero manage.py se va utilizar para muchos procesos.\nDjango a cualquier operación de replicar lo que hagamos en los archivos de los modelos lo llama migración.\nArrancar el servidor\r#\r\rPara arrancar el servidor hay que estar en el directorio del proyecto rest_api y lanzar el siguiente comando\nmanage.py runserver Si accedemos a esta http://localhost:8000/ o http://127.0.0.1:8000/ veremos si todo ha ido bien.\nLa ventana que nos abra con el proceso corriendo no la podemos cerrar ya que sino no funcionara ninguna llamada.\nPanel de administración\r#\r\rLo bueno de Django es que viene con un panel de control para gestionar usuarios, grupos y modelos. Pero primero hay que generar un superusuario. Para ello desde el directorio del proyecto y ejecutar el siguiente script:\nmanage.py createsuperuser Nos pedirá nombre de usuario, mail y password. Para acceder al panel tan solo hay que acceder a http://localhost:8000/admin/\nCrear aplicación\r#\r\rSerá la aplicación donde se crearán los modelos y servicios. Para ellos desde el directorio del proyecto hay que lanzar el siguiente script:\nmanage.py startapp image Donde image es el nombre de nuestra aplicación.\nPara que poder usar la aplicación hay que registrarlo en el fichero settings.py de nuestro proyecto y editarlo. En el fichero hay que localizar la variable INSTALLED_APPS y añadir el valor \u0026lsquo;image.apps.ImageConfig\u0026rsquo;. Donde image es el directorio que ha creado el proyecto, apps es el fichero apps.py y ImageConfig es el nombre del clase dentro del fichero apps.py.\nCom replicar los modelos en la base de datos\r#\r\rComo crear modelos hay multitidud de manuales y guias explicandolos. Pero aquí explicamos los pasos para sincronizarlos con la base de datos:\nimport uuid from django.db import models class Image(models.Model): id = models.UUIDField(auto_created=True, primary_key=True, default=uuid.uuid4, editable=False) content = models.BinaryField() filename = models.TextField(max_length=100, default=\u0026#39;\u0026#39;) Una vez creando el modelo hay que lanzar dos script para que sincronicen con nuestro base de datos:\n Crear una foto de los cambios en los modelos de la aplicación  manage.py makemigrations image La fotos de los cambios replicarlos a la base de datos:  manage.py migrate Si queremos borrar un modelo tan solo hay que borrarlo del fichero models.py y realizar los dos pasos anteriores para sincronizarlos.\nComo administrar los modelos creados\r#\r\rSi accede a la página de administración los modelos no aparecen automáticamente, se tienen que registrar. Para ello hay que ir al directorio donde esta la aplicación y editar el fichero admin.py y añadir la siguiente línea, o líneas según modelos creados:\nadmin.site.register(Image) Donde Image es el nombre del modelo(es una clase) que esta definida en el archivo models.py del mismo directorio.\nArtículos sobre Django\r#\r\r\r\rAutentificación por sesión\r\r\rAutentificación por sesión\r\r\rComo utilizar React como frontend\r\r\rComo utilizar React como frontend\r\r\rConfiguración URLs aplicación\r\r\rConfiguración URLs aplicación\r\r\rEquivalencia métodos HTTP y DRF\r\r\rEquivalencia métodos HTTP y Djando Rest Framework\r\r\rIncluir librerías propias\r\r\rIncluir librerías propias\r\r\rServicios con ficheros\r\r\rEnvio, recepción y borrado de ficheros\r\r\r"});index.add({'id':55,'href':'/docs/','title':"Docs",'content':""});index.add({'id':56,'href':'/docs/python/machine_learning/arbol_decisiones/ejemplo_basico/','title':"Ejemplo básico",'content':"Introducción\r#\r\rEjemplo extraído del video Machine Learning episodio 3. Árboles de Decisiones\nEl ejemplo esta creado el Jupiter Notebool y se irá poniendo el código de las distintas celdas\nCódigo\r#\r\r\rCódigo fuente descargable\nCarga de librerías\n# Clasificador de arbol de decisiones from sklearn.tree import DecisionTreeClassifier # Dos set de datos uno de cancer y otro de iris from sklearn.datasets import load_breast_cancer, load_iris # Librería para separar datos de entrenamiento y testing from sklearn.model_selection import train_test_split # Estos últimos son para poder mostrar gráficamente el arbol de decisiones from sklearn.tree import export_graphviz import graphviz import matplotlib.pyplot as plt import numpy as np Datos para el ejemplo\n# Carga de los datos IRIS a una variable iris = load_iris() Split de datos de entrenamiento y test\n# Las variables _ent son las que se usarán para entrenamiento y testing # Target son las etiquetas y data y son las características X_ent, X_test, y_ent, y_test = train_test_split(iris.data, iris.target) # Variable con el arbol de decisiones arbol = DecisionTreeClassifier() Se entrena a algoritmo\n# Se le pasan los datos de entrenamiento y los valores guia arbol.fit(X_ent, y_ent) Resultado del entrenamiento con los datos de test\n# Con esto se averigua el % de aprendizaje. # Devuelve un 97% que no esta nada mal arbol.score(X_test, y_test) Resultado del entrenamiento con los datos del entrenamiento. Se produce sobreajuste que no es bueno ya que puede provocar falsos positivos a futuro.\n# Se hace lo mismo sobre los datos de entrenamiento. En este caso devuelve el 100%. # Parece ser que devuelva un 100% en los datos de entrenamiento no es muy bueno arbol.score(X_ent, y_ent) Se genera un fichero para visualizar el arbol de deciciones\n# Genera un archivo .dot con los datos del arbol. # Al ejecutarlo aparece el archivo en el mismo directorio donde esta este script export_graphviz(arbol, out_file=\u0026#39;arbol.dot\u0026#39;, class_names=iris.target_names, feature_names=iris.feature_names,impurity=False,filled=True) Visualización del fichero\n# Abre el fichero generado en el paso anterior, se lee en una variable with open(\u0026#39;arbol.dot\u0026#39;) as f: dot_graph=f.read() # Se muestra el arbol de decisiones graphviz.Source(dot_graph) Muestra un gráfico de barra para ver cual es la carácterística más importante en la clasificación\n# Esto lo que nos dice que tan de importante es una característica especifica # para ayudarnos a definir a la clase a la que pertenece. # En el gráfico se mostrará la característica más importante es la longitud del petalo # Caract obtenemos el número de características que tiene la tupla caract = iris.data.shape[1] # \u0026#34;feature_importances_\u0026#34; es la importancia de nuestras características # Indica que va crear un gráfico de barras donde: # El primer parámetro \u0026#34;y\u0026#34; tendrá cuatro registros, el número de características # El segundo parámetro son los valores que se hará las características plt.barh(range(caract),arbol.feature_importances_) # Se le informan las etiquetas. El primer parámetro es un array en # blanco(np.arange crea un array en base al número pasado). El segundo # parámetro son los textos de la etiqueta plt.yticks(np.arange(caract),iris.feature_names) # Se le pasa el texto para el eje X e Y plt.xlabel(\u0026#39;Importancia de las características\u0026#39;) plt.ylabel(\u0026#39;Característica\u0026#39;) plt.show() Se cambia los parámetros del árbol para mejorarlo.\n# Se van ajustar los parámetros del clasificador para mejorar la precisión. # max_depth le indica cuantos niveles en la clasificación le permitimos. # Si inicialmente erán 6 niveles encontrados, según la gráfica ahora serán 3. arbol = DecisionTreeClassifier(max_depth=3) De nuevo al entrenamiento\n# Se vuevle a entrenar arbol.fit(X_ent, y_ent) Resultados del entrenamiento\n# El resultado con los datos de test son del 94% arbol.score(X_test,y_test) # El resultado con los datos de entrenamiento es del 97%, antes era del 100% arbol.score(X_ent, y_ent) Ejemplo de como hace el proceso de clasificación el arbol\n# Ejemplo de como se muestra gráficamente como # realiza la clasificación el algoritmo. Se usa la # librería Matplotlib para visualizarlo n_classes = 3 plot_color = \u0026#34;bry\u0026#34; plot_step = 0.2 for pairidx, pair in enumerate([[0,1], [0, 2],[0, 3], [1, 2], [1, 3], [2, 3]]): X = iris.data[:, pair] y = iris.target # Entrenar algoritmo # Sin usar la opción max_depth=3 en la gráfica se ven unas rayas, como las rojas, # que son debidas a los sobreajustes. Esta raya le sirve al modelo para clasificar # a futuro uno de los elementos. Entonces en un futuro cuando un elemento caigan # en esas rayas lo va a clasificar de manera incorrecta. # Si se pone ela opción max_depth=3 esas rayas ya no se ven. Ya no hay sobreajuste # en el entrenamiento clf = DecisionTreeClassifier(max_depth=3).fit(X,y) plt.subplot(2, 3, pairidx + 1) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) cs = plt.contour(xx, yy, Z, cmap=plt.cm.Paired) plt.xlabel(iris.feature_names[pair[0]]) plt.ylabel(iris.feature_names[pair[1]]) plt.axis(\u0026#34;tight\u0026#34;) # Plot the training point for i, color in zip(range(n_classes), plot_color): idx = np.where(y == i) plt.scatter(X[idx, 0], X[idx, 1], c = color, label = iris.target_names[i], cmap=plt.cm.Paired) plt.axis(\u0026#34;tight\u0026#34;) plt.suptitle(\u0026#34;Ejemplo de clasificador de arboles\u0026#34;) plt.legend() plt.show() En esta imagen se ve como se ha realizado la clasificación:\n\rLos recuadros azules son los sobreajustes que produce el algoritmo al intentar clasificar alguno de los elementos, pero que esto a futuro dará falsos positivos.\nAjustando el parámetro max_depth=3 vemos que algunas de las líneas desparecen:\n\rEn el siguiente artículo se intenta explica un poco mejor que es el sobreajuste.\n"});index.add({'id':57,'href':'/docs/python/machine_learning/tensorflow/clasificador_imagenes_ejemplo/','title':"Ejemplo clasificador de imagenes",'content':"Introducción\r#\r\rEjemplo extraído de los videos (pt. 1/2) Cómo hacer un clasificador de imagenes desde cero con Tensorflow y (pt. 2/2) Cómo hacer un clasificador de imagenes desde cero con Tensorflow\nY el modelo de datos de entrenamiento se puede en el repositorio del ejemplo de Alex Puig\nEl ejemplo consta de dos archivos: el primero sirve para entrenar el algoritmo y el segundo para la predicción. Ambos ejemplos son ejemplo en código fuente de Python\nNOTA 1: Debido a una descompensación en el número de imagenes la tasa de acierto no es muy alta. Esto lo comenta Alex en algún comentario en la segunda parte del video\nNOTA 2: El código del video esta basado en Tensor Flow 1.0 usando Keras. Pero el Tensor Flow que se ha instalado es la version 2.x, la llamaremos TF2.0 o TF20 . Por lo tanto el código ha sido adaptado para funcione en la versión TF2.x\nCódigo entrenamiento\r#\r\r\rCódigo fuente descargable\nimport sys import os # Ajusta el nivel de log que se muestra en consola. Son logs que son informativos # y generán las propias librerías. Los valores posibles son: # Level | Level for Humans | Level Description # -------|------------------|------------------------------------ # 0 | DEBUG | [Default] Print all messages # 1 | INFO | Filter out INFO messages # 2 | WARNING | Filter out INFO \u0026amp; WARNING messages # 3 | ERROR | Filter out all messages os.environ[\u0026#39;TF_CPP_MIN_LOG_LEVEL\u0026#39;] = \u0026#39;3\u0026#39; import tensorflow as tf import math # Código que soluciona el error: Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR # debido a temas de memoria. physical_devices = tf.config.experimental.list_physical_devices(\u0026#39;GPU\u0026#39;) assert len(physical_devices) \u0026gt; 0, \u0026#34;Not enough GPU hardware devices available\u0026#34; config = tf.config.experimental.set_memory_growth(physical_devices[0], True) # Path donde están las imagenes de entrenamiento y test data_entrenamiento = \u0026#34;K:/github/python_test/tensorflow/clasificador_imagenes/data/entrenamiento/\u0026#34; #data_entrenamiento = \u0026#34;./data/entrenamiento/\u0026#34; data_validacion = \u0026#34;K:/github/python_test/tensorflow/clasificador_imagenes/data/validacion/\u0026#34; #data_validacion = \u0026#34;./data/validacion/\u0026#34; # Parametros de la red neuronal # Número de veces que se iteran los datos en el entrenamiento epocas = 20 # Tamaño de las imagenes longitud, altura = 100, 100 # Numero de imagenes que se procesan en cada paso batch_size = 32 # Al final de cada época se harán 300 pasos con los datos de validación # para ir viendo como de bien va aprendiendo validation_steps = 32 # Número de convoluciones o de capas(profundidad) que tendra la iamagen # Primera convolucion serán 32 filtrosConv1 = 32 # Segunda convolucion serán 64 filtrosConv2 = 64 # Anchura y altura que va a procesar en cada convolucion. tamano_filtro1 = (3, 3) tamano_filtro2 = (2, 2) # Tamaño que se va usar en el maxpooling tamano_pool = (2, 2) # Número de clases que hay en el set de datos: gatos, perros y gorilas clases = 3 # Es el learning rate. Es decir, cuanto de grandes van a ser los ajustes en la red # para ajustarse para buscar una solución óptima. lr = 0.0005 # Preprocesamiento de la imagen # Paso necesario para poder enviar las imagenes a la red # Creamos un generador que se indicará como tiene que procesarlas # El rescale= 1./255 -\u0026gt; Cada uno de los pixeles tiene un rango de 0 a 255 (escale RGB). # Con esto estos pixeles se convierte de 0 a 1 para que sea más eficiente el algoritmo. # El shear_range=0.2 -\u0026gt; Va a inclinar de manera aleatoria la imagen para que el algoritmo no se piense # que la imagen va a estar recta. # El zomm_range=0.2 -\u0026gt; Va hacer zoom de manera aleatoria para que el algoritmo aprenda a que # la imagen no tiene porque estar completa # horizontal_flip=True -\u0026gt; Va a invertir de manera aleatoria la imagen. Para que el algoritmo aprenda direccionabilidad entrenamiento_datagen = tf.keras.preprocessing.image.ImageDataGenerator( rescale=1. / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) # Para el set de datos solo se hace el rescalado. Ya que se quiere comparar imagenes reales. test_datagen = tf.keras.preprocessing.image.ImageDataGenerator( rescale=1. / 255) # Se genera la variable donde se le indicará donde están los datos que tiene que leer. # Se le pasa: # - Ruta donde se leerán las imagenes, incluidas sus subdirectorios # - Altura y longitud de la imagen # - Tamaño del batch # - El tipo de clasificación, que indicará que las etiquetas son los directorios donde # están las imagenes entrenamiento_generador = entrenamiento_datagen.flow_from_directory( data_entrenamiento, target_size=(altura, longitud), batch_size=batch_size, class_mode=\u0026#39;categorical\u0026#39;) validacion_generador = test_datagen.flow_from_directory( data_validacion, target_size=(altura, longitud), batch_size=batch_size, class_mode=\u0026#39;categorical\u0026#39;) # Las etiquetas las genera de manera ordenada. Cuando se haga la predicción hay que saber la relación entre número que y # devolverá y su etiqueta correspondiente. Esto se puede saber ordenando la carpeta de window y viendo el orden. O bien # hacer lo siguiente e imprimirlo en pantalla: label_map = (entrenamiento_generador.class_indices) print(label_map) # Creación de la red convolucional cnn = tf.keras.Sequential() # Primera capa de la red que será la de la convolución. Donde se le pasa: # - Filtro que hemos configurado # - Tamaño del filtro que hemos configurado # - padding=\u0026#34;same\u0026#34; es como se comporta el filtro en las esquinas # - input_shape es el tamaño de la imagen que se le va a pasar a la capa. # el 3 es por las capas RGB de la imagen # - activation es la función de activación cnn.add(tf.keras.layers.Conv2D(filtrosConv1, tamano_filtro1, padding=\u0026#34;same\u0026#34;, input_shape=(longitud, altura, 3), activation=\u0026#39;relu\u0026#39;)) # Segunda capa para realizar el maxpooling donde se le pasa el tamaño definido # en la variable cnn.add(tf.keras.layers.MaxPool2D(pool_size=tamano_pool)) # Tercera capa que será la segunda convolución. # En esta capa no se le pone en input_shape porque esto se hace en la primera porque recibe # las imagenes tal cual. En esta segunda convolución el tamaño de la imagen ya ha sido # ajustada en las capas anteriores. cnn.add(tf.keras.layers.Conv2D(filtrosConv2, tamano_filtro2, padding=\u0026#34;same\u0026#34;)) # Cuarta capa que será el pooling de la convolución anterior cnn.add(tf.keras.layers.MaxPool2D(pool_size=tamano_pool)) # Ahora se realiza la configuración de la clasificación # Cuando llegue aquí la imagen será muy pequeña pero con muchas capas. Lo que hace flatten # es convertirlo a una sola dimensión. Es decir, la aplana cnn.add(tf.keras.layers.Flatten()) # Esto lo que hace es crear una capa \u0026#34;normal\u0026#34; donde donde va estar conectada con la capa # anterior que tiene la información aplanada. # Esta capa va a tener 256 neuronas y la áctivación va ser la \u0026#34;relu\u0026#34; cnn.add(tf.keras.layers.Dense(256, activation=\u0026#39;relu\u0026#39;)) # Esto lo que hace es apagar el 50% de las neuronas, aleatoriamente, de la capa anterior en cada paso. Esto # se hace va evitar los sobreajustes. Si todas las neuronas están activas en todos los pasos # es posible que la red aprende un camino específico para clasificar. Haciendo esto, va aprender caminos # alternativos para la clasificación, evitando el sobreajuste. cnn.add(tf.keras.layers.Dropout(0.5)) # La última capa de la red para a tener 3 neuronas, lo indicado en la variable clases(perro, gato y ave). # Esta función es la que va a decir el % de probabilidad a que clase pertenece. cnn.add(tf.keras.layers.Dense(clases, activation=\u0026#39;softmax\u0026#39;)) # Este es el paso de configuración del aprendizaje del modelo # El parámetro \u0026#34;loss\u0026#34; es la función de perdida, es decir, es lo que indica que tan bien esta aprendiendo # En el parámetro \u0026#34;optimizer\u0026#34; es el algoritmo que se usa para el calculo de peso y el gradiente descendiente. # El algoritmo será el \u0026#34;Adam\u0026#34; y su learning rate será el indicado por la variable global # El parámetro \u0026#34;metrics\u0026#34; que será el cual vamos a estar optimizando será \u0026#34;accuracy\u0026#34; que indica el % de que tal # esta aprendiendo el algoritmo. cnn.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=tf.keras.optimizers.Adam(lr=lr), metrics=[\u0026#39;accuracy\u0026#39;]) # Los pasos que se hará por época depende del tamaño de datos y el valor del batch_size. Ya que si se pone un dato # menor no se procesarán todos los datos y un dato superior dará el error: # \u0026#34;Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches\u0026#34; # por eso, hay que aplicar la siguente formula para que los pasos sea los justos para procesar toda la información pasos = math.ceil( ( len(entrenamiento_generador.filenames) / batch_size ) ) # Se entrena el algoritmo pasandole: # Datos de entrenamiento # Pasos por cada epocas. Es la operación de dividir el tamaño total de datos / # Número de epocas # Datos de validacion # Número de pasos con los datos de validación # Esto lo que va hacer es en cada epoca va hacer mil iteracciones para entrenar el algoritmo. Cuando termine # esa epoca va hacer doscientos pasos de validación para como de vien va el entrenamiento. Y luego volverá a # procesar la siguiente epoca. cnn.fit( entrenamiento_generador, steps_per_epoch=pasos, epochs=epocas, validation_data=validacion_generador, validation_steps=validation_steps ) # El resultado del entrenamiento se guardará en el directorio \u0026#34;modelo\u0026#34;, que se creará si no existe previamente #target_dir = \u0026#39;./modelo/\u0026#39; target_dir = \u0026#39;K:/github/python_test/tensorflow/clasificador_imagenes/modelo\u0026#39; if not os.path.exists(target_dir): os.mkdir(target_dir) # Grabación del modelo #cnn.save(\u0026#39;./modelo/modelo.h5\u0026#39;) cnn.save(\u0026#39;K:/github/python_test/tensorflow/clasificador_imagenes/modelo/modelo.h5\u0026#39;) # Grabación de los pesos de cada una de las capas #cnn.save_weights(\u0026#39;./modelo/pesos.h5\u0026#39;) cnn.save_weights(\u0026#39;K:/github/python_test/tensorflow/clasificador_imagenes/modelo/pesos.h5\u0026#39;) Código de predicción\r#\r\r\rCódigo fuente descargable\nimport os os.environ[\u0026#39;TF_CPP_MIN_LOG_LEVEL\u0026#39;] = \u0026#39;3\u0026#39; import numpy as np import tensorflow as tf # Altura y longitud de la imagen que tiene que ser la misma que se ha usado para el entrenamiento longitud, altura = 100, 100 # Ruta donde esta el modelo y los pesos modelo = \u0026#39;K:/github/python_test/tensorflow/clasificador_imagenes/modelo/modelo.h5\u0026#39; pesos_modelo = \u0026#39;K:/github/python_test/tensorflow/clasificador_imagenes/modelo/pesos.h5\u0026#39; # Se carga el modelo keras cnn = tf.keras.models.load_model(modelo) # Se carga los pesos del modelo cnn.load_weights(pesos_modelo) # Función que se le pasa el path de la imagen y los dira que es. def predict(file): # Se carga la imagen con la longitud y altura definida x = tf.keras.preprocessing.image.load_img(file, target_size=(longitud, altura)) # Convierte la imagen en una array de valores x = tf.keras.preprocessing.image.img_to_array(x) # Lo que hace es añadir una nueva dimensión en el eje 0 x = np.expand_dims(x, axis=0) # Hacemos la predicción en base a la imagen pasada convertida en una array. # Esto devuelve una array de 2 dimensiones tal que así: [[1,0,0]] array = cnn.predict(x) # Recogemos el valor de la dimensión 0 que es la que obtiene el resultado result = array[0] # Se mira en que indice estará el valor más alto. Como el modelo trabaja con 0 y 1 (por ello # se divide la imagen en 255). Donde este el 1, será la categoria que se corresponde a la imagen answer = np.argmax(result) if answer == 0: print(\u0026#34;pred: Gato\u0026#34;) elif answer == 1: print(\u0026#34;pred: Gorila\u0026#34;) elif answer == 2: print(\u0026#34;pred: Perro\u0026#34;) return answer predict(\u0026#39;K:/github/python_test/tensorflow/clasificador_imagenes/cat.4928.jpg\u0026#39;) "});index.add({'id':58,'href':'/docs/python/machine_learning/vectorizacion/conversion_palabra_a_numeros/','title':"Ejemplo de conversión de palabras a números",'content':"Introducción\r#\r\rEjemplo extraído del video Machine Learning episodio 7. Vectorización.\nPartimos de las siguiente dos frases:\n Vectorizar el texto es necesario en machine learning Machine learning es una gran herramienta  El primer pasar es tokenizar. Tokenizar es dividir el texto en tokens o palabras.\nLos token los obtendremos de las palabras no repetidas que son:\n Vectorizar el texto es necesario en machine learning una gran herramienta  Nota: Se cuenta como misma palabra \u0026ldquo;Machine\u0026rdquo; que \u0026ldquo;machine\u0026rdquo;\nLo siguiente es contar el número de tokens obtenidos que son 11.\nA continuación se va a crear dos vectores, uno por cada frase, que tendrá como tamaño el número de palabras o tokens obtenidos. Y a cada posición se le va asignar el número de veces que se repite una palabra en la frase según su posición en el token obtenido no en la frase.\nEl vector de la primera fase sería:\n [1,1,1,1,1,1,1,1,0,0,0] \u0026ndash;\u0026gt; Las primeras 8 palabras de la frase solo aparece una vez en el token obtenido, mientras que las tres últimas no aparecen.  El vector de la segunda frase sería:\n [0,0,0,1,0,0,1,1,1,1,1] \u0026ndash;\u0026gt; En la 4 posición aparece un 1 es porque el token número 4 contiene el valor es que se repite una vez en la segunda frase. Lo mismo para el resto de 1. En la posición 7 del vector se correspondería a la palabra machine que se repite una vez en la segunda frase.  Según como se ve en la vectorización se van a tener vectores muy similares cuando las frases son muy parecidas. Por ejemplo si hablamos de Machine learning vamos a ver que las posición 7 y 8 van a estar activados, de esta manera se van a encontrar similitudes en ambos.\nEs muy parecido a lo que hace con el juego de datos de Iris pero en este caso las características van a ser las palabras.\nEn el ejemplo siguiente debido al volúmen de datos del set importado hay palabras extrañas. Lo ideal será limpiar los datos de estas palabras extrañas para evitar que procesen, para ello lo mejor es usar la libreria NLTK para ello. Con esta librería se preprocesarían dichas palabras para dej\nCódigo\r#\r\r\rCódigo fuente descargable\nLibrerias de ejemplo\nimport sklearn # Set de datos que viene con las características: Un cuerpo de texto y con etiquetas # la categoria a la cual pertenece. La categoria es de que tema se esta hablando. from sklearn.datasets import fetch_20newsgroups # Separación de datos de entrenamiento y test from sklearn.model_selection import train_test_split # Algoritmo que va permitir vectorizar todo el texto que tenemos. from sklearn.feature_extraction.text import CountVectorizer # Algoritmo de regresión logísticas para la clasificaci´ from sklearn.linear_model import LogisticRegression Set de datos\n# Set de datos noticias = fetch_20newsgroups(subset=\u0026#34;train\u0026#34;) Ejemplo de datos\n# Podemos ver un ejemplo de datos que se va a procesar. Que se verá que es un corre electrónico print(\u0026#34;Cuerpo del mensaje: \u0026#34;, noticias.data[0]) # Si hacemo los mismo para ver a que clasificación pertenec. Aparecerá el valor 7, que es la clasificación a la que pertenece. print(\u0026#34;Clasificación: \u0026#34;, noticias.target[0]) Número de bloques de texto\n# Cuantos bloques de texto hay en el modelo de datos print(\u0026#34;Número de bloques de texto que hay: \u0026#34;, len(noticias.data)) Clasificaciones del set de datos\n# Nombre de las clasificaciones print(\u0026#34;Nombres de las clasificaciones:\u0026#34;, noticias.target_names) Variable para la vectorización\n# Variable para la vectorización vector = CountVectorizer() Datos para la vectorización\n# Se le pasan los datos que tenemos en la variable vector. vector.fit(noticias.data) Datos generados en el proceso de vectorización\n# Se visualizan los tokens generados de los datos pasados.print # Un token = una palabra única en todo el set de datos. print(\u0026#34;Tokens generados: \u0026#34;, vector.vocabulary_) Variable con el vector que contiene las repeticiones de cada fila de datos\n# Se crea una variable con la bolsa de palabras. Esta bolsa es el vector donde se cuenta el número de repeticiones que tienen cada una de las palabras bolsa = vector.transform(noticias.data) Resultado de la vectorización\n# Resultado de la matriz. # Devuelve un valor de 11314, 130105, # 11314 es el número de datos, o filas, que hay en el set de datos. # Y por cada fila tiene 130107 elementos, que es el número de palabras # distintas que hay en el modelo. print(\u0026#34;Filas de datos x columnas: \u0026#34;, bolsa.shape) Clasificaciones para el algoritmo\n# Ahora se crea la variable que contiene las clasificaciones bolsay = noticias.target Datos para entrenar y test\n# Se separán los datos para entrenamiento y test. X_ent, X_test, y_ent, y_test = train_test_split(bolsa, bolsay) Algoritmo de regresión\n# Algoritmo de regresion lr = LogisticRegression() Resultado del entrenamiento\n# Se le pasan los datos para el entrenamiento lr.fit(X_ent, y_ent) "});index.add({'id':59,'href':'/docs/python/machine_learning/redes_neuronales/ejemplo_sklearn/','title':"Ejemplo de redes neuronales",'content':"Introducción\r#\r\rEjemplo extraído del video Machine Learning episodio 6. Redes neuronales.\nEl ejemplo esta creado el Jupiter Notebool y se irá poniendo el código de las distintas celdas.\nEn este ejemplo se usa la librería Scikit learn(Sklearn) que según el video no es lo mejor para ello. Hay que mejores como TenserFlow pero requiere de más conocimientos de algebra lineal.\nCódigo\r#\r\r\rCódigo fuente descargable\nLibrerias del proceso\nimport sklearn # Set de datos from sklearn.datasets import load_iris # Separación de datos de entrenamiento y test from sklearn.model_selection import train_test_split # Clasificador de la red neuronal from sklearn.neural_network import MLPClassifier Datos para pasarle al clasificador\n# Carga de datos iris = load_iris() Características de los datos\n# Datos con las características caract = iris.data Etiquetas de los datos\n# Datos con las etiquetas etiq = iris.target # Separación de los datos de test y entrenamiento X_ent, X_test, y_ent, y_test = train_test_split(caract, etiq) Creación de la red neuronal\n# Se asigna el model de la red neuronal # max_iter indica que las veces en que los nodos se van a intercambiar información # para aprender solo va ocurrir 10 veces. # hidden_layer_sizes indica que va una capa oculta que tendrá 10 nodos. red = MLPClassifier(max_iter=10, hidden_layer_sizes=(10)) Entrenamiento\n# Se entra al algoritmo red.fit(X_ent, y_ent) Resultado\n# Resultado del entrenamiento red.score(X_test,y_test) En este ejemplo devuelve un 28% predicciones correctas, es un valor muy malo. Para mejorarlo se puede subir el valor de MAX_ITER a 100, con que el % de aprendizaje sube al 81%. Si aumentamos a 500 el valor de MAX_ITER el % sube al 97%.\nHay que tener en cuenta que el set de datos de Iris es pequeño y al aumentar el número de iteracciones no afecta mucho al tiempo total de calculo. Contra más datos se tenga y más iteracciones se hagan más tiempo consumaría el algoritmo de aprendizaje.\nEn HIDDEN_LAYER_SIZES se le puede pasar un valor de 10,10. Esto quiere decir que habrá 10 capas ocultas y en cada una de ellas 10 nodos.\nHay que tener en cuenta que si el resultado del entrenamiento es del 100% es que muy posiblemente tenga un sobreajuste.\n"});index.add({'id':60,'href':'/docs/python/machine_learning/algoritmo_aprendizaje_no_supervisado/ejemplo_kmeans/','title':"Ejemplo KMeans",'content':"Introducción\r#\r\rEjemplo extraído de Machine Learning episodio 8. KMeans.\nEl ejemplo esta creado el Jupiter Notebool y se irá poniendo el código de las distintas celdas.\nComo ejemplo para ver el funcionamiento de Kmeans se va utilizar el set de datos de iris. Pero en este caso solo se le va a pasar las características pero no las etiquetas para que clasifique en tres grupos los datos. Luego se le pasarán las etiquetas para ver como ha aprendido el algoritmo, pero no se pasarán las etiquetas para su aprendizaje.\nEn este ejemplo es un ejemplo de como este tipo de algoritmo, como se explica en el índice, es de prueba y error. Ya que variando el número de cluster o iteracciones la capacidad de aprendizaje aumenta o disminuye. En este tipo de cada lo mejor es poner el algorimo dentro de un FOR para que se pueda ir viendo cual es la combinación más idónea.\nUna vez de las ventajas del set de datos de iris es que tiene a que etiqueta pertnece cada flor y es fácil hacer comprobaciones. En caso de no disponer dicho valor comparativo entonces casí hay que meterse en cada valor para ir viendo como lo clasifico. Por ejemplo, si en el set datos hubiese una fecha, de cuando se inicio la medición, posiblemente estaría agrupando por dicho campo y afectaría a la clasificación. En este caso habría que eliminar dicha información para mejor el proceso.\nEste tipo de algoritmo es mucho de prueba y error e ir interpretando los datos para ir puliendolos para que el algoritmo clasifique mejor.\nCódigo\r#\r\r\rCódigo fuente descargable\n# Algoritmo aprendizaje no supervisado from sklearn.cluster import KMeans # Datos para el entrenamiento from sklearn import datasets # Libreria para ver que tal aprendido el algoritmo from sklearn import metrics # Datos de iris iris = datasets.load_iris() # Características de los datos. # Cada fila es una flor y cada columna es el grosor o longitud del petalo y cepalo X = iris.data # Etiquetas de los datos. Que indica a que especie pertenece cada flor de los datos anteriores. Y = iris.target # Datos de las carácterísticas print(\u0026#34;Características: \u0026#34;, X) # Datos de las etiquetas print(\u0026#34;Etiquetas: \u0026#34;, Y) # Algoritmo KMeans. Donde se le indica: # que dividida los datos en dos grupos o cluster. Nota el set de datos tiene 3 grupos # pero es para se vea la diferencia. # Y haga 3000 iteracciones para ajustar los centroides lo máximo posible km = KMeans(n_clusters=4,max_iter=3000) # Entrenamiento del algoritmo km.fit(X) # Es la predicción que indicará a que grupo pertenece los datos pasados. predicciones = km.predict(X) # Predicciones que realiza. El valor 0 indica el primer grupo, el 1 al segundo y así sucesivamente print(\u0026#34;Predicción realizada por el algoritmo: \u0026#34;, predicciones) # Variable para que compare la predicción realizada versus las etiquetas de los datos score = metrics.adjusted_rand_score(y, predicciones) # Resultado de hace 2 cluster y 3000 iteracciones arroja un resultado del 53% de acierto. # Si se cambia el algoritmo a 3 cluster el % sube al 73% # Si el número de cluster se sube a 4, el % disminuye al 65%. # Con lo cual el número de grupos/cluster idónea es 3. print(\u0026#34;Comparativa etiqueta vs predicción: \u0026#34;, score) "});index.add({'id':61,'href':'/docs/python/machine_learning/tensorflow/ejemplo_transfer_learning/','title':"Ejemplo transfer learning",'content':"Introducción\r#\r\rEjemplo extraído del video Importar modelo para clasificar imágenes.\nY el modelo de datos de entrenamiento se puede descargar en el repositorio del ejemplo de Alex Puig.\nEl transfer learning o transferencia de aprendizaje es una técnica en la cual incorporamos un modelo ya pre-entrenado para utilizarlo para que clasifique lo que nosotros queramos.\nEstos modelos son más complejos a nivel de configuración de redes convolucionales como el número de datos usados para entrenarlos. La gracia de usarlos es porque la configurad de la red detecta formas, sombras, cambios de contrastes, etc. Es decir, todo lo necesario para detectar una imagen. Estos modelos para reentranalos haría falta mucha fuerza computacional que uno no tiene en casa.\nKeras tiene modelos ya preentrenados que se pueden utilizar en nuestras propias clasificaciones. En el ejemplo del video se usará el modelo VGG16 que contiene clasificación de mil imagenes distintas.\nEn el ejemplo del video se base en el clasificador de imagenes por lo que las explicaciones que hace cada parte del código esta explicado en la página enlazada.\nUnas de las cosas que ha mejorado al usar la red VGG16 es que la precisión al clasificar las imagenes ha mejorado muchísimo. Salvo los gorilas que al haber pocas imagenes la tasa de acierto sigo siendo muy baja.\nEl ejemplo esta creado el Jupiter Notebool y se irá poniendo el código de las distintas celdas. Pero para ver que tal ha mejorado la precisión hay que ejecutar el código de predict.py del clasificador de imagenes.\n** NOTA IMPORTANTE: El algoritmo esta entrenado para imagenes 224x224 pixeles. Por lo tanto el codigo utilizado en el clasificador de imagenes hay que cambiarlo para adaptarlo a dicho tamaño**\nCódigo\r#\r\r\rCódigo fuente descargable\nimport sys import os import tensorflow as tf import math # VGG16 es un modelo preentrenado que contiene multitud de imagenes. # Este modelo preentrado es uno de los muchos que tiene Keras vgg = tf.keras.applications.vgg16.VGG16() # Resumen de las capas que tiene la red neuronal VGG16 vgg.summary() # Creación de la red convolucional cnn = tf.keras.Sequential() # Se añaden las capas que se ha visto antes a nuestra propia red for capa in vgg.layers: cnn.add(capa) # Resumen con las capas de nuestra propia red. Debe coincidir con el resumen # realizado en la red VGG16 cnn.summary() # Con esto se elimina la capa de predicción. El pop elimina la ultima capa # del modelo. La ultima capa era la de \u0026#34;predictions\u0026#34; que tiene 1000 parámetros, tipos de imagenes, # que pued clasificar. Como queremos usarlo para el ejemplo de gatos, gorilas y perros, se quita dicha # capa para añadir una propia. cnn.pop() # Las capas que provienen del VGG16 no queremos entrenarlas porque ya lo han sido # en el pasado. Por lo tanto, indicamos que no es necesario entrenarlas. for layer in cnn.layers: layer.trainable = False # Añadimos una capa de predicción de softmax que tendrá 3 neuronas. Ya que se usará el ejemplo # de gatos, gorilas y perros. cnn.add(tf.keras.layers.Dense(3,activation=\u0026#39;softmax\u0026#39;)) # Esta función encapsula lo que hemos hecho en el código anterior paso a paso. def modelo(): vgg=tf.keras.applications.vgg16.VGG16() cnn=tf.keras.Sequential() for capa in vgg.layers: cnn.add(capa) cnn.pop() for layer in cnn.layers: layer.trainable = False cnn.add(tf.keras.layers.Dense(3,activation=\u0026#39;softmax\u0026#39;)) return cnn # Se limpia todas las variables, estado, etc. de sesión que hemos abierto de keras tf.keras.backend.clear_session() # Trozo de código del clasificador de imagenes pero aquí no se indicarán las capas convolucionales data_entrenamiento = \u0026#34;K:/github/python_test/tensorflow/clasificador_imagenes/data/entrenamiento/\u0026#34; data_validacion = \u0026#34;K:/github/python_test/tensorflow/clasificador_imagenes/data/validacion/\u0026#34; # Parametros de la red neuronal # Número de veces que se iteran los datos en el entrenamiento epocas = 20 # Tamaño de las imagenes. Este es el tamaño que espera la red VGG16, en la documentación # oficial lo indica. longitud, altura = 224, 224 # Numero de imagenes que se procesan en cada paso batch_size = 32 # Al final de cada época se harán 300 pasos con los datos de validación # para ir viendo como de bien va aprendiendo validation_steps = 32 # Número de convoluciones o de capas(profundidad) que tendra la iamagen # Primera convolucion serán 32 filtrosConv1 = 32 # Segunda convolucion serán 64 filtrosConv2 = 64 # Anchura y altura que va a procesar en cada convolucion. tamano_filtro1 = (3, 3) tamano_filtro2 = (2, 2) # Tamaño que se va usar en el maxpooling tamano_pool = (2, 2) # Número de clases que hay en el set de datos: gatos, perros y gorilas clases = 3 # Es el learning rate. Es decir, cuanto de grandes van a ser los ajustes en la red # para ajustarse para buscar una solución óptima. lr = 0.0004 # Como se van a transformar las imagenes para poderlas pasar al procesao de entrenamiento entrenamiento_datagen = tf.keras.preprocessing.image.ImageDataGenerator( rescale=1. / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) # Lo mismo pero para los datos de test test_datagen = tf.keras.preprocessing.image.ImageDataGenerator( rescale=1. / 255) # Proceso de lectura y transformación de los datos para los datos de entrenamiento entrenamiento_generador = entrenamiento_datagen.flow_from_directory( data_entrenamiento, target_size=(altura, longitud), batch_size=batch_size, class_mode=\u0026#39;categorical\u0026#39;) # Lo mismo para los datos de test validacion_generador = test_datagen.flow_from_directory( data_validacion, target_size=(altura, longitud), batch_size=batch_size, class_mode=\u0026#39;categorical\u0026#39;) # Se llama a la función que devolverá el modelo adaptado basandonas en la red BGG16 cnn = modelo() # Se indica como aprenderá el algoritmo cnn.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=tf.keras.optimizers.Adam(lr=lr), metrics=[\u0026#39;accuracy\u0026#39;]) # Calculo de pasos por epoca pasos = math.ceil( ( len(entrenamiento_generador.filenames) / batch_size ) ) cnn.fit( entrenamiento_generador, steps_per_epoch=pasos, epochs=epocas, validation_data=validacion_generador, validation_steps=validation_steps ) # Directorio donde se guardará el modelo target_dir = \u0026#39;K:/github/python_test/tensorflow/clasificador_imagenes/modelo\u0026#39; if not os.path.exists(target_dir): os.mkdir(target_dir) # Grabación del modelo cnn.save(\u0026#39;K:/github/python_test/tensorflow/clasificador_imagenes/modelo/modelo.h5\u0026#39;) # Grabación de los pesos de cada una de las capas cnn.save_weights(\u0026#39;K:/github/python_test/tensorflow/clasificador_imagenes/modelo/pesos.h5\u0026#39;) "});index.add({'id':62,'href':'/docs/python/machine_learning/regresion_lineal/entrenar_algoritmo/','title':"Entrenar algoritmo de regresion lineal",'content':"Introducción\r#\r\rEjemplo extraído del video Machine Learning episodio 2. Algoritmos de regresión\nEl ejemplo esta creado el Jupiter Notebool y se irá poniendo el código de las distintas celdas\nCódigo\r#\r\r\rCódigo fuente descargable\nSe cargan las librerias\n# Regresor de KNN o vecinos cercanos from sklearn.neighbors import KNeighborsRegressor # Set de datos de boston from sklearn.datasets import load_boston # Divide los datos entre entrenamiento y testing from sklearn.model_selection import train_test_split # Algoritmos de regresión linea y ridge from sklearn.linear_model import LinearRegression, Ridge Carga del modelo de datos que se van a usar\n# A la variable boston se le asigna el set de datos. Que son los precios de las # casas en boston que depende de varías características boston = load_boston() # Visualiza las claves que tiene los datos boston.keys() Son sentencias que permite los datos del modo cargado en el paso anterior.\nSi se descomenta las sentencia y se ejecuta se van viendo los datos\r#\r\r# La data son las características #boston.data # Respuestas a nuestras características #boston.target # Ver los ejemplos que hay en data. Que devuelve 506, 13. Que son 506 casa # con 13 características #boston.data.shape # Si se hace lo mismo pero con el target devuelve 506 respuestas #boston.target.shape Variables que se usarán para el entrenamiento y test\n# Las que terminan en \u0026#34;_ent\u0026#34; son para entrenar y las que terminan # en \u0026#34;_test\u0026#34; son para testing. # Estas variables se inicializan con la data o características y las etiquetas X_ent, X_test, y_ent, y_test = train_test_split(boston.data, boston.target) Permite ver que se datos se usan para entrenamiento y para test\n# Si hacemos esto se que son los datos que ha usado para entrenar. Que serán 379, 13 print(\u0026#34;Datos para entrenar:\u0026#34;,X_ent.shape) # Y para testing devuelve el resto 127, 13 print(\u0026#34;Datos para testing:\u0026#34;,X_test.shape) # resto de variables que son los vector pero sin las características. # Solo devuelve 379 para _ent y 127 para _test print(\u0026#34;Datos y para entrenar:\u0026#34;, y_ent.shape) print(\u0026#34;Datos y para testing:\u0026#34;, y_test.shape) Primer algoritmo de regresion líneal.\n# Variable que se le asocia con el algoritmo de vecinos cercanos. Pasándole # que considere 3 vecinos. knn = KNeighborsRegressor(n_neighbors=3) Datos de entrenamiento\n# Se le pasa al algoritmo los datos de entrenamiento y los valores # objetivos o guia. Al ejecutar se visualizará los parámetros # del algoritmo que han sido modificados, como es el número de vecinos. knn.fit(X_ent,y_ent) Resultado del entrenamiento\n# Para saber como aprendio nuestro algoritmo se le pasan los # datos de test. # Si se le pasa 3 vecinos el % de aprendizaje es un 48%. Pero si se le # sube el valor del parámetro n_neighbors a 5 el % de aprendizaje # disminuye al 47%. # Y si ponemos 4 es un poco inferior al 3. Por ello, el más optimo # es dejarlo en 3 para este set de datos. knn.score(X_test,y_test) Borrao de datos para probar otros algoritmos.\n# Se borra el contenido de la variable para poder usar la libreria # \u0026#34;LinearRegressionLinearRegression\u0026#34; # El objetivo de borrar es evitar que la maquia se sature al procesos varios modelos del knn Se repiten los mismos pasos para usar otro algoritmo.\nrl = LinearRegression() # Se alimenta con los mismos datos que para el algoritmo de vecinos cercanos rl.fit(X_ent,y_ent) # Vamos a ver cual ha sido su nivel de aprendizaje. Pasandole # los mismos datos que para el KNN. En este caso ha aprendido un 72% # mucho mejor que el 48% del KNN rl.score(X_test,y_test) # Ahora se va borra el algoritmo usado para utilizar el algoritmo de ridger del rl Lo mismo pero para el algoritmo ridge\nridge = Ridge(alpha=1) # Se vuelve alimentar con los mismo datos de los algoritmos anteriores ridge.fit(X_ent, y_ent) # Y ahora a ver cuanto ha aprendido # Devuelve un 72%(redonde hacia arriba) que es un poco mejor al # algoritmo de regression lineal. Cambiando el parámetro alpha a 0.5 # devuelve un valor sensible inferior al valor por defecto que es alpha = 1 ridge.score(X_test,y_test) del ridge "});index.add({'id':63,'href':'/docs/python/framework/django/equivalencia_metodos_http_drf/','title':"Equivalencia métodos HTTP y DRF",'content':"Equivalencia en vistas de tipo ViewSet\r#\r\rEn las vistas de tipo ViewSet si queremos sobrecargar algun tipo de llamada HTTP hay que saber que método sobrecargar. La equivalencia es la siguiente:\n   HTTP Método ViewSet     GET Si queremos una lista de datos el método es list. Si es un valor concreto el método es retrieve   POST create   PUT update   PATCH partial_update   DELETE destroy    "});index.add({'id':64,'href':'/docs/python/machine_learning/estimador_incertidumbre/','title':"Estimador incertidumbre",'content':"Introducción\r#\r\rEjemplo extraído del video Machine Learning episodio 5. Estimador de incertidumbre. Se pondrán imagens obtenidas del video para explicar mejor el funcionamiento tal como lo hace en el video.\nEl ejemplo esta creado el Jupiter Notebool y se irá poniendo el código de las distintas celdas\nEl estimador de incertidumbre es una función que viene incluida en algunos algoritmos de Scikit learn(Sklearn), y nos ayuda a ver como de seguro esta nuestro algoritmo al clasificar un nuevo valor o punto, como lo llama en el video. El estimador es el paquete SVM dentro de la librería de Sklearn.\nEjemplo si tenemos un algoritmo que tiene que clasificar entre 0 y 1, el estimador nos va a decir que tan seguro se siente nuestro algoritmo para predicir que el nuevo valor lo va a clasificar al grupo 0 o al grupo 1. Por ejemplo nos puede servir para detectar que si un nuevo punto el algorito nos dice que esta al 50% seguro que lo va a clasificar al grupo 0, podríamos llegar a la conclusión que preferimos que no haga la predicción. Porque podría ser igual de valido que se clasificará al grupo 1. Ya que el nuevo valor tiene las misma posibilidades de ir a un grupo como al otro.\nEl uso del SVM es útil para poder escoger el algoritmo que nos va mejor a la hora de clasificar. Lo que hay que tener en cuenta el sobreajuste, si un algoritmo esta muy seguro a la hora de clasificar puede ser que este sobreajustando por lo tanto habrá que tratar dicho ajuste.\nCodigo\r#\r\r\rCódigo fuente descargable\nEl primer paso es siempre cargar las librerías.\n# Set de datos from sklearn.datasets import load_iris # Separador de datos de entrenamiento y test from sklearn.model_selection import train_test_split # Libreria de los estados de incertidumbre from sklearn import svm Datos para las pruebas\n# Carga del modelo de datos iris=load_iris() Generación de las variables con los datos\nX_ent, X_test, y_ent, y_test = train_test_split(iris.data, iris.target) Variable para poder poner en marcha el SVM\n# Se crea la variable con el algoritmo SVM algoritmo = svm.SVC(probability=True) Paso habitual para entrenar el algoritmo\n# Entrenamiento del algoritmo algoritmo.fit(X_ent, y_ent) Se informo como se quiere que se comporte el SVM.\nalgoritmo.decision_function_shape = \u0026#34;ovr\u0026#34; # decision_function nos va a devolver un numero que nos va a decir que # tan lejos esta el punto del hiperplano que clasifica. # Al algoritmo se le pasa los valores de testing y que solo nos traiga 10 ejemplos. algoritmo.decision_function(X_test)[:10] En este trozo de código vamos a explicar como funciona el SVM tal como lo hace el video. Veamos el siguiente modelo de datos:\n\rLa línea verde o hiperplano nos separa los puntos azules de los rojos. Lo que queremos que devuelva el algoritmo sea el número más alto, ya que indicará que esta lo más alejado posible de la línea/hiperplano. Si el algoritmo un número que esta sobre la línea no va a estar muy seguro de como clasificarlo, pero si el punto esta lo más lejano de la línea el algoritmo va a estar más seguro a la hora de clasificarlo.\nEl resultado que devuelve al ejecutar es el siguiente:\narray([[-0.20269857, 2.23688486, 0.84169686],\r[-0.21313057, 2.23268086, 0.88330921],\r[ 2.22580348, 1.15214418, -0.24872264],\r[ 2.23455235, 1.13095515, -0.25044655],\r[ 2.23313006, 1.13211798, -0.24964792],\r[ 2.2269661 , 1.1259788 , -0.24423893],\r[-0.23439994, 1.08865842, 2.22247787],\r[-0.23387148, 1.18608119, 2.17366629],\r[ 2.22684708, 1.14267576, -0.24739233],\r[-0.21427536, 2.21884877, 0.96646885]])\rQue significa, que el primer registro o dato lo va a clasificar en el segundo grupo porque es el número más alto, es decir, el más alejado al hiperplano. En el segundo datos también iría al segundo grupo, pero en el tercer dato lo clasificaría en el primer grupo.\nNOTA: Nos devuelve tres grupos la función porque en el set de datos de Iris soy hay tres tipos de flores para clasificar\nSe lanza otra función contra los datos de test para devuelve unos valores similares al código anterior pero a nivel de probalidad.\n# Esta función nos va decir algo parecido al \u0026#34;decision_function\u0026#34; pero en el # ámbito de la probabilidad. # Se le pasa los datos de testing y se le indica que solo procese los 10 primeros algoritmo.predict_proba(X_test)[:10] El resultado que devuelve al ejecutar es el siguiente:\narray([[0.01701662, 0.97626968, 0.0067137 ],\r[0.01130569, 0.97524721, 0.0134471 ],\r[0.95145127, 0.03609592, 0.01245281],\r[0.97121252, 0.01879533, 0.00999214],\r[0.96865943, 0.02079702, 0.01054356],\r[0.95443207, 0.03348436, 0.01208357],\r[0.00997134, 0.04291861, 0.94711005],\r[0.01111241, 0.46790436, 0.52098323],\r[0.95385933, 0.03421938, 0.0119213 ],\r[0.01653293, 0.95084668, 0.03262039]])\rAquí vemos que el primer dato lo va clasificar al segundo grupo con un 97% de probabilidades, lo mismo para el segundo datos. Pero para el tercer registro iría al primer grupo con un 95% de probabilidades.\nComo se ve la clasificación es coherente con la función anterior.\nNOTA: La suma de los porcentajes de cada fila va ser del 100%\nFinalmente se lanza la función de predicción para ver como va a predecir el algoritmo.\n# Con al función o algoritmo predict nos va a decir como va a predecir el algoritmo de clasificación algoritmo.predict(X_test)[:10] El resultado que devuelve al ejecutar es el siguiente:\narray([1, 1, 0, 0, 0, 0, 2, 2, 0, 1])\rEl resultado que devuelve coincide con los valores que nos han devuelto las funciones anteriores. Hay que recordar que los arrays empiezan por el índice 0, por lo tanto en el primer registro que vale \u0026ldquo;1\u0026rdquo; equivale al 97% que devuelve el PREDICT_PROBA y el \u0026ldquo;2.23\u0026rdquo; que devuelve la función DECISION_FUNCTION\n"});index.add({'id':65,'href':'/docs/sap/abap/rtts/estructuras/','title':"Estructuras",'content':"Introducción\r#\r\rQue se puede hacer con estructuras ya sean del diccionario como tipos de datos estructurados creados dentro del código.\nEjemplo\r#\r\rManera simple de saber los campos de una estructura. Dentro de los campos de la tabla devuelta esta el campo con el tipo de datos del campo. De él se puede obtener la info técnica de dicho campos.\nDATA(lt_components) = CAST cl_abap_structdescr( cl_abap_typedescr=\u0026gt;describe_by_data( cs_row_data ) )-\u0026gt;get_components( ). "});index.add({'id':66,'href':'/docs/python/machine_learning/arbol_decisiones/explicacion_sobreajuste/','title':"Explicación sobreajuste",'content':"Introducción\r#\r\rEjemplo extraído del video Machine Learning episodio 4. Sobreajuste. Este artículo es un añadido a lo que se comenta brevemente en el artículo de ejemplo sobre arbol de decisiones\nEn el video indicado se explica perfectamente lo que és, aún así, intento explicarlo a mi manera que es un sobreajuste o overfilling, en ingles. Se usarán pantallazos del video para explicar algunos puntos.\nEl sobreajuste se produce cuando se entrena el algoritmo ya que intenta generar una formula o ecuación que permite clasificar los datos. El problema viene que al hacer esto sacrifica fiabilidad en las predicciones futuras.\nEn el ejemplo básico sobre arbol de decisiones se ponía la siguiente imagen de lo que producía el algoritmo sin sobreajuste:\n\rLos recuadros azules marcan el sobreajuste que se produce y una vez ajustado los parámetros del algoritmo ese sobreajuste desaparece:\n\rExplicación\r#\r\rImaginemos que tenemos el siguiente set de datos que se tienen que clasificar:\n\rY queremos que clasifica lo que son rojos y verdes. En un caso normal el algoritmo lo clasificaría de esta manera:\n\rPero en la imagen hay ciertos puntos verdes y rojos que están mezclados y que con una clasificación normal no se haría correctamente. Lo que ocurre con el algoritmo de clasificación es que intenta hacer esta curva rara:\n\rComo se ve intenta clasificarlo todo siguiente la línea. El problema es que si en un futuro viene un valor rojo justo al lado de un verde que este en esas curvas extrañas lo va a clasificar mal.\nCon la regresión ocurre lo mismo, en un caso normal intentaría ajuste de la siguiente manera:\n\rPero con sobreajuste intentaría hacer todo esto:\n\rEl sobreajuste se detecta con la opción score de nuestro algoritmo:\n# El resultado con los datos de test son del 94% arbol.score(X_test,y_test) Que según el % que devuelva veremos que si el % es 97 para arriba es que esta haciendo sobreajuste. Si el % es más alto más sobreajuste habrá. La solución es ir ajustando los parámetros del modelo o del algoritmo para ir ajustandolos.\n"});index.add({'id':67,'href':'/docs/sap/abap/funciones/fecha/','title':"Fecha / Hora",'content':"Objetivo\r#\r\rDebido a que hay multitud de funciones para fechas y horas que uso, las pongo en un apartado aparte en vez de ponerlas en la de Basic Component.\nLista\r#\r\r   Función Descripción     GET_WEEK_INFO_BASED_ON_DATE Te devuelve la semana y el día que es lunes y domingo.   DATE_GET_WEEK Solo devuelve la semana. Ojo que la semana viene al principio con el año.   HR_AUPBS_MONTH_DAY Le pones dos fechas y te devuelve la diferencia en días, meses, años y días de calendario   HR_MONTHS_BETWEEN_TWO_DATES Dos fechas y te devuelve la diferencia en meses   MONTH_NAMES_GET Devuelve los textos de los meses para un idioma.   RP_CALC_DATE_IN_INTERVAL Permite obtener una fecha a partir de meses, dias o años. Se puede indicar que se sumen o resten   SD_DATETIME_DIFFERENCE Le pasas dos fechas y horas y te devuelve la diferencia en dias y horas   SWI_DURATION_DETERMINE\tFuncion que devuelve en segundos la diferencia entre dos fechas y dos horas    WEEK_GET_FIRST_DAY Devuelve el primer día de la semana pasada   TZ_SYSTEM_TO_LOCAL Convierto la hora y fecha del huso horario del servidor a un huso horario   L_MC_TIME_DIFFERENCE Es como la SD_DATETIME_DIFFERENCE pero devuelve la diferencia en minutos   LAST_WEEK De una semana te devuelve la fecha del lunes y domingo. Y además la semana anterior.   CONVERT_DATE_TO_INTERNAL Convierte una fecha en char a formato interno.   FIMA_DAYS_AND_MONTHS_AND_YEARS Devuelve la diferencia en meses y años entre dos fechas   RSARCH_DATE_CONVERT De una fecha te devuelve mes, periodo, dia, semana, mes, etc..   EWU_ADD_TIME Le pasas fecha y hora. Le pones un campo fecha para que sume hora/minuto/segundo   BKK_ADD_WORKINGDAY Suma/resta dias a una fecha teniendo en cuenta dias laborables    "});index.add({'id':68,'href':'/docs/python/sentencias/ficheros/','title':"Ficheros",'content':"Ficheros\r#\r\rLeer un fichero\r#\r\rSe puede hacer de dos manera con la sentencia WITH y sin ella.\nEjemplo modo sin WITH\nfile = open(\u0026#34;welcome.txt\u0026#34;) data = file.read() print data file.close() # Importante hay que cerrar el fichero para evitar bloqueos Con la opción WITH\nwith open(\u0026#34;hola.txt\u0026#34;) as file: data = file.read() Con WITH parece ser que el control de excepciones es mejor y tareas simples como la preparación del fichero y su cierre se hace de manera automática. Con lo cual evita errores tontos en el tratamiento de ficheros.\nSi queremos abrir el fichero para escritura con WITH sería:\nwith open(\u0026#39;salida.txt\u0026#39;, \u0026#39;w\u0026#39;) as file: file.write(\u0026#39;hola!\u0026#39;) "});index.add({'id':69,'href':'/docs/sap/abap/sentencias_74/filtros/','title':"Filtros",'content':"Introducción\r#\r\rEn este grupo se irán poniendo distintos ejemplos de la sentencia FILTER que es un sentencia que no he usado mucho.\nEstas sentencias se pueden utilizar para crear nuevas variables o ponerlas como entrada de parámetro en clases.\nEjemplo 1\r#\r\r\u0026lt;ls_values_search_sap\u0026gt;-kotab_data = FILTER #( lt_values IN FILTER #( lt_konp USING KEY loevm WHERE loevm_ko = \u0026#39;\u0026#39; ) WHERE knumh = knumh ). Lo bueno del filter es que te crea una tabla o te mueve directamente el contenido de la tabla con el filtro aplicado. En el caso de usar búsqueda directa (segundo FILTER) te obliga, eso creo, ha tener una clave para la búsqueda.\nEjemplo 2\r#\r\rdata(et_cust_hier) = FILTER #( lt_cust_hier IN lt_cust_sales USING KEY kunnr WHERE kunnr = kunnr ). Se filtra el contenido de la tabla LT_CUST_HIER con los valores en LT_CUST_SALES filtrando por el cliente. Importante la tabla LT_CUST_SALES debe ser de tipo SORT. ET_CUST_HIER se creará del mismo tipo que LT_CUST_HIER.\n"});index.add({'id':70,'href':'/docs/sap/abap/bopf/tablas/finanzas/','title':"Finanzas",'content':"Objetivo\r#\r\rTablas que permiten descubrir determinadas configuraciones de dicho módulo.\nLista\r#\r\rGenerales\r#\r\r   Función Descripción     GB01 Tabla que contiene los campos que se pueden sustituir en la OBBH   T045DTA Partidas que se graban en la FBWE. Si se borran permite reprocesarlas.    "});index.add({'id':71,'href':'/docs/sap/abap/funciones/finanzas/','title':"Finanzas",'content':"Objetivo\r#\r\rFunciones del módulo de finanzas\nLista\r#\r\r   Función Descripción     FI_FIND_PAYMENT_CONDITIONS Obtiene la fecha de vencimiento en base a la fecha base y condicion de pago   GET_CURRENT_YEAR Se le pasa sociedad y fecha de documento y devuelve el ejercicio fiscal   BAPI_COMPANYCODE_GET_PERIOD Devuelve el periodo y ejercicio contable a partir de una fecha y sociedad. Esta funcion puede ser llamada en remota.   SD_PRINT_TERMS_OF_PAYMENT Calcula la fecha de vto   FI_TERMS_OF_PAYMENT_PROPOSE Calcula la fecha de vto. La diferencia con el anterior es que se le pasa el proveedor.   FI_F4_ZTERM sTe muestra las condiciones de pago o devuelve una tabla con la información en base a un tipo de cuenta    "});index.add({'id':72,'href':'/docs/sap/abap/funciones/','title':"Funciones",'content':"Objetivo\r#\r\rRecopilación de funciones que son interesantes\nPublicaciones\r#\r\r\r\rBasic Component\r\r\rBasic Component\r\r\rFecha / Hora\r\r\rFecha y Hora\r\r\rFinanzas\r\r\rFinanzas\r\r\rLogística\r\r\rLogística\r\r\rVentas\r\r\rVentas\r\r\r"});index.add({'id':73,'href':'/docs/python/sentencias/funciones_internas/','title':"Funciones internas",'content':"Funciones internas\r#\r\rAquí una lista de funciones internas\n MAX y MIN \u0026ndash;\u0026gt; Devuelve el valor máximo y mínimo de una lista. Si es un string nos devolvería la letra más alta y la mínima. Ejemplo:  print(\u0026#34;Letra máxima: \u0026#34;,max(\u0026#34;hola mundo\u0026#34;)) print(\u0026#34;Letra mínima: \u0026#34;,min(\u0026#34;hola mundo\u0026#34;)) Resultado:\nLetra máxima: u\rLetra mínima:  LEN \u0026ndash;\u0026gt; Devuelve el numéro de elementos de una lista. Si es un string devuelve el número de caracteres. Ejemplo:  print(\u0026#34;Número de caracteres: \u0026#34;,len(\u0026#34;hola mundo\u0026#34;)) Resultado:\nNúmero de caracteres: 10\r"});index.add({'id':74,'href':'/docs/sap/gateway/','title':"Gateway",'content':"Objetivo\r#\r\rGateway es el módulo que tiene para la publicación de servicios REST en SAP.\nProyectos personales\r#\r\rEn el siguiente enlace se aglutian utilidades para simplificar determinar procesos en Gateway.\nPublicaciones\r#\r\r\r\rInformación general\r\r\rInformación general\r\r\r"});index.add({'id':75,'href':'/docs/python/machine_learning/guardar_modelos_entrenados/','title':"Guardar modelos entrenados",'content':"Introducción\r#\r\rEjemplo extraído del video Machine Learning episodio 9. Cómo guardar un modelo entrenado.\nEl ejemplo esta creado el Jupiter Notebool y se irá poniendo el código de las distintas celdas.\nEn esta página veremos como se guardan los modelos entrenados para no tener que hacer entrenamientos cada vez que queremos hacer algo. En los ejemplos entrenar algoritmos se hace en segundos, pero en casos reales este entrenamiento puede durar dias o semnanas. Por eso es vital guardar el entrenamiento. En el video lo llamán Persistencia del modelo.\nLa página la pongo a nivel general dentro de machine learning porque es aplicable a todos los ejemplos.\n** NOTA: La librería tal como se ve en el video es obsoleta. He puesto la manera en que se grabaría actualmente**\nCódigo guardar el modelo entrenado\r#\r\r# Carga de datos de ejemplo y algoritmo de regesion logísitca from sklearn import datasets, linear_model # Separación de datos de test y reales from sklearn.model_selection import train_test_split # Guardar entrenamiento a fichero from joblib import dump, load # Datos de las flores iris = datasets.load_iris() # Algoritmo de regresion líneal clf = linear_model.LogisticRegression() # Se muestra las claves de los datos. Esas claves es la que # se usa para poder entrenar el algoritmo print(\u0026#34;Claves de los datos: \u0026#34;, iris.keys()) # Separación de datos de entrenamiento y test X_ent, X_test, y_ent, y_test = train_test_split(iris.data, iris.target) # Entrenamiento del modelo clf.fit(X_ent, y_ent) # Resultado del entrenamiento clf.score(X_test, y_test) # Se guarda el fichero con el entrenamiento dump(clf, \u0026#34;modelo_entrenado.pkl\u0026#34;) Código para recuperar modelo entrenado\r#\r\r"});index.add({'id':76,'href':'/docs/sap/abap/idoc/','title':"IDOC",'content':"Objetivo\r#\r\rRecopilar información útil sobre los IDOC\u0026rsquo;s\nProyectos personales\r#\r\rEn el siguiente enlace hay utilidades para simplificar procesos.\nPublicaciones\r#\r\r\r\rInformación general\r\r\rInformación general\r\r\r"});index.add({'id':77,'href':'/docs/python/framework/django/incluir_librerias_propias/','title':"Incluir librerías propias",'content':"Introducción\r#\r\rDjango arranca su propio middleware, o servidor, donde solo se puede trabajar con los archivos del proyecto y aplicaciones generadas. Pero no podemos usar librerías propias que tengamos la misma carpeta del proyecto, o en una carpeta dentro del proyecto de Django. El motivo es que no se incluyen dentro de las variables entorno y por lo tanto no las reconoce.\nLa forma de solucionar son dos dependiendo de donde tengamos la carpeta con nuestra libreria.\nEn ambos casos la solución se realiza en el archivo setting.py que esta dentro de la carpeta de configuración del proyecto Django. El motivo de hacerlo en dicho archivo porque es el que se lee cuando arranca el proceso de Django.\nAVISO IMPORTANTE:\nLa versión en que la librería esta fuera del proyecto funciona, las peticiones van sin problemas. He detectado que al sincronizar los modelos de las aplicaciones:\nmanage.py makemigrations \u0026lt;app\u0026gt;\rDa un error que no encuentra la librería y no hay manera de hacerlo funcionar. Dejo como se hace por si en futuro encuentro la manera de corregirlo. Porque a nivel organizativo me gusta más fuera del proyecto.\nLibrería fuera de la carpeta del proyecto\r#\r\rTenemos la siguiente estructura:\nProyecto VS Code |\u0026mdash; Proyecto Django | |\u0026mdash; Aplicación Django |\u0026mdash; lib | |\u0026mdash; img | | |\u0026mdash; image\nHay que añadir las siguientes dos líneas:\nEXTERNAL_LIBS_PATH = os.path.join(Path(__file__).parent.parent.parent, \u0026quot;lib\u0026quot;)\rsys.path = [\u0026quot;\u0026quot;, EXTERNAL_LIBS_PATH] + sys.path\rAquí tenemos:\n La primera fila creamos una variable donde: 1) encontrados el directorio donde esta el proyecto de VS Code 2) A ese path se le concatena la carpeta, \u0026ldquo;lib\u0026rdquo;, con la librerias del proyecto Al path de las variables de entonro de python se le añade: el directorio raíz donde esta el archivo, la variable creada y las variables de entorno previa.  Finalmente en nuestro fichero de django la librería se declaría de la siguiente manera:\nfrom lib.img import image Librería dentro de la carpeta de proyecto\r#\r\r** Esta versión es la que uso actualmente porque me funciona con las sincronizaciones de los modelos **\nTenemos la siguiente estructura:\nProyecto VS Code |\u0026mdash; Proyecto Django | |\u0026mdash; Aplicación Django | |\u0026mdash; lib | | |\u0026mdash; img | | | |\u0026mdash; image\nHay que añadir las siguientes dos líneas:\nEXTERNAL_LIBS_PATH = os.path.join(BASE_DIR, \u0026quot;lib\u0026quot;)\rsys.path = [\u0026quot;\u0026quot;, EXTERNAL_LIBS_PATH] + sys.path\rAquí tenemos:\n Una variable que concatena el valor de la variable anterior + el directorio donde están las librerías Se añade la variable a la variante de entorno  "});index.add({'id':78,'href':'/docs/sap/abap/archivelink/informacion_general/','title':"Información general",'content':"Objetivo\r#\r\rEsta página tendrá información general sobre el archivelink, GOS o BDS que son maneras de distintas de archivar documentos en SAP.\nMenú ámbito\r#\r\rEl menú SOA_COPY contiene todas las transacciones para crear, gestionar y monitorizar el archivelink\nProgramas\r#\r\r   Programa Descripción     RSAOT1 Ejemplo de cómo subir y leer documento en archivelink    Tablas\r#\r\r   Tabla Descripción     SGOSATTR Tabla para parametrizar las opciones del menu de archivelink en transacciones    Funciones\r#\r\r   Función Descripción     BDS_GOS_CONNECTIONS_GET Obtiene los adjuntos de un objeto del GOS. El parámetro es la clase de objeto definida en la SWO1    "});index.add({'id':79,'href':'/docs/sap/abap/idoc/informacion_general/','title':"Información general",'content':"Objetivo\r#\r\rEsta página tendrá un listado de información general sobre los IDOC\u0026rsquo;s: transacciones, programas, etc..\nMenú ámbito\r#\r\rEl menú WEDI contiene todas las transacciones para crear, gestionar y monitorizar IDOCs\nTransacciones\r#\r\r   Transacción Descripción     WE19 Clonar IDOC para poderlo procesar   BD87 Analizar IDOC\u0026rsquo;s    Programas\r#\r\r   Programa Descripción     RC1_IDOC_SET_STATUS Permite cambiar el status de un idoc a otro    Tablas\r#\r\r   Tabla Descripción     TBDBE Tabla donde esta la configuración de la función que se lanzará a través de la función BAPI_IDOC_INPUT1    "});index.add({'id':80,'href':'/docs/sap/abap/recursos_humanos/informacion_general/','title':"Información general",'content':"Objetivo\r#\r\rEsta página tendrá un listado de información general sobre el módulo de recursos humanos\nFunciones\r#\r\r   Función Descripción     HR_READ_INFOTYPE_AUTHC_DISABLE Hace que en la funcion HR_READ_INFOTYPE no haga control de autorizacion. Se ha de poner por cada función de lectura de infotipo   HR_CHECK_AUTHORITY_INFTY Permite saber si se tiene autorizacion para leer/escribir un infotipo    Ejemplos\r#\r\rHR_CHECK_AUTHORITY_INFTY\r#\r\rCALL FUNCTION \u0026#39;HR_CHECK_AUTHORITY_INFTY\u0026#39; EXPORTING pernr = pernr-pernr infty = \u0026#39;0008\u0026#39; subty = \u0026#39;0 \u0026#39; begda = p0001-begda endda = p0001-endda level = \u0026#39;R\u0026#39; uname = sy-uname EXCEPTIONS no_authorization = 1 internal_error = 2 OTHERS = 3. Transacciones\r#\r\r   Transacción Descripción     PC_PAYRESULT Programa para ver resultados de nomina    "});index.add({'id':81,'href':'/docs/sap/BW/informacion_general/','title':"Información general",'content':"Objetivo\r#\r\rEsta página contendrá información de general sobre BW que no se muy bien donde clasificar.\nComo probar un cubo en R/3\r#\r\rNormalmente los cubos se suelen utilizar desde herramientas de SAP que están fuera del SAP GUI. Pero puede ser interesante, poder probar el cubo para ver que datos tienes y que se puede hacer con él. El siguiente ejemplo de como hacerlo se basa en cubo generado con un CDS desde un sistema S/4 HANA.\nLo primer es ir a la transacción RSRT. En el campo de query hay que pulsar sobre la ayuda para búsqueda y buscar el cubo:\n\rEn este caso hemos ido a la pestaña de InfoÁreas y en nodos no asignados. Hemos hecho doble click en el nodo seleccionado y pulsaremos el botón Ejecutar para lanzar la query:\n\rAparecerá una pantalla para introducir los datos de selección. Y una vez introducido se pulsará el siguiente botón y se ejecutará la consulta:\n\rLos datos que aquí se muestran serán los mismos que salgan en cualquier heramienta que explote dicho cubo.\n"});index.add({'id':82,'href':'/docs/sap/gateway/informacion_general/','title':"Información general",'content':"Objetivo\r#\r\rEsta página tendrá un listado de información general de Gateway\nTransacciones\r#\r\r   Transacción Descripción     SMGW Transaccion para ver el monitor   SEGW Transacción para los servicios de Gateway   /IWFND/CACHE_CLEANUP Transaccion para borrar el cache del modelo   /IWFND/ERROR_LOG Transaccion para ver los logs   /IWFND/MAINT_SERVICE Activar y mantener servicios    Como debugear error XML Parse\r#\r\rLos errores de gateway muchas nos hay por donde cogerlos. Sobreto cuando se hace deep entity que no hay manera de saber que produce el error. He aquí una manera de saberlo a través del debugging:\nIr a la clase /IWCOR/CL_DS_EP_READER_JSON método READ_ENTITY_INTERNAL. Este método procesa el JSON que recibe y se puede ver el fallo de mapeos\n"});index.add({'id':83,'href':'/docs/sap/ventas/informacion_general/','title':"Información general",'content':"Objetivo\r#\r\rEsta página tendrá información a nivel general\nFlujo\r#\r\rUna imagen que sintetiza muy bien el flujo de ventas:\n\r"});index.add({'id':84,'href':'/docs/python/sentencias/iteracciones/','title':"Iteracciones",'content':"Iteracciones\r#\r\rIteracciones condicionales\r#\r\rLlamo iteracciones condicionales aquellas sentencias que no recorren listas sino simplemente repetin un bloque de sentencias hasta que se cumple una condición. La sentencia WHILE sería un ejemplo de ello:\nn = 5 print(\u0026#34;Contador\u0026#34;) while n \u0026gt;= 0: print(n) n = n - 1 Resultado:\nContador\r5\r4\r3\r2\r1\r0\rCon la sentencia BREAK se puede salir de un bucle. Ejemplo:\nn = 5 print(\u0026#34;Contador\u0026#34;) while True: print(n) n = n - 1 if n == 0: break Resultado:\nContador\r5\r4\r3\r2\r1\rTambién se tiene la sentencia CONTINUE. Esta sentencia salta a la siguiente interacción. Ejemplo:\nn = 6 print(\u0026#34;Contador\u0026#34;) while n \u0026gt;= 0: n = n - 1 if ( n % 2 ) != 0: # Solo se pintan los pares continue print(n) Resultado:\n4\r2\r0\rNOTA PERSONAL: Esta sentencia no la uso en otros lenguajes menos en Python. Pero la pongo para saber que existe.\nIteracciones listas\r#\r\rAquí encontramos la sentencia FOR que permite recuperar una lista de valores y procesarlas. Ejemplo:\nnumeros = [5,4,3,2] for numero in numeros: print(\u0026#34;Número: \u0026#34;, numero) Resultado:\nNúmero: 5\rNúmero: 4\rNúmero: 3\rNúmero: 2\rUsando enumerate\r#\r\renumerate permite recorre una lista recorriendo todos sus valores y devolviendo el índice y su valor. Ejemplo:\nnumeros = [5,4,3,2] for indice, valor in enumerate(numeros): print(f\u0026#34;indice: {indice} valor: {valor}\u0026#34;) Resultado:\nindice: 0 valor: 5\rindice: 1 valor: 4\rindice: 2 valor: 3\rindice: 3 valor: 2\rLo que permite el enumarete es olvidarte de saber cuantos registros tiene la tabla y lo gestiona él.\nYo esta sentencia la he usado junto a los dictionary para saber en que indice esta un valor:\nblock_num_index = {} for index, value in enumerate(data[\u0026#39;block_num\u0026#39;]): block_num_index.setdefault(value,[]).append(index) En debugging el dictionary quedaría así:\n\rLa clave son números, ya que son códigos de grupo, y el valor de cada clave es el índice donde aparecerá dicho grupo.\n"});index.add({'id':85,'href':'/docs/sap/abap/sentencias_74/lectura_tablas_diccionario/','title':"Lectura tablas del diccionario",'content':"Introducción\r#\r\rRealmente no tenemos sentencias específica sino que es ampliación del Open SQL para dotarle de más flexibilidad a la hora de leer información\nEjemplo 1\r#\r\rCon este select extendido se busca una parte de los campos de una tabla, pero el resto de campo se completan segun variables rellenadas previamente.\nSELECT kschl, kotab, @is_header-customer_code AS keyvalue, @is_header-validity_to AS datab, @is_header-validity_from AS datbi, @zif_rtn_data=\u0026gt;cv_updkz_insert AS updkz INTO CORRESPONDING FIELDS OF TABLE @et_cond_crud FROM ztippqps WHERE pqtyp = @is_header-negotiation_type_code AND keyfield = @lv_keyfield. Tenemos que pondemos rellenar campos de una tabla/estructura a partir de una constantes. Antes se tendría que haber hecho un loop para hacerlo. Cuando usamos estas capacidades extendidas hay que usar el símbolo @ en las variables que no pertencen al selecy: parámetros, variables, etc..\nEjemplo 2\r#\r\rMismo select que el del ejemplo 1 pero crea la tabla interna donde se guardarán los datos de manera automática, sin declarar previamente.\nSELECT kschl, kotab, @is_header-customer_code AS keyvalue, @is_header-validity_to AS datab, @is_header-validity_from AS datbi, @zif_rtn_data=\u0026gt;cv_updkz_insert AS updkz INTO TABLE @data(et_cond_crud) FROM ztippqps WHERE pqtyp = @is_header-negotiation_type_code AND keyfield = @lv_keyfield. "});index.add({'id':86,'href':'/docs/sap/abap/programas/programas/','title':"Lista de programas",'content':"Lista\r#\r\rGenerales\r#\r\r   Programa Descripción     RSSPO* Tiene ejemplos de las funcion POPUP_*   BCS_EXAMPLE_7 BCS: Enviar correu-e amb adjunt Excel autocreat (exemple)   RSCPSETEDITOR Este es el programa que a partir del EHP4 permite activar desactivar el word como editor para smartforms y sapscript   BALVBUFDEL Resetea memoria intermedia de ALV. Ideal cuando se cambian textos de elementos de datos.   RSBTONEJOB Este programa se pone en un job, se le indica el nombre de otro job y si está activo saca un error message y cancela   PRGN_CORRMEN2 Sincroniza los textos de un idioma del menu de ámbito en un rol   RSDRI_INFOPROV_READ_DEMO Saber como recuperar datos de un ODS mediante funciones.   RADMASG0 Permite hacer activaciones en masa de objetos normales o estándar. Útil en ampliaciones al estándar    Ventas\r#\r\r   Programa Descripción     RV80HGEN Activar rutinas VOFM. Ver nota: 327220 - VOFM function and its objects to learn how to works VOFM    Finanzas\r#\r\r   Programa Descripción     RGUGBR00 Genera la sustituciones y validaciones de FI   RFBIBL00 Programa para generar contabilizacion en FI    Recursos humanos\r#\r\r   Programa Descripción     RGUGBR00 Genera la sustituciones y validaciones de FI   RHMOVE30 Programa para pasar a una orden de transporte las estructura organizativa de HR   RPUDELPN  Borrado masivo de empleados de HR (transaccion PU00 para individual)    Workflow\r#\r\r   Programa Descripción     RSWUWFML2 Programa notificación Workitems Workflow WKF aviso email envío   RMPS_SET_SUBSTITUTE Programa para ponerte de sustituto de cualquier usuario SAP para el SAPOffice    "});index.add({'id':87,'href':'/docs/python/sentencias/listas_tuplas/','title':"Listas y tuplas",'content':"Listas y tuplas\r#\r\rLas listas son las que usan [] y las tuplas(). Ambas sirven para guardar datos. Y ambas están indexadas por 0, es decir, al primer registro se accede por el valor 0.\nLos string en si mismos son listas como se explica en pagína de cadenas. Pero mientras que en la cadenas los valores son carácteres en una lista/tupla puede contener cualquier valor.\nEjemplo de inicialización:\ntupla=(1,2,3,4,5,6,7) lista=[8,9,10,11,12] La diferencia en que la tupla no puede ser modificada durante el programa pero una lista si que se puede. Salvo en las cadenas, que no pueden ser modificadas.\nUn truco para obtener el último valor sin usar el LEN es usar el -1. Ejemplo:\ntupla=(1,2,3,4,5,6,7) lista=[8,9,10,11,12] print(\u0026#34;Último valor: \u0026#34;, lista[-1]) print(\u0026#34;Penúltimo valor: \u0026#34;, lista[-2]) Resultado:\nÚltimo valor: 12\rPenúltimo valor: 11\rLista\r#\r\rModificar valores\r#\r\rPara cambiar una lista es tan sencillo al índice y poner un nuevo valor. Ejemplo:\nlista=[8,9,10,11,12] print(\u0026#34;Valores antes del cambio:\u0026#34;, lista) # Cambiamos el segundo registro lista[1]=15 print(\u0026#34;Valores después del cambio:\u0026#34;, lista) Resultado:\nValores antes del cambio: [8, 9, 10, 11, 12]\rValores después del cambio: [8, 15, 10, 11, 12]\rBorrar valores\r#\r\rPara modificar valores se usa el método interno de la lista llamado POP. Ejemplo como borrar el último valor:\nlista=[8,9,10,11,12] print(\u0026#34;Valores antes:\u0026#34;, lista) lista.pop() print(\u0026#34;Valores después:\u0026#34;, lista) Resultado:\nValores antes: [8, 9, 10, 11, 12]\rValores después: [8, 9, 10, 11]\rPara borrar una posición concreta se puede hacer de dos maneras con el método POP o con la sentencia DEL. Ejemplo con POP:\nlista=[8,9,10,11,12] print(\u0026#34;Valores antes:\u0026#34;, lista) lista.pop(1) print(\u0026#34;Valores después:\u0026#34;, lista) Resultado:\nValores antes: [8, 9, 10, 11, 12]\rValores después: [8, 10, 11, 12]\rEjemplo con DEL\nlista=[8,9,10,11,12] print(\u0026#34;Valores antes:\u0026#34;, lista) del lista[1] print(\u0026#34;Valores después:\u0026#34;, lista) Resultado:\nValores antes: [8, 9, 10, 11, 12]\rValores después: [8, 10, 11, 12]\rEn ambos casos hay que tener en cuenta que el índice empieza por 0.\nAñadir valores\r#\r\rPara añadir se usa el método APPEND de la lista. Ejemplo:\nlista=[8,9,10,11,12] print(\u0026#34;Valores antes:\u0026#34;, lista) lista.append(13) print(\u0026#34;Valores después:\u0026#34;, lista) Resultado:\nValores antes: [8, 9, 10, 11, 12]\rValores después: [8, 9, 10, 11, 12, 13]\rSublistas\r#\r\rEs posible tener una sublista dentro de una lista ejemplo:\nlista=[\u0026#34;valor 1\u0026#34;,2, 3.4,[\u0026#34;subvalor\u0026#34;, 10]] print(\u0026#34;Contenido \u0026#34;, lista) Resultado:\nContenido ['valor 1', 2, 3.4, ['subvalor', 10]]\rSegmentos\r#\r\rIgual que en las cadenas también se puede acceder a segmentos. Ejemplo:\nlista=[1,2,3,4,5,6] print(\u0026#34;Contenido \u0026#34;, lista[3:5]) Resultado:\nContenido [4, 5]\rSi no se pone el primer valor del segmento se comienza a leer desde el primer valor. Ejemplo:\nlista=[1,2,3,4,5,6] print(\u0026#34;Contenido \u0026#34;, lista[:5]) Resultado:\nContenido [1, 2, 3, 4, 5]\rSi queremos leer todos los valores menos el último se puede hacer de la siguiente manera:\nlista=[1,2,3,4,5,6] print(\u0026#34;Contenido \u0026#34;, lista[:-1]) Resultado:\nContenido [1, 2, 3, 4, 5]\rSi ponemos menos saldría -2\nContenido [1, 2, 3, 4]\rY así hasta el inicio del vector.\nSi queremos leer el último registro se puede hacer así:\nlista=[1,2,3,4,5,6] print(\u0026#34;Contenido \u0026#34;, lista[-1]) Resultado:\nContenido 6\rTuplas\r#\r\rEl acceso a los valores de las tuplas se hace igual que con las listas.\nCreación de rangos\r#\r\rEs posible crear rangos con números preasignados que es útil para hacer contadores o cualquier otra cosa.\nEjemplo:\nx = range(6) for n in x: print(\u0026#34;Valor: \u0026#34;, n) Resultado:\nValor: 0\rValor: 1\rValor: 2\rValor: 3\rValor: 4\rValor: 5\rEs posible indicarle el incremento en cada valor:\nx = range(0,6, 2) for n in x: print(\u0026#34;Valor: \u0026#34;, n) Resultado:\nValor: 0\rValor: 2\rValor: 4\r"});index.add({'id':88,'href':'/docs/sap/abap/funciones/logistica/','title':"Logística",'content':"Objetivo\r#\r\rFunciones del módulo de logística. Logística entraría la parte de compras, almacenes, etc.\nLista\r#\r\r   Función Descripción     RM_KOKRS_TO_PLANT_FIND De una centro te devuelve su sociedad FI y CO   CO_RM_COMPANYCODE_FIND De una centro te devuelve su sociedad FI   RK_KOKRS_FIND De una centro te devuelve su sociedad CO   CS_WHERE_USED_MAT De un material devuelve en que lista de material se usa.   ME_READ_HISTORY Historial de documento de compras   BAPISDORDER_GETDETAILEDLIST Se le pasa los pedidos y que se desea recuperar y devuelve informacion completa del pedido   NAST_GET_MESSAGE_OBJECT_RECV Función que devuelve el nombre de la lista de distribucion que esta en el campo NAST-TDNAME. Nota: Se podría haber puesto también en ventas   SO_DLI_READ_API1 Devuelve los componentes de una lista de distribucion   CARO_ROUTING_READ De una hoja de ruta te devuelve sus componentes.   AC_DOCUMENT_RECORD Te devuelve los documentos subsiguientes de un documento. I_AWREF es el numero de documento. I_AWORG es ejercicio. I_AWTYP es el tipo de objeto. RMRP es para la MIRO.    "});index.add({'id':89,'href':'/docs/python/machine_learning/','title':"Machine learning",'content':"Introducción\r#\r\rTodo sobre machine learning en Python\nSecciones\r#\r\rLas sección son las siguientes:\n\r\rLibrerias y paquetes\r\r\rLibrerias y paquetes\r\r\rAlgoritmos de aprendizaje no supervisado\r\r\rAlgoritmos de aprendizaje no supervisado\r\r\rArbol de decisiones\r\r\rArbol de decisiones\r\r\rEstimador incertidumbre\r\r\rEjemplo sobre los estimadores de incertidumbre\r\r\rGuardar modelos entrenados\r\r\rGuardar modelos entrenados\r\r\rProcesamiento de datos\r\r\rProcesamiento de datos\r\r\rRedes neuronales\r\r\rRedes neuronales\r\r\rRegresion lineal\r\r\rRegresion lineal\r\r\rSet de datos propios\r\r\rSet de datos propios\r\r\rTensor Flow\r\r\rTensor Flow\r\r\rUso GPU\r\r\rUsar GPU para utilizar el machine learning\r\r\rVectorización\r\r\rVectorización\r\r\r"});index.add({'id':90,'href':'/docs/sap/abap/sentencias_74/mapeo_campos/','title':"Mapeo de campos",'content':"Introducción\r#\r\rEn este grupo se irán poniendo las distintas sentencias que nos permiten hacer mapeo de campos, digamos que son las que sustituyen al MOVE-CORRESPODING.\nEstas sentencias se pueden utilizar para crear nuevas variables o ponerlas como entrada de parámetro en clases.\nEjemplo 1\r#\r\rLa clase CL_ABAP_CORRESPONDING permite crear mapeos entre tablas y estructuras. En este ejemplo se construye la tabla de mapeos en base a una tabla interna. Nota: Los campos que se informan en el mapeo deben de existir ya que sino se provoca una excepción.\ncl_abap_corresponding=\u0026gt;create( source = is_header destination = es_header_crud mapping = VALUE cl_abap_corresponding=\u0026gt;mapping_table( FOR ls_mapping IN mt_mapping_crud WHERE ( area = \u0026#39;HEADER_NEGOTIATION\u0026#39; AND id_nivel1 = cv_level1 ) ( level = 0 kind = cl_abap_corresponding=\u0026gt;mapping_component srcname = ls_mapping-field_from dstname = ls_mapping-field_to ) ) )-\u0026gt;execute( EXPORTING source = is_header CHANGING destination = es_header_crud ). Ejemplo 2\r#\r\r\u0026lt;ls_summary_all\u0026gt; = CORRESPONDING #( BASE ( \u0026lt;ls_summary_all\u0026gt; ) \u0026lt;ls_datos\u0026gt; ). Se mueven los campos de \u0026lt;ls_datos\u0026gt; a \u0026lt;ls_summary_all\u0026gt;. Esto por defecto hace que los campos que están en \u0026lt;ls_summary_all\u0026gt; pero no en \u0026lt;ls_datos\u0026gt; se dejan en blanco aunque tengan valores. Para eso hay que usar la cláusula BASE y entre paréntesis la estructura destino para que no limpie los campos que no tiene.\n"});index.add({'id':91,'href':'/docs/sap/abap/bopf/nomenclaturas/','title':"Nomenclaturas",'content':"Objetivo\r#\r\rEs tener las indicaciones de las nomenclaturas que uso cuando creo un BOPF. Ya que de esta manera es fácil identificar los objetos a simple vista.\nYo normalmente cuando hago proyecto suele dar un nombre de tres carácteres que usará como prefijo para cualquier objeto que creo, ya sea, BOPF, ya sea, cualquier otro objeto: clases, transacciones, etc.\nEstructuras y tablas de diccionario\r#\r\rEn la siguiente imagen:\n\rEs un ejemplo de la nomencltura utilizada en un proyecto real. NOTA: A día de hoy hay una ligera variación, cambia el nombre de la estructura persistente\nEl esquema sería el siguiente:\n La estructura para los persistentes sería \u0026ndash;\u0026gt; Z\u0026lt;nombre proyecto\u0026gt;_BO_SP_\u0026lt;libre\u0026gt; La estructura de los datos transitorios \u0026ndash;\u0026gt; Z\u0026lt;nombre proyecto\u0026gt;_BO_ST_\u0026lt;libre\u0026gt; La estructura que combina las dos estructura anteriores \u0026ndash;\u0026gt; Z\u0026lt;nombre proyecto\u0026gt;_BO_SC_\u0026lt;libre\u0026gt; El tipo de tabla para la estructura combinada \u0026ndash;\u0026gt; Z\u0026lt;nombre proyecto\u0026gt;_BO_I_\u0026lt;libre\u0026gt; Tabla de base de datos \u0026ndash;\u0026gt; Z\u0026lt;nombre proyecto\u0026gt;_T_\u0026lt;libre\u0026gt;  Determinaciones\r#\r\rSería: ZCL_\u0026lt;nombre proyecto\u0026gt;_D_\u0026lt;nombre bo\u0026gt;_\u0026lt;nodo\u0026gt;\nEjemplo: ZCL_ATRON_D_FILE_ENG_HEADER\nValidaciones\r#\r\rSería: ZCL_\u0026lt;nombre proyecto\u0026gt;_V_\u0026lt;nombre bo\u0026gt;_\u0026lt;nodo\u0026gt;\nEjemplo: ZCL_ATRON_V_FILE_ENG_HEADER\nAcciones\r#\r\rClase\r#\r\rSería: ZCL_\u0026lt;nombre proyecto\u0026gt;_A_\u0026lt;nombre bo\u0026gt;_\u0026lt;nodo\u0026gt;\nEjemplo: ZCL_ATRON_A_FILE_ENG_HEADER\nEstructura para los parámetros y tipo de datos para la salida de datos\r#\r\rEstructura de parámetros\r#\r\rSería: Z\u0026lt;nombre proyecto\u0026gt;_BO_SA_PARAMS_\u0026lt;nombre accion\u0026gt;\u0026gt;\nEl SA sería \u0026ldquo;Structure Action\u0026rdquo;\nEjemplo: ZATRON_BO_SA_PARAMS_LAUNCH_ETL\nEstructura para la salida de datos\r#\r\rSería: Z\u0026lt;nombre proyecto\u0026gt;_BO_SA_EXPORT_\u0026lt;nombre accion\u0026gt;\u0026gt;\nEjemplo: ZATRON_BO_SA_EXPORT_LAUNCH_ETL\nTipo tabla para la salida de datos\r#\r\rSería: Z\u0026lt;nombre proyecto\u0026gt;_BO_IA_EXPORT_\u0026lt;nombre accion\u0026gt;\u0026gt;\nEjemplo: ZATRON_BO_IA_EXPORT_LAUNCH_ETL\nQuerys\r#\r\rSería: ZCL_\u0026lt;nombre proyecto\u0026gt;_Q_\u0026lt;nombre bo\u0026gt;_\u0026lt;nodo\u0026gt;\nEjemplo: ZCL_ATRON_Q_FILE_ENG_HEADER\n"});index.add({'id':92,'href':'/docs/sap/abap/notas/','title':"Notas de SAP",'content':"Objetivo\r#\r\rRecopilar notas que son interesante para entender el funcionamiento de determinados procesos\nLista\r#\r\r   Nota Descripción     1420281 Note 1420281 - CO-OM tools: SE16N: Deactivating \u0026amp;SAP_EDIT   1083986 Explica las BAPIS para el sistema de clasificacion. Y tiene subnotas que explican como funcionan muchas de ellas.    "});index.add({'id':93,'href':'/docs/python/sentencias/operadores/','title':"Operadores",'content':"Operadores\r#\r\rAritméticos\r#\r\rLos operadores son los habituales:\n Sumar: + Resta: - División: / Multiplicación: * Elevar a la potencia: ** Obtener el resto: %  Ejemplos:\nd = a * b print(\u0026#34;Resultado\u0026#34;, d) d = ( a + b ) / a print(\u0026#34;Resultado:\u0026#34;, d) Resultado:\nResultado 416.15999999999997\rResultado: 3.89\rString\r#\r\rCon los string podemos contaner y incluso hacer que un caracter aparezca n veces.\ne=\u0026#34;,que haces?\u0026#34; print(\u0026#34;Concatenación:\u0026#34;,c+e) print(\u0026#34;El string se va a mostrar 3 veces repetido:\u0026#34;,c*3) Resultado:\nConcatenación: hola mundo,que haces?\rEl string se va a mostrar 3 veces repetido: hola mundohola mundohola mundo\r"});index.add({'id':94,'href':'/docs/python/machine_learning/procesamiento_datos/','title':"Procesamiento de datos",'content':"Introducción\r#\r\rTodo sobre el procesamiento de datos.\nObjetivo\r#\r\rEn el ejemplo de conversión de palabras a números se hablaba que es bueno hacer un procesamiento de datos de previo para limpiar los datos para que los algoritmos trabajen más eficientemene.\nEn esta sección se hablará todo lo referente a estos procesos.\nSecciones\r#\r\rLas secciones son las siguientes:\n\r\rProcesamiento de datos con PCA\r\r\rProcesamiento de datos con PCA\r\r\r"});index.add({'id':95,'href':'/docs/python/machine_learning/procesamiento_datos/procesamiento_datos_pca/','title':"Procesamiento de datos con PCA",'content':"Introducción\r#\r\rEjemplo extraído del video Machine Learning episodio 10. Procesamiento de datos (PCA). De este mismo video se extraerán fotos para aclarar mejor conceptos tal como hace el video.\nEl ejemplo esta creado el Jupiter Notebool y se irá poniendo el código de las distintas celdas.\nExplicación\r#\r\rLa librería PCA se utiliza para principalmente dos cosas:\n Reducir el ruido que hay en nuestros datos Poder ver los datos que están en muchas dimensiones en solo dos dimensiones. En muchos set de datos hay treinta características distintas que a nivel gráfico es muy complicado de ver y entender. Con PCA permite reducir las treinta dimensiones en tal solo dos lo que permite ver correlaciones que nos pueda interesar o entender mejor la información.  Para enteder como funciona el algoritmo partimos de la siguiente imagen:\n\rTenemos dos ejes que representan dos características. Lo primero que va hacer detectar PCA es en que dirección hay más variedad de datos que sería la siguiente:\n\rEsta primera línea la llama Principal component 1(PCA1). Lo siguiente que va hacer es encontrar un eje ortoganal, es decir a 90º grados, el cual intentar maximizar la variedad de datos que sería el que se ve en la siguiente imagen en color verde:\n\rEste nueva dirección la va a llamar PCA2. Lo siguiente que hará PCA es tomar estos dos ejes como nuevos parámetros para poderlos mostrar gráficamente todos los puntos que existen. De esta manera va a poder hacer una representación abstracta de nuestros datos.\nCódigo\r#\r\r\rCódigo fuente descargable\nimport sklearn # Librería para representar gráficamente los datos import mglearn # Datos sobre el cancer donde tiene 30 características para # saber si un tumor es maligno o benigno from sklearn.datasets import load_breast_cancer # Librería para la representación gráfica import matplotlib.pyplot as plt # Algoritmo PCA from sklearn.decomposition import PCA # Permite ver las gráficas en nuestro editor jupyter %matplotlib inline # Ejemplo de lo que hace PCA # En el recuadro \u0026#34;original data\u0026#34; se ve como encuentra los dos ejes # explicados anteriormente. El largo es el component 1 y el corto el 2 # Aquí se ve mejor el eje ortogonal # En el recuadro \u0026#34;transformed data\u0026#34; lo que hace es ver los datos horizontalmente # lo que hace es retar el valor promedio a cada datos haciendo que los datos # se centren en el eje 0. # En el recuadro \u0026#34;transformed data...\u0026#34; lo que hace es representar los datos # con el eje horizontal del component 1. El segundo eje va ser las caracaterísticas # (feature 2) de la primera gráfica. Lo que se ve es una línea recta de los datos # que se ven la segunda gráfica al eje del component 1. # En el recuadro \u0026#34;back-rotation...\u0026#34; lo que hace es sumar de nuevo el promedio # que le resto en la segunda gráfica y lo va rotar de nuevo. Con lo cual # queda como en la primera imagen pero eliminando el ruido que habia. mglearn.plots.plot_pca_illustration() Imagen con la imagen que devuelve el ejemplo: # Datos para entrenar el algoritmo cancer = load_breast_cancer() print(\u0026#34;Valor de las características: \u0026#34;, cancer.feature_names) print(\u0026#34;Número de características: \u0026#34;, cancer.feature_names.shape) # Algoritmo PCA donde se le indica el número de ejes. # El eje 1 será el que más variedad de datos talc omo se ve en el gráfico # de ejemplo. El eje 2 que es el ortogonal. pca = PCA(n_components=2) # Datos para el entrenamiento algoritmo pca.fit(cancer.data) # Genera una variable con los datos transformados. # Es decir, transforma las 30 características a las 2 que se # le ha indicado transformada = pca.transform(cancer.data) # Comparativa de las características originales versus la transformada print(\u0026#34;Mediciones, características de los datos originales: \u0026#34;, cancer.data.shape) print(\u0026#34;Mediciones, características de los datos procesados: \u0026#34;, transformada.shape) # Al gráfico se le pasa como eje X los valores principal, y en el 1 las características mglearn.discrete_scatter(transformada[:,0],transformada[:,1], cancer.target) plt.legend(cancer.target_names,loc=\u0026#39;best\u0026#39;) plt.xlabel(\u0026#34;PCA 1\u0026#34;) plt.ylabel(\u0026#34;PCA 2\u0026#34;) El gráfico que aparecerá será el siguiente: Este gráfico es una abstracción de los datos. En el se puede ver un comportamiento en los datos. Los triangulos narajanjas son tumores benignos y están agrupados en el lado izquierdo. Los triangulos azules son los tumores malignos y están agrupados en el lado derecho pero más dispersos algunos puntos.\n# Para ver cuan de importante es procesar los datos y para ver # se va usar la función MinMaxScaler que permitira ver los datos en una escalar similar, # que será un rango de 0 a 1. De esta manera se puede validar que no se procesando datos # con valores muy pequeños contra valores muy grandes. from sklearn.preprocessing import MinMaxScaler escala = MinMaxScaler() # Pasamos los datos a la función escala.fit(cancer.data) # transformamos los datos para que todos los datos esten en rango similar, de 0 a 1. escalada = escala.transform(cancer.data) # Se le pasa los datos tratados al algoritmo PCA pca.fit(escalada) transformada=pca.transform(escalada) mglearn.discrete_scatter(transformada[:,0],transformada[:,1], cancer.target) plt.legend(cancer.target_names,loc=\u0026#39;best\u0026#39;) plt.gca() plt.xlabel(\u0026#34;PCA 1\u0026#34;) plt.ylabel(\u0026#34;PCA 2\u0026#34;) El resultado del gráfico es el siguiente:\n\rEn este gráfico vemos que el rango del eje Y y X se han reducido respecto a los datos originales. Aquí también se puede ver que hay una correlación donde los tumores benignos están hacía la izquierda y los malignos a la derecha. Con lo cual los datos van a servir para hacer predicciones. A partir de aquí se puede decidir si el algoritmo lo entrenamos con la data real o con la transformada. Ya que si le enviamos los datos tratados claramente el algoritmo va trazar una línea que separe los datos de la izquierda y derecha para tratar los valores.\nFinalmente dos sentencias para ver los datos tratados versus los datos sin tratar.\n# Los datos escalados se verán que van siempre de 0 a 1. print(\u0026#34;Ejemplo de datos escalados: \u0026#34;, escalada) # Aquí los datos son más dispersos lo que puede afectar al tratamiento en los algoritmos de clasificación print(\u0026#34;Ejemplo de datos sin escalar: \u0026#34;, cancer.data) "});index.add({'id':96,'href':'/docs/sap/abap/programas/','title':"Programas",'content':"Objetivo\r#\r\rRecopilar programas que hagan determinados procesos o contengan ejemplos. Vamos cualquier programa que me parezca interesante.\nPublicaciones\r#\r\r\r\rLista de programas\r\r\rLista de programas\r\r\r"});index.add({'id':97,'href':'/docs/python/machine_learning/tensorflow/reconocimiento_numeros_escrito_mano/','title':"Reconocimiento números escritos a mano",'content':"Introducción\r#\r\rEjemplo extraído del video Tensorflow: Cómo clasificar números escritos a mano.\nEl ejemplo esta creado el Jupiter Notebool y se irá poniendo el código de las distintas celdas.\nNOTA: El código del video esta basado en Tensor Flow 1.0. Pero el Tensor Flow que se ha instalado es la version 2.x, la llamaremos TF2.0 o TF20 . Por lo tanto hay muchas cosas que no son compatibles. En el código habrá partes las principales que se indique que no se usa en TF20, pero el resto de partes se pondrá en código TF20 para no poner demasiada basura en el código\nEste ejemplo según el video es el clásico Hola mundo de las redes neuronales. El problema se basa al que al algoritmo se le van a pasar uno números escritos a mano y el nos va a devolver cual es el número. En el video lo llamada el problema de MNIST. Pero en Wikipedia indica que es una base de datos de números escritos a mano que se usa para entrenar algoritmos.\nEn el ejemplo se va a crear un red neuronal que solo va a tener dos capas:\n Capa que recibe los pixeles de las imagenes Capa de salida que será un vector de diez elementos donde nos dirá que número del 0 al 9 se corresponde la imagen introducida  AVISO IMPORTANTE 1\nEl paquete de set de datos de MINST no se incluye por defecto al instalar el paquete Tensor Flow vía Anaconda, ni siquiera aparece instalando el paquete tensorflow-datasets. La solución más sencilla es:\n Descargarse, o clonar lo que se quiera, el repositorio de Github y copiar la carpeta \u0026ldquo;tutoriales Descomprimir o navegar dentro del zip a la carpeta tensorflow\\examples, estará la carpeta tutorials. Copiarla. Pegar en el directorio del entorno de Anaconda donde ejecutaremos el ejemplo. En mi caso esta aquí: D:\\Users\\ivan\\anaconda3\\envs\\test\\Lib\\site-packages\\tensorflow_core\\examples Con eso ya no dará error al importar el set de datos.  AVISO IMPORTANTE 2\nEste ejemplo en el video esta creado con Tensor Flow 1. Que no funciona con la versión 2, he intentado migrarlo no me ha sido posible por falta de conocimientos y porque no hay manera de conseguir que esto funcione.\nPor eso, uso un Keras porque es un middleware muy usado y que es para los principiantes, como yo, simplifica el uso de redes de neuronales. El ejemplo me he basado en este excelente artículo donde se explica paso a paso como hacerlo.\nAún así, voy a poner tres versiones de código:\n Código del video con la opción de compatibilidad con Tensor Flow 2. Codigo migrado por mi que no funciona. Lo pongo por si algún día soy capaz de hacerlo funcionar. Código que funciona usando parte de mi código ya migrado.  Versión del video, compatible con Tensor Flow 2\r#\r\r\rCódigo fuente descargable\n#import tensorflow as tf import tensorflow.compat.v1 as tf tf.disable_v2_behavior() from tensorflow.examples.tutorials.mnist import input_data mnist=input_data.read_data_sets(\u0026#34;MNIST_data/\u0026#34;, one_hot=True) #La imagenes tienen dimension de 28x28 x=tf.placeholder(tf.float32,[None,784]) #imagen del numero descompuesta a un vector P=tf.Variable(tf.zeros([784,10])) #Matriz de pesos, 784 para recibir la imagen, 10 por las posible salidas b=tf.Variable(tf.zeros([10])) #Vector con bias y=tf.matmul(x,P)+b #La operacion que se hara en los nodos que reciben entradas yR=tf.placeholder(tf.float32,[None,10]) # Matriz con las etiquetas REALES del set de datos softmax=tf.nn.softmax_cross_entropy_with_logits(labels=yR,logits=y) costo=tf.reduce_mean(softmax) optimizador=tf.train.GradientDescentOptimizer(0.5).minimize(costo) prediccion = tf.equal(tf.argmax(y, 1), tf.argmax(yR, 1)) #Nos da arreglo de booleanos para decirnos #cuales estan bien y cuales no accuracy = tf.reduce_mean(tf.cast(prediccion, tf.float32))#Nos da el porcentaje sobre el arreglo de prediccion Produccion = tf.argmax(y,1) init=tf.global_variables_initializer() #Funcion que usaremos para ver que tan bien va a aprendiendo nuestro modelo def avance(epoca_i, sess, last_features, last_labels): costoActual = sess.run(costo,feed_dict={x: last_features, yR: last_labels}) Certeza = sess.run(accuracy,feed_dict={x:mnist.validation.images,yR: mnist.validation.labels}) print(\u0026#39;Epoca: {:\u0026lt;4} - Costo: {:\u0026lt;8.3} Certeza: {:\u0026lt;5.3}\u0026#39;.format(epoca_i,costoActual,Certeza)) with tf.Session() as sess: sess.run(init) for epoca_i in range(100): lotex, lotey = mnist.train.next_batch(100) sess.run(optimizador, feed_dict={x: lotex, yR: lotey}) if (epoca_i%50==0): avance(epoca_i, sess, lotex, lotey) print(\u0026#39;RESULTADO FINAL: \u0026#39;,sess.run(accuracy, feed_dict={x: mnist.test.images,yR: mnist.test.labels})) print (\u0026#39;Resultado de una imagen\u0026#39;,sess.run(Produccion,feed_dict={x: mnist.test.images[5].reshape(1,784)})) mnist.test.labels[5] Versión código migrado que no funciona\r#\r\r\rCódigo fuente descargable\n#import tensorflow as tf import tensorflow.compat.v1 as tf tf.disable_v2_behavior() from tensorflow.examples.tutorials.mnist import input_data # Libreria tensor flowe import tensorflow as tf # Daset de datos con números escritos a manorom from tensorflow.examples.tutorials.mnist import input_data import tensorflow_datasets as tfds # API de tensorfow para construir y entrenar modelos de alto nivel from tensorflow import keras from keras.utils.np_utils import to_categorical # Se recuperan los datos y el metadata de los datos #mnist_train2, mnist_info = tfds.load(\u0026#39;mnist\u0026#39;, split=\u0026#39;train\u0026#39;, as_supervised=True, shuffle_files=True,with_info=True) # De los datos, se extrae los datos de entrenamiento y test mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape[0], 784) x_test = x_test.reshape(x_test.shape[0], 784) # El formato original esta en \u0026#34;unit8\u0026#34; con lo que se tiene que convertir en un float32 para que funcione x_train = x_train.astype(\u0026#39;float32\u0026#39;) x_test = x_test.astype(\u0026#39;float32\u0026#39;) #Se divide la imagen en 255 porque es la escala de grises y es mejor operar con 0 y 1. Más sencillo x_train, x_test = x_train / 255.0, x_test / 255.0 # Paso necesario porque devuelve los registros y toda la información en la misma columna. Pero # se necesita esos valores esten en un matriz de 10 columnas para calcular bien los acietros. y_train = to_categorical(y_train, 10) y_test = to_categorical(y_test, 10) mnist_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)) # El ejemplo de .map es lo mismo que se ha hecho arriba cuando se han leido todos los datos # El TFDS devuelve las imagenes en uint8, mientras que la red espera float 32 # hay que normalizar para que funcione #def normalize_img(image, label): # \u0026#34;\u0026#34;\u0026#34;Normalizes images: `uint8` -\u0026gt; `float32`.\u0026#34;\u0026#34;\u0026#34; # return tf.cast(image, tf.float32) / 255, label # # map permite realizar una transformación a cada uno de los datos del conjuntos de datos # que lo compone. En este caso lo que se se hace convertir las imagenes del TDFS al formato # compatible de la red. #mnist_train = mnist_train.map( # normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE) # Activa que los datos se guarden cache para mejorar el rendimiento mnist_train = mnist_train.cache() # Activa que los datos se recuperan de manera aleatoria #mnist_train = mnist_train.shuffle(mnist_info.splits[\u0026#39;train\u0026#39;].num_examples) # Número de lotes que hará en cada iteracción mnist_train = mnist_train.batch(100) # Imagen del número descompuesto a un vector. Se informa que el vector es de 784 porque es # multiplicación de 28 x 28 que son los pixeles de la imagen # El valor None es para indicar que el valor, o filas, son variables según el número de imagenes x = tf.keras.Input(name=\u0026#39;x\u0026#39;, shape=(None,784), dtype=tf.float32) # Matriz de pesos, que se inicializa a 0, con el tf.zeros. # La matriz tiene 784 filas, que son el número de pixeles. Y 10 columnas porque # la siguienet capa tiene 10 elementos. La siguiente capa es la de salida P = tf.Variable(tf.zeros([784,10])) # Son los bias, o sesgo. Será un vector de 10 elementos inicializado a 0. b = tf.Variable(tf.zeros([10])) # Es la operación de salida. Aquí es la formula que se explica en la parte de redes # neuronales, como funciona. # la formula es la multiplicación de la matriz de entrada * peso, más el sesgo. y = tf.matmul(x,P)+b # Matriz con las etiquetas reales del set de datos. Que tendrá un nuevo variable de filas # y 10 columnas. yR = tf.keras.Input(name=\u0026#39;yR\u0026#39;, shape=(None,10), dtype=tf.float32) print(\u0026#34;x \u0026#34;, x.shape) print(\u0026#34;P \u0026#34;, P.shape) print(\u0026#34;b \u0026#34;, b.shape) print(\u0026#34;yr \u0026#34;, yR.shape) print(\u0026#34;y \u0026#34;, y.shape) def compute_loss(): log_x = tf.math.log(x) m = tf.math.square(log_x) return m # Funcion para ver como esta aprendiendo el algoritmo def avance(epoca_i,X_data, Y_data): x = X_data yR = Y_data y = tf.matmul(X_data,P)+b #print(\u0026#34;x \u0026#34;, x.shape) #print(\u0026#34;P \u0026#34;, P.shape) #print(\u0026#34;b \u0026#34;, b.shape) #print(\u0026#34;yr \u0026#34;, Y_data.shape) #print(\u0026#34;y \u0026#34;, y.shape) # Se definen los algoritmos de como va aprender la red en cada una de las iteracciones # Softmax nos va dar un vector de 10 elementos para generar la predicción pero en probabilidades # Esto nos va a dar un vector donde en cada posición nos va dar el % de probalidades que sea. # Es la libreria es la que nos va dar el error de nuestra predicciones. Se le pasará las # etiquetas de los números y la salida de la red para que nos diga que tan bien o tan mal esta # el algoritmo para repartir los pesos. softmax = tf.nn.softmax_cross_entropy_with_logits(labels=Y_data,logits=y) # Es la función del coste de la predicción este número tiene que tender a 0. costo = tf.reduce_mean(softmax) # Código original tf.train.GradientDescentOptimizer(0.5).minimize(costo) # Es el que permitir ajustar los pesos de nuestra matriz. Es decir, si el costo = 2, ajustará los pesos # para que ese costo tienda a 0. optimizador = tf.keras.optimizers.SGD(learning_rate=0.5,momentum=0.0, nesterov=False) #.minimize(compute_loss,costo) print(optimizador) # Da el arreglo de booleanos para decirnos cuales estan bien y cuales están mal # argmax nos va dar el valor más alto de todas las predicciones que hizo softmax. # Y se va comparar con los datos reales para decirnos si se hizo bien o mal la # predicción #prediccion = tf.equal(tf.argmax(y,1),tf.argmax(yR,1)) # Da el % sobre el arreglo de predicción. Si en la variable de predicción hay # 10 elementos, 5 están bien y 5 están mal. El % será edl 50% #accuracy = tf.reduce_mean(tf.cast(prediccion, tf.float32)) #if(epoca_i%50==0): # print(\u0026#39;Epoca: {:\u0026lt;4} - Costo: {:\u0026lt;8.3} Certeza: {:\u0026lt;5.3}\u0026#39;.format(epoca_i,costo,accuracy)) for epoch in range(1): ds = mnist_train.take(10) for image, label in tfds.as_numpy(ds): avance(epoch, image, label) Versión que funciona usando Keras\r#\r\r\rCódigo fuente descargable\nimport tensorflow as tf # Daset de datos con números escritos a manorom from tensorflow.examples.tutorials.mnist import input_data # API de tensorfow para construir y entrenar modelos de alto nivel from tensorflow import keras from keras.utils.np_utils import to_categorical # Librería para representación gráfica import matplotlib.pyplot as plt # Librería para tratar arrays import numpy as np # Se recuperan los datos y el metadata de los datos #mnist_train2, mnist_info = tfds.load(\u0026#39;mnist\u0026#39;, split=\u0026#39;train\u0026#39;, as_supervised=True, shuffle_files=True,with_info=True) # De los datos, se extrae los datos de entrenamiento y test mnist = tf.keras.datasets.mnist (x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = mnist.load_data() x_train = x_train_orig.reshape(x_train.shape[0], 784) x_test = x_test_orig.reshape(x_test.shape[0], 784) # El formato original esta en \u0026#34;unit8\u0026#34; con lo que se tiene que convertir en un float32 para que funcione x_train = x_train.astype(\u0026#39;float32\u0026#39;) x_test = x_test.astype(\u0026#39;float32\u0026#39;) #Se divide la imagen en 255 porque es la escala de grises y es mejor operar con 0 y 1. Más sencillo x_train, x_test = x_train / 255.0, x_test / 255.0 # Paso necesario porque devuelve los registros y toda la información en la misma columna. Pero # se necesita esos valores esten en un matriz de 10 columnas para calcular bien los acietros. y_train = to_categorical(y_train_orig, 10) y_test = to_categorical(y_test_orig, 10) # El modelo keras se basa en capas donde vas añadiendolas para # ir construyendo la red neuronal model = tf.keras.Sequential() # La función sigmoid es la que hace el calculo de peso. Haría primero: # 1. y = tf.matmul(x,P)+b en la versión que no funciona # 2. Luego devolvería 0 o 1 según se acerque al valor. Que sería, creo, calcular el gradiente # descendiente. Es por ello que los datos de entrada se # dividen entre 255 para obtener un valor entre 0 y 1. model.add(tf.keras.layers.Dense(10,activation=\u0026#39;sigmoid\u0026#39;, input_shape=(784,))) # Esta función es la que se encarga de calcular la probabilidad que # el resultado devuelto en el paso anterior a que número pertenzca. Por eso # la red neuronal tiene 10 columnas, y las etiquetas del set de datos # se han convertido a 10. model.add(tf.keras.layers.Dense(10,activation=\u0026#39;softmax\u0026#39;)) # Este paso es opcional y permite ver los parámetros de cada capa que se añade # La primera es que se usa para el calculo de pesos y la gradiente descendiente # y tiene 7850. Que son 784 de la entrada de datos(28x28)*10(número de neuronas de entrada) # + 10 sesgo. # La segunda capa son 10 neuronas de salida *10 del paso anterior + 10 del sesgo model.summary() Resultado:\nModel: \u0026quot;sequential\u0026quot;\r_________________________________________________________________\rLayer (type) Output Shape Param # =================================================================\rdense (Dense) (None, 10) 7850 _________________________________________________________________\rdense_1 (Dense) (None, 10) 110 =================================================================\rTotal params: 7,960\rTrainable params: 7,960\rNon-trainable params: 0\r# Este es el paso de configuración del aprendizaje del modelo # El parámetro \u0026#34;loss\u0026#34; es que la algoritmo/proceso de calculo en el softmax. # En el parámetro \u0026#34;optimizer\u0026#34; es el algoritmo que se usa para el calculo de peso y el gradiente descendiente # El parámetro \u0026#34;metrics\u0026#34; sirve para monitorizar el proceso de entrenamiento. Hay varias metricas que se puede usar # pero en el ejemplo solo usa esta. model.compile(loss=\u0026#34;categorical_crossentropy\u0026#34;, optimizer=\u0026#34;sgd\u0026#34;,metrics = [\u0026#39;accuracy\u0026#39;]) # Ahora toca entrenar el modelo. Se le pasan los datos de entrenamiento y sus etiquetas # El nuúmero de \u0026#34;epochs\u0026#34; es la veces que se va entrenar el algoritmo # Hay un parámetro que no esta que es el batch_size que indica cuantos datos se van a usar # para calcular los parámetro del modelo(los pesos) # El parámetro verbose es para indicar si nos va dar salida de los pasos que va haciendo # 0 = no sale nada. 1 barra de progreso. 2 una linea por epoch model.fit(x_train, y_train, epochs=10, verbose=1) Imagen resumen de como sería su proceso de aprendizaje: # Evaluar el modelo a partir de los datos de test. test_loss, test_acc = model.evaluate(x_test, y_test) # En el paso anterior devolverá el % de acierto porque al modelo le hemos indicado # que muestre el progreso que va haciendo. Pero también se puede saber # mostrando el resultado de la variable print(\u0026#34;% de acierto\u0026#34;, test_acc) En esta parte se puede ir cambiando el valor de image para poder ver que imagen es, y ver si la predice bien o mal.\n# Ahora vamos a predecir el número de una imagen de test. # Numero de imagen que vamos a usar, será 5. Y la mostramos antes de nada image = 5 # Para que se visualice hay que usarl el modelo de original sin transformar plt.imshow(x_test_orig[image], cmap=plt.cm.binary) # Le pasamos al modelo los datos que se usarán para la predicción. Que es el transformado prediction = model.predict(x_test) # el np.argmax nos dice el índice del vector que tiene la posición más alta. # La salida de la red es una matriz de 10 columnas. Donde en cada una de ellas contendrá 0 o 1 # según la predicción del modelo. print(\u0026#34;Número que se predice: \u0026#34;, np.argmax(prediction[image]) ) "});index.add({'id':98,'href':'/docs/sap/abap/recursos_humanos/','title':"Recursos humanos",'content':"Objetivo\r#\r\rRecopilar información útil sobre la programación en recursos humanos\nPublicaciones\r#\r\r\r\rInformación general\r\r\rInformación general\r\r\r"});index.add({'id':99,'href':'/docs/python/machine_learning/redes_neuronales/redes_convolucionales/','title':"Redes convolucionales",'content':"Introducción\r#\r\rEjemplo extraído de Redes neuronales convolucionales CNN (Clasificación de imagenes). Se pondrán imagens obtenidas del video para explicar mejor el funcionamiento tal como lo hace en el video.\nComo se explico en la página de tipos de redes neuronales este tipo de red nace para procesar de una manera eficiente imagenes, aunque también se usa para procesamiento de texto. Pero realmente su fuerte es el procesamiento de imagenes.\nExplicación\r#\r\rLa estructura de este tipo de redes es la siguiente:\n\rDentro de las capas ocultas se hacen dos tipos de operaciones:\n Pooling o agrupación. Que lo hace es reducir el tamaño de la imagen para que cuando se pase a la siguiente capa sea menos pesado procesarla:  \rConvoluciones. Diriamos que ir pasandole filtros a nuestra imagen para ir detectando ciertos patrones en la imagen:  \rPara entender mejor como funcionan vamos con el problema de clasificar imagenes. Se le pasa un animal y se tiene que identificar que es: perro, ave, etc.\nPor ejemplo tenemos la siguiente imagen de entrada: En una red neuronal DNN se mirará la longitud, altura y la profundidad (son 3 capas ya que hay 3 canales colores: RGB). Primero lo que haría es calcular el número de conexiones, si asumimos que la imagen es de 100x100 pixeles, que hay que multiplicar * 3 por el número de capas de colores, y la capa oculta tiene 100 neuronas. Esto haría 3 millones de conexiones. Además, este tipo de red no aprendería bien los animales, ya que lo que queremos es que si el animal esta arriba, abajo o donde sea de la imagen sea capaz de identificarlo. Y eso la red DNN no son capaces de hacerlo. Es cuando las redes CNN son más eficientes.\nLas red CNN no tomará de inicio los pixeles de la imagen y los comenzaría a procesar. Sino que va a empezar a convolucionar la imagen, es decir, para ir procesando la imagen por zonas. De esta manera al no procesar la imagen en su totalidad el número de conexiones es mucho menor. Lo que va haciendo en cada convolución:\n\rEs ir procesando partes de la imagen para generando nuevas imagenes de salida:\n\rDonde serán más pequeñas tanto en el ancho como en el alto pero va aumentar en profundidad.\nEl siguiente paso es el pooling. Este paso aún va a reducir más la altura y la anchura de la imagen pero va dejar intacta la profundidad.\nLos parámetro importantes de la capa convolucional son:\n Tamaño del filtro. Es el tamaño de la imagen que se va ir procesando en cada capa. Se indicará tanto la anchura como la altura. Profundidad de la capa. Es el número de filtros que se le va aplicar a nuestra imagen.  \rEn la imagen el filtro 1 sería la capa roja, el filtro 2 la capa azul, el filtro 3 la capa verde y el filtro 4 la capa amarilla. Cada filtro que se aplica se genera una nueva imagen. Cada filtro se aplica sobre la imagen original pero aplicando otros patrones de búsqueda para detectar cosas que no han sido detectadas por los filtros anteriores. Un primer filtro puede detectar bordes, un segunfo filtro, sombras, etc. Estos filtros son los que van a ir incrementando la profundida de la imagen, ya que son el número de filtros que se van aplicando.\nStride o Paso. Es como se va ir recorriendo la imagen en cada filtro.  \rEs decir, cuantos pixeles, longitud y altitud, se van a ir procesando en cada filtro. Hay que tener en cuenta que contra más sea el paso más se va a reducir la imagen.\nEl paso final de la convolución es el pooling. Este paso ya se ha indicado que reduce todavía más el tamaño de la imagen. El motivo son dos:\n Para quitar el número de conexiones y no sea tan pesado procesar la imagen. Y ayuda a no sobreajustar el modelo.  Dentro del pooling hay dos tipos:\n Maxpooling. average pooling  Ambos lo que hacen toman los valores indicados en la longitud y latitud del filtro para:\n\rEn maxpooling toma el valor más alto de cada zona leída por el filtro. Mientras que el avergage toma el promedio de los valores de los pixeles. El resultado sirve para componer la siguiente imagen.\nEl orden de ejecución es siempre primero convolución y segundo el pooling, así por cada capa oculta de la red.\nA medida que la red va llegando al final la convolución va aplicando filtros cada vez más elaborados. En los primeros filtros de la red se detectarán rayas, sombras colores pero a medida que avanza va identificando elementos más elaborados:\n\r"});index.add({'id':100,'href':'/docs/python/machine_learning/redes_neuronales/','title':"Redes neuronales",'content':"Introducción\r#\r\rTodo sobre redes neuronales\nQue son\r#\r\rLas redes nacen para poder emular el cerebro humano. Lo que se intentar es construir nodos en una máquina, y que estos nodos esten comunicados y se transfieren información entre ellos.\nComo funciona el algoritmo\r#\r\rEjemplo extraído de Machine Learning episodio 6. Redes neuronales. Se pondrán imagens obtenidas del video para explicar mejor el funcionamiento tal como lo hace en el video.\nEn la siguiente imagen tenemos un ejemplo de diagrama de una red neuronal:\n\rEn este diagrama tenemos cuatro capas de nodos interconectados entre ellos.\nLos nodos de la izquierda del todo son los datos son los que se van a usar para alimentar el algortimo.\nLas capas ocultas son aquellas capas que no son ni de entrada ni de salida. Se llaman así porque se no se saben muy bien lo que hacen, solo que se pasan información entre ellas para ir alterando los valores para obtener predicciones más acertadas en el futuro.\nLas capas de salida son las que devolverán los datos del algoritmo. En el caso de los datos de prueba de Iris(clasificación de las flores) serían el tipo de flor a la que pertenece.\nCada nodo del diagrama tiene un peso y un valor que se lo irá pasando al resto de nodos. Por ejemplo si estamos en el primer nodo de la segunda capa (la de la capa oculta), este nodo va a recibir información de los cuatro nodo de entrada. La información que recibe se divide en dos: Por un lado el valor que es la X, de la formula de la imagen, y su peso que es la W al que se le multiplará a X. A ese valor, que indicará que tan importante es dicho valor, se le sumará B. B es el bias, o sesgo, en castellano. Esto indicará que tan propensa es el nodo a activarse, como una especie de interruptor.\nEn este ejemplo:\n\rindica que Y se activará si el valor obtenido supera el 0,5. Con los valores de la imagen el nodo no se activará y no enviará información a las siguientes capas.\nAhora si cambiamos los valores que recibe por los siguientes: El nodo si que se va activar, y enviará información a la siguiente capa.\nVolviendo al diagrama de la red. Lo que hace el algoritmo de entrenamiento es como sabe las respuestas que debe esta recibiendo es ir modificando los valores de W y B para que las últimas capas de salida se activen cuando sea necesario. De esta manera es como el algoritmo aprende.\nSecciones\r#\r\rLas secciones son las siguientes:\n\r\rComo funcionan las redes neuronales\r\r\rComo funcionan las redes neuronales\r\r\rEjemplo de redes neuronales\r\r\rEjemplo de redes neuronales usando Sklearn\r\r\rRedes convolucionales\r\r\rRedes convolucionales\r\r\rTipos de redes redes neuronales\r\r\rTipos de redes redes neuronales\r\r\r"});index.add({'id':101,'href':'/docs/python/machine_learning/regresion_lineal/','title':"Regresion lineal",'content':"Introducción\r#\r\rTodo sobre regresion lineal\nPara oider usar los algoritmos de regresion líneas en el ambiente de anaconda tiene que estar instalado la librería Scikit learn se puede obtener más información en la página que habla sobre las librerías\nSecciones\r#\r\rLas secciones son las siguientes:\n\r\rEntrenar algoritmo de regresion lineal\r\r\rEjemplo de entrenamiento de algoritmo de regresion lineal\r\r\r"});index.add({'id':102,'href':'/docs/sap/abap/sentencias_74/relleno_valores/','title':"Relleno de valores",'content':"Introducción\r#\r\rEn este grupo se irán poniendo las distintas sentencias, que son las que más uso, para poder informar tablas internas en base a otras tablas internas.\nEstas sentencias se pueden utilizar para crear nuevas variables o ponerlas como entrada de parámetro en clases.\nLa sentencia estrella de esta página es la FOR. Pero ojo: que si usamos field-symbols/variable no tiene que estar declarada previamente.\nEjemplo 1\r#\r\rSe recorren los datos de MT_TIPPQPS filtrando por dos campos y se rellenando los valores de ET_COND_CRUD con los datos de la estructura leída y de otras variables:\net_cond_crud = VALUE #( FOR ls_tippqps IN mt_tippqps WHERE ( pqtyp = is_header-negotiation_type_code AND keyfield = lv_keyfield ) ( kschl = ls_tippqps-kschl kotab = ls_tippqps-kotab keyvalue = is_header-customer_code datab = is_header-validity_to datbi = is_header-validity_from updkz = zif_rtn_data=\u0026gt;cv_updkz_insert ) ). Nota: Si et_cond_crud tuviera datos se perderían, si se quieren añadir los datos a los existentes hay que añadir la opción BASE:\nUtilización de la opción BASE\r#\r\rBASE es una opción del VALUE que permite que los datos tiene la tabla donde se van a volcar los datos no se pierdan. Pero tiene un funcionamiento distinto si es una estructura o tabla interna.\nSi es una tabla interna funciona de esta manera:\nlt_header_selparams = VALUE #( BASE lt_header_selparams FOR \u0026lt;wa\u0026gt; IN s_soldto ( attribute_name = zif_car_bo_orders_c=\u0026gt;sc_node_attribute-root-soldto sign = \u0026#39;I\u0026#39; option = \u0026#39;EQ\u0026#39; low = \u0026lt;wa\u0026gt;-low ) ). Si es una estructura va de la siguiente manera:\n\u0026lt;ls_datos\u0026gt; = CORRESPONDING #( BASE ( \u0026lt;ls_datos\u0026gt; ) \u0026lt;ls_positions\u0026gt; ). La diferencia radica en que la tabla interna no lleva paréntesis y la estructura si.\nEjemplo 2\r#\r\rAmpliación de la sentencia INSERT de siempre pero con en este caso no es necesario crear estructuras intermediante o usar el insert/append a un field-symbols y luego informar los valores\nINSERT VALUE #( sign = \u0026#39;I\u0026#39; option = \u0026#39;EQ\u0026#39; low = abap_false ) INTO TABLE lt_params_sl. Ejemplo usando CORRESPONDING\r#\r\rHacer un insert + corresponding en una tabla interna.\nINSERT CORRESPONDING #( \u0026lt;ls_orders\u0026gt; ) INTO TABLE lt_positions. "});index.add({'id':103,'href':'/docs/sap/abap/rtts/','title':"RTTS",'content':"ABAP\r#\r\rEl RTTS, Runtime Type Services, es un conjunto de clases que permite la creación de variables, tablas internas de manera dinámica.\nEl RTTS es una herramienta tremendamente útil para crear objetos de manera dinámica de una manera bastante sencilla. La jerarquía de clases es la siguiente:\nCL_ABAP_TYPEDESCR | |--CL_ABAP_DATADESCR | | | |--CL_ABAP_ELEMDESCR | |--CL_ABAP_REFDESCR | |--CL_ABAP_COMPLEXDESCR | | | |--CL_ABAP_STRUCTDESCR | |--CL_ABAP_TABLEDESCR | |--CL_ABAP_OBJECTDESCR | |--CL_ABAP_CLASSDESCR |--CL_ABAP_INTFDESCR Aquí se irán poniendo ejemplos diversos sobre el RTTS.\nPublicaciones\r#\r\r\r\rEstructuras\r\r\rEstructuras\r\r\r"});index.add({'id':104,'href':'/docs/python/sentencias/','title':"Sentencias",'content':"Introducción\r#\r\rSentencias propias del lenguaje.\nComo nota estoy usando para hacer las pruebas el Jupyter Notebook porque directamente pones el código y pulsando CTRL+ENTER se ejecuta y lo ves justo debajo.\nComentarios\r#\r\rLos comentarios se ponen con el carácter #\nUso del \u0026ldquo;:\u0026rdquo; para sentencia con bloques de código\r#\r\rEn las sentencias que tienen dentro de ellas bloques de código: IF, WHILE, FOR,etc, hay que poner el carácter \u0026ldquo;:\u0026rdquo; al final de la sentencia. Ejemplo:\nif condicion 1 == condicion2: while n \u0026lt; 10: Código dentro de sentencias\r#\r\rA diferencia de otros lenguajes que usán, o bien, carácteres especiales (como el {} como en Javascript), o bien, sentencias de inicio y fin para identificar el código que hay dentro de una sentencia: IF, FOR, etc.. En Python eso se hace identado, ya sea con un espacio en blanco o tabulacion, para indicar que las líneas de código pertenecen a una sentencia. Por ejemplo:\nif a \u0026lt;\u0026gt; b: sentencia 1 del IF sentencia 2 del IF setencia fuera del IF Si no identamos entonces va a dar un mensaje de error porque se va pensar que la sentencia, en este caso el IF, no tiene código.\nVisualizar datos\r#\r\rCon el comando print podemos visualizar el contenido de variables\nprint(a) print(\u0026#34;El valor de a es:\u0026#34;,a) Resultado:\n12\rEl valor de a es: 12\rSecciones\r#\r\rEl detalle de sentencias más especificamente:\n\r\rEntrada de datos\r\r\rEntrada de datos\r\r\rVariables\r\r\rVariables\r\r\rCadenas\r\r\rCadenas\r\r\rCaptura de excepciones\r\r\rCaptura de excepciones\r\r\rCondiciones\r\r\rCondiciones\r\r\rConstantes internas\r\r\rConstantes internas\r\r\rCrear paquetes y librerías\r\r\rCrear paquetes y librerías propias\r\r\rDiccionario\r\r\rDiccionario\r\r\rFicheros\r\r\rFicheros\r\r\rFunciones internas\r\r\rFunciones internas\r\r\rIteracciones\r\r\rIteracciones\r\r\rListas y tuplas\r\r\rListas y tuplas\r\r\rOperadores\r\r\rOperadores\r\r\rSistema\r\r\rSistema\r\r\rUsar librerias\r\r\rUsar librerias\r\r\rVisualizar datos\r\r\rVisualizar datos\r\r\r"});index.add({'id':105,'href':'/docs/python/framework/django/publicar_servicio_ficheros/','title':"Servicios con ficheros",'content':"Pre-Introducción :-)\r#\r\rAntes de nada explicar como se guardan los ficheros usando Django. Los ficheros en vez de guardarse en binario en un campo de la base de datos aquí por lo que que leído, se guardan en un directorio interno y en base de datos se guarda la ruta a dicha fichero. Por lo cual, cuando el servicio que devuelve el fichero no devuelve el contenido. Devuelve la url directamente.\nEn SAP, que es lo que más experiencia tengo, los ficheros solo guardar el binario en una tabla. Pero si que es verdad que cuando lo devuelve lo hago a través de una URL. Esto es mucho más óptimo para todos, tanto backend como frontend.\nOtra nota importante. Es que cuando se lanza el servicio de borrado, se borra los datos en el modelo pero no el directorio que se guarda. Esto he léido es para garantizar integridad y bla bla. Yo lo veo, que en ese caso se tendría que tener un proceso de borrado de ficheros huerfanos.\nOtro punto es que si en el mismo path donde se guardan las imagenes hay un fichero con el mismo nombre le añade un sufijo final para distinguirlo. Eso esta muy bien.\nA modo de prueba personal, he intentado guardarme el binario en un campo del modelo pero me ha sido imposible conseguir. Consigo, eso creo, recuperar el binario en la petición HTTP pero no consigo grabarla en el modelo.\nIntroducción\r#\r\rEl objetivo es explicar como funcionan los servicios que operan con ficheros. Esto por si solo se puede ir haciendo con la documentación oficial, y algun que otro ejemplo encontrado por ahí. Pero hay dos cosas que me han obligado a hacerlo a medida:\n Cuando se recibe un fichero quiero guardar el nombre del fichero y tipo de fichero. Quiero borrar el fichero en el directorio donde se ha guardado cuando se lanze el proceso de borrado.  Las vistas del ejemplo son las de tipo viewSets de Django Rest Framework, de aquí en adelante DRF. El mótivo es que este tipo de vistas implementan de manera automática los procesos CRUD sin necesidad de añadir código. Y a mi me interesa sobrecargar la creación y borrado, pero el de lectura y lista quiero seguir usando los que me ofrece DRF.\nComo apunte final las pruebas del servicio las he hecho con el postman.\nLa estructura de Django es:\n Nombre de proyecto: backend Nombre de aplicación: invoices  Con lo que a nivel de directorio tenemos:\nbackend |\u0026mdash;backend -\u0026gt; Donde esta la configuración del proyecto |\u0026mdash;invoices |\u0026mdash;media -\u0026gt; Carpeta donde se guardará los archivos\nConfiguración de Django\r#\r\rConfiguración general\r#\r\rPara poder usar ficheros a Django hay que decirle donde lo tiene que guardar. Para eso hay que localizar el fichero settings.py del proyecto (que esta en la carpeta, del mismo nombre, dentro de la carpeta del proyecto). En ese fichero hay que añadir la siguiente línea:\n# Directorio donde se guardan los archivos de imagen, video, eetc.. MEDIA_URL = \u0026#39;/media/\u0026#39; MEDIA_ROOT = os.path.join(BASE_DIR, \u0026#34;media\u0026#34;) El primero indica la carpeta dentro del proyecto que se creará (no hace falta crearla de antemano, se crea de manera automática). Y al segunda el path completo donde estará la carpeta en el sistema operativo. Esto sirve para que cuando tengamos el objeto de campo ImageField habrá un atributo donde esta la ruta completa del fichero. Útil para borrar el fichero, leerlo, etc.\nConfiguración de URLs\r#\r\rEn el fichero urls.py del proyecto hay que añadir lo siguiente a las URLs que tengamos configuradas:\nfrom django.contrib import admin from django.urls import path, include from invoices import urls from django.conf import settings from django.conf.urls.static import static urlpatterns = [ path(\u0026#39;admin/\u0026#39;, admin.site.urls) ] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) Se añade el + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) para añadir en los archivos estáticos del proyecto el path de las imagenes\nEjemplo de recepción del fichero\r#\r\rModelo de datos\r#\r\rEn el fichero models.py de la aplicación se añade el siguiente modelo:\nimport uuid from django.db import models from django.utils import timezone class invoiceImage(models.Model): id_file = models.UUIDField( default=uuid.uuid4, primary_key=True, editable=False) file_name = models.CharField(max_length=100, default=\u0026#39;\u0026#39;) file_type = models.CharField(max_length=100, default=\u0026#39;\u0026#39;) created_date = models.DateTimeField(default=timezone.now) file_path = models.ImageField(default=\u0026#39;\u0026#39;, upload_to=\u0026#39;invoice/image/\u0026#39;) Campos:\n id_file \u0026ndash;\u0026gt; ID único que se informa al grabar file_name \u0026ndash;\u0026gt; nombre del fichero que viene en el servicio file_type \u0026ndash;\u0026gt; Es el content-type del fichero del servicio created_date \u0026ndash;\u0026gt; Fecha de creación del fichero file_path \u0026ndash;\u0026gt; URL donde estará la imagen. Para que funcione la grabación de la imagen el tipo de fichero debe ser ImageField. Hay otro que se llama FileField, que funciona lo mismo(eso creo) pero con ficheros de todo tipo. En este tipo de fichero es posible añadir el path (que se añade al indicado de la configuración) donde se guarda el fichero. Esto permite clasificar las imagenes por directorios.  Serializers\r#\r\rPara mi es el controlador, es que recibe los datos del modelo y los devuelve en formato JSON a la vista. El código que le he puesto en el fichero serializers.py es el siguiente:\nfrom rest_framework import serializers from . import models class invoiceImageSerializer(serializers.ModelSerializer): class Meta: model = models.invoiceImage fields = (\u0026#39;id_file\u0026#39;, \u0026#39;file_name\u0026#39;, \u0026#39;created_date\u0026#39;, \u0026#39;file_path\u0026#39;, \u0026#39;file_type\u0026#39;) Es muy sencillo en la variable model se indica el modelo definido en el paso anterior. Y en fields los campos que queremos que vea la vista.\nViews\r#\r\rAquí esta la parte más importante.\nfrom rest_framework import parsers from rest_framework import response from rest_framework import status from rest_framework import viewsets from django.http import Http404 import os from . import models from . import serializers class invoiceImageViewSet(viewsets.ModelViewSet): queryset = models.invoiceImage.objects.all() serializer_class = serializers.invoiceImageSerializer parser_classes = [parsers.MultiPartParser] \u0026#34;\u0026#34;\u0026#34; Este método se llamada al hacer la llamado POST \u0026#34;\u0026#34;\u0026#34; def create(self, request): # Se recupera el objeto fichero que esta parseado por django rest framework file_obj = request.data[\u0026#39;content-file\u0026#39;] # Se inicializa el serializer pasando los valores del fichero: nombre, tipo y el propio objeto. serializer = self.serializer_class( data={\u0026#39;file_name\u0026#39;: request.data[\u0026#39;content-file\u0026#39;].name, \u0026#39;file_path\u0026#39;: file_obj, \u0026#39;file_type\u0026#39;: request.data[\u0026#39;content-file\u0026#39;].content_type}) # Si los datos son correctos se graba el modelo, donde se autoinformará los campos automáticos como el: id, fecha de creación, etc. if serializer.is_valid(): serializer.save() return response.Response(serializer.data, status=status.HTTP_201_CREATED) return response.Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) Cosas a tener en cuenta:\n Las variables queryset y serializer_class se ponen siempre. Es para que el resto de método de consulta funcionen La variable parser_classes hay que poner la que se indica. En los ejemplos de DRF aparece la FileUploadParser pero a mi no me funciona. Tal como se explica en las equivalencias HTTP el método que se sobrecarga el create. En el apartado de testeo se verá pero la variable donde viaja el fichero se llama content-file. Iba usar la file pero el Postman me da un warning y el fichero no viajaba. Una vez grabado los datos en serializer.data se pueden acceder a todos los campos incluídos los que se autorellenan.  URLs\r#\r\rEn el archivo urls.py de la aplicación hay que registrar la nueva vista:\nfrom rest_framework import routers from . import views router = routers.DefaultRouter() router.register(r\u0026#39;invoiceImage\u0026#39;, views.invoiceImageViewSet) Testeo\r#\r\rLas pruebas se han hecho con el postman y esta es la llamada:\n\rEl resultado:\n{\r\u0026quot;id_file\u0026quot;: \u0026quot;8fa3b1bc-975d-472a-b0ae-7f028fc26769\u0026quot;,\r\u0026quot;file_name\u0026quot;: \u0026quot;DSC05753.JPG\u0026quot;,\r\u0026quot;created_date\u0026quot;: \u0026quot;2020-08-22T18:39:55.165869+02:00\u0026quot;,\r\u0026quot;file_path\u0026quot;: \u0026quot;/media/invoice/image/DSC05753_apEwEM9.JPG\u0026quot;,\r\u0026quot;file_type\u0026quot;: \u0026quot;image/jpeg\u0026quot;\r}\rEjemplo de borrado del fichero\r#\r\rSiguiendo el ejemplo anterior vamos añadir como procesar el borrado para que nos haga el borrado físico de la imagen, aparte del borrado en el modelo. Nos basaremos en el ejemplo anterior por lo que solo hay que modificar el fichero views.py para añadir el siguiente método\ndef destroy(self, request, pk=None): try: # Obtengo el objeto pasado image = self.get_object() # Se borra la imagen image.delete() # Finalmente se borra la imagen fisica os.remove(image.file_path.path) except Http404: # Si no existe se lanza exepción raise Http404 # Si no hay errores entonces se envia que ha ido bien el borrado return response.Response(status=status.HTTP_204_NO_CONTENT) Cosas a tener en cuenta:\n La equivalencia de método HTTP DELETE es la función destroy El self.get_object() nos recupera los datos del ID de la imagen pasada por parámetro. Si no existe se devuelve un error 404 Se hace primero el borrado en el modelo por si hay cualquier error o excepción. Y finalmente el borrado físico. En el atributo file_path.path tenemos el path absoluto donde esta la imagen. Si todo va bien se devuelve el código HTTP 204 que no hay contenido.  Para probarlo en Postman sería con la siguiente llamada:\n\r"});index.add({'id':106,'href':'/docs/python/machine_learning/set_datos_propios/','title':"Set de datos propios",'content':"Introducción\r#\r\rTodo sobre disponer de tu propio set de datos para usarla en IA.\nObjetivo\r#\r\rMuchos ejemplos usan set de datos que ya vienen incluidos en la librería de Sklearn. En esta sección se hablará de como usar datos propios.\nSecciones\r#\r\rLas secciones son las siguientes:\n\r\rSubir datos propios\r\r\rSubir datos propios\r\r\r"});index.add({'id':107,'href':'/docs/python/sentencias/sistema/','title':"Sistema",'content':"Introducción\r#\r\rLa librería os hay una serie de funciones de sistema que se pueden utilizar. Aquí se recopilan las que se van usando. Esta librería para utilizar hay que hacer lo siguiente al inicio del programa:\nimport os Logs\r#\r\rCuando se usán librerías de machine learning como Tensor Flow en programa de Python, si se usa Jupyter Notebooks se muestran una cantidad de logs, cientos de líneas, que desvirtuan lo que realmente se quiere mostrar.\nPara eso se puede usar la siguiente sentencia:\nos.environ[\u0026#39;TF_CPP_MIN_LOG_LEVEL\u0026#39;] = \u0026#39;3\u0026#39; Esto reduce los logs que se producen al importar librerias o informativos del propio Python. Esto no elimina los mensajes que se generen a proposito en sentencias de las librerías, para eso la propia librería tendrá, o no, su propias sentencias.\nLa tabla con los valores posibles es la siguiente:\n   Nivel Nivel entendible Descripción del nivel     0 DEBUG [Default] Print all messages   1 INFO Filter out INFO messages   2 WARNING Filter out INFO \u0026amp; WARNING messages   3 ERROR Filter out all messages    Directorios\r#\r\rSentencia que trabajan con directorios\nValidar que exista un directorio\r#\r\rif os.path.exists(target_dir): Crear un directorio\r#\r\ros.mkdir(target_dir) "});index.add({'id':108,'href':'/docs/sap/abap/sentencias_74/string_templates/','title':"String template",'content':"Introducción\r#\r\rEn este grupo se irán poniendo las distintas sentencias que nos permiten procesar los string.\nLos string template son útiles porque simplifican el tratamiento de string o conversiones de campo.\nDe momento voy a poner en enlace a la ayuda de SAP porque tiene muy buenos ejemplo. Los iré poniendo aquí porque a veces cuesta encontrarlos.\nEstas sentencias se pueden utilizar para crear nuevas variables o ponerlas como entrada de parámetro en clases.\n"});index.add({'id':109,'href':'/docs/python/machine_learning/set_datos_propios/subir_datos_propios/','title':"Subir datos propios",'content':"Introducción\r#\r\rEjemplo extraído del video Machine Learning episodio 11. ¿Cómo subir tu set de datos propio?.\nEl ejemplo esta creado el Jupiter Notebool y se irá poniendo el código de las distintas celdas.\nExplicación\r#\r\rEn el video se muestra un excel con los datos iris con multidud de valores. Para este página se ha creado un excel sencillo con la misma estructura pero con datos limitados e inventados. Y se ha conviertido a CSV para que pueda importado por la librerías de machine learning. El fichero tiene ocho columnas de características y una columna para la etiqueta.\nCódigo\r#\r\r\rCódigo fuente descargable\n# Paquete para leer el fichero CSV import pandas as pd # Algoritmo de regresión línea from sklearn import linear_model # Separador de datos de test y entrenamiento from sklearn.model_selection import train_test_split # Algoritmo de regresión reg = linear_model.LogisticRegression() # Datos con el modelo archivo = \u0026#34;set_datos_propio.csv\u0026#34; # Lectura del modelo. # Hay que indicarle el separador para que funcione. df = pd.read_csv(archivo,\u0026#39;;\u0026#39;) # Visualización de los datos cargados df Este bloque no esta igual que el ejemplo orignal porque el método as_matrix ya no existe y se usa su equivalente.\n# Arreglox contendrá todas las características. # El -1 es para que no se lea la última columna # que es la etiqueta #print(\u0026#34;Valores: \u0026#34;, df.columns[0:-1]) arreglox = df[df.columns[:-1]].to_numpy() # Arregloy contendrá la ultima columna, que es la etiqueta arregloy = df[df.columns[-1]].to_numpy() print(\u0026#34;Características: \u0026#34;,arreglox) print(\u0026#34;Etiquetas: \u0026#34;, arregloy) # Separacion de datos de entrenamiento y test X_ent, X_test, y_ent, y_test = train_test_split(arreglox, arregloy) # Entrenamiento del algoritmo reg.fit(X_ent, y_ent) # Resultado del entrenamiento reg.score(X_test, y_test) "});index.add({'id':110,'href':'/docs/sap/abap/bopf/tablas/','title':"Tablas",'content':"Objetivo\r#\r\rRecopilación de tablas interesantes. Aquí solo habrán tablas \u0026ldquo;raras\u0026rdquo; e interesantes que pueden ser útiles en el día a día.\nPublicaciones\r#\r\r\r\rBasic Component\r\r\rBasic Component\r\r\rFinanzas\r\r\rFinanzas\r\r\rVentas\r\r\rVentas\r\r\r"});index.add({'id':111,'href':'/docs/python/machine_learning/tensorflow/','title':"Tensor Flow",'content':"Introducción\r#\r\rTodo sobre Tensor Flow.\nObjetivo\r#\r\rMuchos ejemplos usan set de datos que ya vienen incluidos en la librería de Sklearn. En esta sección se hablará de como usar datos propios.\nSecciones\r#\r\rLas secciones son las siguientes:\n\r\rEjemplo clasificador de imagenes\r\r\rEjemplo clasificador imagenes\r\r\rEjemplo transfer learning\r\r\rTransfer learning\r\r\rReconocimiento números escritos a mano\r\r\rReconocimiento números escritos a mano\r\r\rUsar dataset\r\r\rUsar datasets\r\r\rVariables y sesiones\r\r\rVariables y sesiones\r\r\r"});index.add({'id':112,'href':'/docs/python/machine_learning/redes_neuronales/tipos_redes_neuronales/','title':"Tipos de redes redes neuronales",'content':"Introducción\r#\r\rEjemplo extraído de Tipos de redes neuronales. Se pondrán imagens obtenidas del video para explicar mejor el funcionamiento tal como lo hace en el video.\nEn el video habla de los siguientes tipos:\n Profundas, o DNN. Convolucionales, o CNN. Recurrente, o RNN.  Deep Neural Net(DNN)\r#\r\rEs una red versátil que permite procesas:\n Texto Imagenes pequeñas Datos númericos. Como el set de datos de Iris.  La estructura es la que más se ha visto en los ejemplos:\n\rEn esta estructura se reciben los datos en la capa de entrada se procesan en la capa oculta y se el resultada se ve en la capa de salida. Cada neurona de cada capa esta interconectada con las neuronas de las siguientes capas.\nEste tipo de red se le llama profundas porque dentro de las capas ocultan hay más subcapas.\nEl problema de este tipo de red es que debido a la multitud de conexiones que hay entre capas, hace que el cálculo sea muy pesado a medida que el tamaño de los datos de entrada sea más grande. Ejemplo una imagen de 300x300 pixeles equivale a 90.000 entradas. En el ejemplo de Tensor Flow de reconomiento de números escritos a mano, donde se explica la versión que funciona, hay un ejemplo del número de parámetros que necesita la red para procesar los datos.\nPara el este tipo de tareas, como el de procesar imagenes, se ha ido la red CNN.\nConvolutional Neural Net(CNN)\r#\r\rEsta red aunque también se uas para procesar textos su punto fuerte, y donde más se usa, es para el procesamiento de imagenes. La estructura de red es parecida a la anterior solo cambia como procesa las capas ocultas:\n\rLas capas ocultas son las convoluciones y maxpooling. A medida que va procesando la imagenes en las distintas capas va reduciendo el tamaño de la imagen, pooling, y a través del convoluciones para detectando o identificando los valores más importantes. Cada capa que se va procesando va teniendo un nivel de abstracción más elaborado a medida que se acerca a la capa de salida.\nEjemplo, si se hace un clasificador de imagenes de coches. En la primera capa va identificar líneas(rectas, diagonales, etc..), en la siguiente capa va a encargarse de realizar figuras(cuadrados, circulos, rombos, etc.), en la siguiente capa va a identicar elementos más elaborados como llantas, puertas, etc.\nRecurrent Nerual Net(RNN)\r#\r\rSe usa para tipos de datos que suelen ser secuenciales. Ejemplo el precio de una acción, es un valor sencuencial cuyo valor depende de los valores que haya tenido en días pasados. Este tipo de red se usa mucho para texto ya que al final texto es secuencial. La estructura es la siguiente:\n\rLas capas ocultas también se llaman LSTM(Long Short Trem Memory/Memoria corta a largo plazo). Este tipo de capas reciben una información, hacen una predicción y la salida sirve para volver a alimentar la capa oculta. De esta manera la red sabe lo que sucedio antes.\nEjemplo del video: El problema es Hola cómo estas . A la capa inicial se le pasa el hola:\n\rEl capa oculta predice que la siguiente palabra será cómo. Que vuelve a pasarsela como parámetro de entrada de nuevo a la capa oculta para la siguiente predicción. La capa oculta detecta que ahora me envías la palabra cómo pero antes me has pasado la palabra hola, por la tanto determina que la siguiente palabra tiene que ser estas\n\rDe nuevo la salida de la capa vuelve a ser la entrada de la capa oculta. Con lo cual el sabrá que las palabras anteriores han sido hola y como y puede determinar que seá \n"});index.add({'id':113,'href':'/docs/sap/abap/transacciones/','title':"Transacciones",'content':"Objetivo\r#\r\rRecopilación de las transacciones que me parecen interesantes.\nTransacciones\r#\r\rSAP R/3 o ECC\rGenerales    Transacción Descripción     I18N Internacionalizacion. Permite configurar el smartforms para usar word (a partir del EHP4 lo hace este report: RSCPSETEDITOR)   CG3Y Descarga de ficheros del servidor a local   CG3Z Subido de fichero al servidor desde local   SM01 Bloquea transacciones estándar   SLXT Guarda las traducciones hechas en la transacción SE63   DWDM Ejemplos controles de pantalla   BAPI Permite ver los objetos de negocio con sus respectiva funcionalidad. Y sobretodo que función realiza dicha funcionalidad.    Financieras    Transacción Descripción     GGB0 Transación global para validaciones   GGB1 Transación global para sustituciones   OV51 Documentos modificacion deudores    Monitarización SAP PO    Transacción Descripción     SXMB_MONI Monitor de mensajes XML. Lo que viene de PI con ABAP Proxy    Comunicaciones    Transacción Descripción     SMQ2 Colas qRFC   SCOT Configuración de los canales de comunicación: mail, fax, etc.   SOST Transacción que gestiona la salida de los canales de comunicación configurado: mail, fax, etc.   SOSG Como la SOST pero solo de tu usuario    IDOC    Transacción Descripción     WEDI Menu ambito de IDOCS    Recursos humanos    Transacción Descripción     PU00 Borrar empleado de manera individual (programa RPUDELPN para masivos)   PC_PAYRESULT Ve los resultados de nomina de un empleado    Trace    Transacción Descripción     ST12 Hibrido entre la SAT y ST05 para temas de rendimiento ya sea web, programa, etc..    Para SAP BW    Transacción Descripción     RSO2 Creacion extractores para BW   RSRT Ejecutar cubos desde R/3. Si necesitas de usar las herramientas externas    SAP BW\rGenerales    Transacción Descripción     RSINPUT Permite rellenar registros en un ODS de BW    \r"});index.add({'id':114,'href':'/docs/sap/ui5/','title':"UI5",'content':"Objetivo\r#\r\rUI5 es un framework de desarrolo de aplicaciones web hecho por SAP y que combina las técnologías de HTML5+JavaScript.\nUI5 tiene dos sabores, una manera de llamarlo: SAP UI5 que son el conjunto de librerias propietarias y Open UI5 que es lo mismo pero en versión open source. La diferencia entre ambas son las librerías gráficas que solo están en SAP UI5 y que las principales novedades llegan primero a SAP UI5.\nYo suelo usar Open UI5 porque para mis aprendizajes y por el tipo de librerías que uso me es más que suficiente. En UI5 no trabajo solo he realizado pequeñas cosas para aprender como funciona.\nPublicaciones\r#\r\r\r\rDesarrollo con VS Code\r\r\rDesarrollar aplicaciones con Visual Studio Code\r\r\r"});index.add({'id':115,'href':'/docs/python/machine_learning/tensorflow/usar_datasets/','title':"Usar dataset",'content':"Introducción\r#\r\rEste articulo lo he tenido que hacer para aprender la base de como funciona los dataset en Tensor Flow, porque si no hay manera de entender los ejemplos más complejos. O ejemplos creados en versión 1.x de Tensor Flow que al usarlos en la versión 2.0 hay que migrarlos.\nLibreías de cada ejemplo\r#\r\rLas librérías que uso en cada ejemplo son las sigueintes:\n# Libreria tensor flowe import tensorflow as tf # Daset de datos con números escritos a manorom from tensorflow.examples.tutorials.mnist import input_data import tensorflow_datasets as tfds # API de tensorfow para construir y entrenar modelos de alto nivel from tensorflow import keras Mecánica básica\r#\r\r\rCódigo fuente descargable\n# Creamos una set de datos que es un array de seis elementos dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1]) # Para listar los ejemplos del dataset hay que # recorrelos con un FOR. # Cada elemento del dataset es de tipo numpy. Por eso hay que usar # \u0026#34;numpy\u0026#34; para obtener el valor. for elem in dataset: print(elem.numpy()) Resultado:\n8\r3\r0\r8\r2\r1\r# Ahora vamos crear un dataset de tipo matriz. # Que tendrá 4 filas y 10 columnas dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10])) # Para ver los elementos: for elem in dataset1: print(\u0026#34;Fila: \u0026#34;,elem.numpy()) Resultado:\nFila: [0.58600104 0.80292606 0.13899899 0.6027132 0.2812158 0.06768107\r0.01700175 0.4155041 0.42036211 0.94815934]\rFila: [0.09541941 0.7888255 0.5253123 0.960852 0.962497 0.5518899\r0.16105258 0.98514235 0.82917464 0.5447879 ]\rFila: [0.30786347 0.2631991 0.83314383 0.4485308 0.992118 0.17097425\r0.22812164 0.20294225 0.95297885 0.30839372]\rFila: [0.7794962 0.05844533 0.678041 0.50282586 0.60348916 0.94892263\r0.3606732 0.57894313 0.08367121 0.97359645]\r# El siguiente ejemplo es lo más parecido al dataset de minst. # Ya que tiene dos niveles uno como un array de 4 posiciones y otro una matriz de 4 filas y 100 posiciones dataset2 = tf.data.Dataset.from_tensor_slices( (tf.random.uniform([4]), tf.random.uniform([4, 100], maxval=100, dtype=tf.int32))) # Se recorre el dataset creado, como tiene dos elementos en el FOR # se recupera ambos valores. for elem1, elem2 in dataset2: print(elem2.numpy()) Resultado\n[84 89 29 72 34 83 85 67 81 48 41 10 61 91 17 83 41 69 68 82 74 96 1 61\r96 55 41 20 57 89 96 78 75 62 66 18 2 80 32 11 95 88 69 97 36 84 78 83\r68 59 11 72 29 89 23 53 78 91 51 24 37 59 32 18 57 74 50 42 76 0 55 24\r50 45 70 85 1 56 32 70 15 50 36 23 68 22 93 52 54 86 91 62 93 21 83 77\r16 61 83 29]\r[48 76 66 53 96 95 78 40 25 11 64 83 89 15 72 15 63 46 37 44 50 2 26 92\r7 79 73 49 34 53 35 81 85 66 55 23 52 2 78 52 55 23 2 87 86 0 4 29\r50 4 6 80 75 86 98 96 1 61 65 4 20 84 61 94 6 35 94 84 46 13 45 0\r51 34 50 18 56 98 81 16 51 20 69 97 65 25 40 31 59 90 52 94 58 65 79 45\r95 2 25 28]\r[36 71 19 46 22 71 36 75 53 60 47 90 82 23 70 14 41 57 53 69 69 59 76 39\r75 78 21 83 80 55 16 25 96 1 74 13 74 27 42 8 8 59 52 94 76 98 74 53\r80 61 19 76 7 42 77 78 61 52 51 39 67 49 84 34 30 9 73 56 57 11 70 32\r33 81 60 99 54 52 18 28 71 60 65 46 87 38 62 0 14 98 37 26 32 0 15 50\r16 19 82 58]\r[63 54 20 24 36 54 90 14 89 89 13 38 44 73 49 22 78 47 72 89 1 92 38 78\r61 48 55 34 18 72 54 92 95 12 61 84 65 49 7 3 65 97 46 0 21 69 77 98\r94 12 91 16 21 67 60 46 83 43 12 55 45 78 73 53 42 91 37 88 86 46 92 67\r65 14 71 37 85 35 63 45 55 51 73 55 26 15 45 42 61 29 99 39 91 19 7 17\r74 95 6 88]\rResultado al mostrar elem1\n0.2338922\r0.64260435\r0.48011637\r0.24890351\rMecánica batch\r#\r\r\rCódigo fuente descargable\nEl proceso batch es muy útil cuando hay un gran volúmen de datos en el dataset. Esto permite ir leyendo los datos en bloques y procesandolos:\nTomamos como base el ejemplo anterior:\n# Creamos una set de datos que es un array de seis elementos dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1, 20]) # Los dataset se pueden procesar en lotes, batch. # En la siguiente convertimos nuestro dataset en batch # indicando que lotes sean de 3 en 3 dataset_batch = dataset.batch(3) # Se recorre los set de datos. El take indica cuantos # veces se van a leer datos for batch in dataset_batch.take(100): print([arr.numpy() for arr in batch]) Con el valor 100 del ejemplo el resultado es el siguiente:\n[8, 3, 0]\r[8, 2, 1]\r[20]\rSi pone el valor 1 al take el resultado es el siguiente\n[8, 3, 0]\rEn el siguiente se indica como se calculará el valor a pasar a la función taken para saber que valor hay que pasarle para recuperar todos los datos\n# Vamos a ver cuantos pasadas hay que hacer para leer los datos # Genera un array con los valores que se leen cada pasada. batch_sizes = [batch.shape[0] for batch in dataset_batch] batch_taken = len([batch.shape[0] for batch in dataset_batch]) print(\u0026#34;Número de datos que se leen en cada pasada:\u0026#34;, batch_sizes) print(\u0026#34;Número valores a pasar a taken: \u0026#34;, batch_taken) Resultado:\nNúmero de datos que se leen en cada pasada: [3, 3, 1]\rNúmero valores a pasar a taken: 3\rCuando se entrena algoritmo de redes neuronales se hacen varias pasadas para al algoritmo para mejorar su entrenamiento. Ese concepto se llama epochs, el esqueleto básico sería el siguiente ejemplo:\nfor epoch in range(3): print(\u0026#34;Inicio época: \u0026#34;, epoch) for batch in dataset_batch.take(100): print([arr.numpy() for arr in batch]) print(\u0026#34;Fin epoca: \u0026#34;, epoch) "});index.add({'id':116,'href':'/docs/python/sentencias/usar_librerias/','title':"Usar librerias",'content':"Usar librerias\r#\r\rImportar libreria se hace con la sentencia Import pero tiene variantes según sean librerias propias de Python o sencillas, eso creo, o librerias que están dentro de un paquete.\nLibrerías propias o sencillas\r#\r\rEjemplo de como usar la libreria matemática para calcular una raíz cuadrada:\nimport math print(math.sqrt(20)) Resultado\n4.47213595499958\rLibrerías dentro de paquetes\r#\r\rEl siguiente ejemplo se recupera la libreria de regresión de KNN:\n# Regresor de KNN o vecinos cercanos from sklearn.neighbors import KNeighborsRegressor Múltiple librerias de un paquete\r#\r\rEs posible recuperar varías librerías de un solo paquete:\nfrom sklearn.datasets import load_breast_cancer, load_iris Importar libreria asignándole un alias\r#\r\rEs posible importar una librería asignándole un alias, por ejemplo, para darle un nombre más sencillo:\nimport numpy as np "});index.add({'id':117,'href':'/docs/python/machine_learning/usar_gpu/','title':"Uso GPU",'content':"Introducción\r#\r\rEl objetivo es explicar que he hecho para poder hacer utilizar la GPUs de la tarjeta gráfica para las operaciones del machine learning. El motivo, es que una tarjeta gráfica tiene muchos más cpus que se pueden usar paralelamente para los calculos.\nSe va explicar como se ha usado para TensorFlow pero puede servir para otras librerías.\nNOTA IMPORTANTE: El sistema operativo es Windows\nTensor Flow\r#\r\rLo primero que hay que hacer es mirar la documentación oficial para averiguar que versión de librerías es necesaria.\nEn el momento que se hace esta página Tensor Flow es compatible con CUDA 10.1. Por lo tanto hay que seguir los siguientes pasos:\nInstalar CUDA\r#\r\rDescargar e instalar la versión de CUDA que sea compatible con TensorFlow. En la instalación solo he indicado que instale las librerías runtime. La integración con Visual Studio, ejemplos, etc.. no lo he seleccionado.\nUna vez instalado hay que añadir el path donde se ha instalado a la variable de entorno PATH del sistema. Aunque cuando se entre al ver las variables de entorno aparece el CUDA_PATH pero no funciona. Hay que añadir la misma ruta que esta en esa variable en la PATH. Y luego reiniciar el sistema.\nInstalar CUDnn\r#\r\rEs la librería que usa TensorFlow para poder usar el CUDA de la tarjeta gráfica. En esta página hay que descargar la versión asociada a la versión de CUDA. En la página ya lo indica. Hay que tener en cuenta que hay que registrarse para podersela descargar.\nSe descarga un fichero que hay que descomprimirlo donde uno quiera, pero luego hay que añadir la ruta+\\bin en el variable de entorno PATH. Ejemplo yo lo tengo instalado en K:\\cuda. Y la variable de entorno he puesto K:\\cuda\\bin. Un vez hecho hay que reiniciar el pc.\nCreación entorno en Anaconda\r#\r\rEn anaconda he creado un entorno nuevo llamado tensorflow_gpu donde la versión escogida de Python es la 3.8. Una vez creado he ido al Anaconda PowerShell para poder instalar los paquetes a través de PiP que es el instalador de paquetes de Python. El motivo de hacerlo de esta manera es que a través de anaconda he tenido muchos conflictos con las versiones de Python y librerias. Y mirando por internet he visto que esta solución me funciona ya que me instala paquetes más recientes que los que haría con Anaconda.\nDentro del powershel de Anaconda he hecho lo siguiente:\nconda activate tensorflow-gpu pip install tensorflow pip install tensorflow-gpu Estas dos librerías instalarán todas las librerías que necesiten.\nAunque se instale con PiP en Anaconda veremos lo que se ha instalado:\n\rPara desintalar habría que hacer:\npip uninstall tensorflow pip uninstall tensorflow-gpu pip uninstall protobuf Solución al error Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r#\r\rEste error aparece por un problema de memoria de las CPUs de NVIDIA. Para solucionar hay que poner las siguiente líneas después del import de la librería de TensorFlow y antes del código principal:\nphysical_devices = tf.config.experimental.list_physical_devices(\u0026#39;GPU\u0026#39;) assert len(physical_devices) \u0026gt; 0, \u0026#34;Not enough GPU hardware devices available\u0026#34; config = tf.config.experimental.set_memory_growth(physical_devices[0], True) "});index.add({'id':118,'href':'/docs/sap/abap/utilidades_generales/','title':"Utilidades generales",'content':"Objetivo\r#\r\rCajón de desastre de cosas que no se muy como clasificar.\nHacks\r#\r\rSaltarse seguridad como desarrollador\r#\r\rPara saltarse la seguridad del usuario desarrollador\n\rURL\r#\r\r http://wiki.sdn.sap.com/wiki/display/Basis/Timezone+changes+best+practices \u0026ndash;\u0026gt; Best practices sobre los husos horarios  Rellenar campos con 0\r#\r\rdata: l_char(50). TRANSLATE l_char USING \u0026#39; 0\u0026#39;. WRITE l_char. Truco para saber que BADIS se utilizan en un proceso\r#\r\rHay que ir a la transacción SE24 informar la clase CL_EXITHANDLER metodo e ir al contenido del método GET_INSTANCE. Hay que poner un break-point en la sentencia CALL METHOD CL_EXITHANDLER=\u0026gt;GET_CLASS_NAME_BY_INTERFACE. Con esto vas viendo cada una de las BADIs que se llaman mientras estas en la transacción, programa, etc.\nPoner sentencia SQL directamente en Produccion\r#\r\rIr al programa RSHOWTIM. Ir al form AUTHORITY_CHECK. Despues que se lance la funcion TR_SYS_PARAMS poner en la variable SYS_CLIINDDEP_LOCK espacio. Si hay cualquier otro problema saltarselo con debugging.\nSaltarse seguridad SE16N\r#\r\rDebido a nota Note 1420281 - CO-OM tools: SE16N: Deactivating \u0026amp;SAP_EDIT. La opción \u0026amp;SAP_EDIT de la SE16N está deshabilitada por SAP a partir de un Support Package determinado. Para poder hacerlo basta con poner un breakpoint en el include LSE16NI01 linea 36: CASE SAVE_OK_CODE. Justo despues del ENDCASE (linea 203) actualizamos las variables gd-edit y gd-sapedit a \u0026lsquo;X\u0026rsquo;.\nEjecutar un programa automáticamente en una orden de transporte\r#\r\rPoner entrada: R3TR XPRA RV80HGEN para que se ejecute un programa, RV80HGEN despues de transporte. El programa en cuestión es el que regenera las rutinas de SD. Este truco es útil para la regeneracion de rutinas de SD.\nUsar comandos de SAPScript en cualquier sitio\r#\r\rDATA ls_header TYPE thead. DATA ls_line TYPE tline. DATA lt_line TYPE TABLE OF tline. ls_header-tdspras = sy-langu. ls_header-tdlinesize = 72. ls_line-tdformat = \u0026#39;/:\u0026#39;. ls_line-tdline = \u0026#39;SET DATE MASK = \u0026#39;\u0026#39;DD/MM/YY\u0026#39;\u0026#39;\u0026#39;. append ls_line to lt_line. ls_line-tdformat = \u0026#39;*\u0026#39;. ls_line-tdline = \u0026#39;\u0026amp;SY-DATUM\u0026amp;\u0026#39;. append ls_line to lt_line. CALL FUNCTION \u0026#39;TEXT_SYMBOL_REPLACE\u0026#39; EXPORTING HEADER = ls_header TABLES LINES = lt_line. READ TABLE lt_line INTO ls_line INDEX 2. WRITE : / ls_line-tdline. Ver un formulario en formato PDF en su previsualización\r#\r\rCuando se esta previsualizando un spool se pone en la barra de comandos PDF! y se ve como si saliese en PDF.\nBúsqueda de fechas\r#\r\rResalto en negrita el título porque esta chuleta me ha ido muy bien para hacer búsqueda en tablas con campos de fecha inicio y fin.\n Registro válido para una fecha v_date: begda \u0026lt;= v_date and endda \u0026gt;= v_date Registro comprendido íntegramente en un periodo f_ini - f_fin: begda \u0026lt;= f_ini and endda \u0026gt;= f_fin Registro válido en algún momento del periodo f_ini - f_fin: begda \u0026lt;= f_fin and endda \u0026gt;= f_ini  Usar simbolos sapscript para reemplazar valores\r#\r\r Con esto se establece el valor (se puede llamar tantas veces como se quiera):  CALL FUNCTION \u0026#39;SET_TEXTSYMBOL\u0026#39; EXPORTING name = \u0026#39;\u0026amp;GV_VIA_PAGO\u0026amp;\u0026#39; value = ld_via_pago. Con esta funcion se hace el reemplazo:  CALL FUNCTION \u0026#39;REPLACE_TEXTSYMBOL\u0026#39; EXPORTING endline = 1 startline = 1 TABLES lines = lt_text. Capturar mensajes de error en funcion\r#\r\rHay que añadir la excepcion ERROR_MESSAGE con un numero inferior a la excepción OTHERS.\n"});index.add({'id':119,'href':'/docs/python/machine_learning/tensorflow/variables_sesiones/','title':"Variables y sesiones",'content':"Introducción\r#\r\rEjemplo extraído del video TensorFlow: Variables y Sesiones.\nEl ejemplo esta creado el Jupiter Notebool y se irá poniendo el código de las distintas celdas.\nNOTA: El código del video esta basado en Tensor Flow 1.0. Pero el Tensor Flow que se ha instalado es la version 2.x, la llamaremos TF2.0 o TF20 . Por lo tanto hay muchas cosas que no son compatibles. En el código habrá partes las principales que se indique que no se usa en TF20, pero el resto de partes se pondrá en código TF20 para no poner demasiada basura en el código\nCódigo\r#\r\r\rCódigo fuente descargable\n# Libería de tensor flow import tensorflow as tf # Estas dos líneas es para que funcione el placeholder #import tensorflow.compat.v1 as tf #tf.disable_v2_behavior() # Definición de una constante constante = tf.constant([2.0,3,4], dtype=tf.float32,name=\u0026#39;Constante1\u0026#39;) # Atributos de la constante print(constante) # Para que funcione el placeholder hay que cargar las librerías # del tensorflow de manera distinta # Placeholder es una variable que se crea de inicio vacia, pero # servira para ir rellenandola más adelante. Normalmente se # usan para el \u0026#34;input\u0026#34; de data # En este ejemplo se usa la versión que sustituye a la antigua #apartado = tf.placeholder(dtype=tf.float32, name=\u0026#39;Variable1\u0026#39;) # Version v1 apartado = tf.keras.Input(name=\u0026#39;Variable1\u0026#39;, shape=(), dtype=tf.float32) # Atributos del placeholder print(apartado) # Variables # Es un atributo que pueda cambiar de valor a lo largo del programa. variable = tf.Variable(3,dtype=tf.float32,name=\u0026#34;variable1\u0026#34;) # Atributos de variable print(variable) # Se genera un matriz con todo ceros que luego se puede manipular # Se indica que tendrá 3 filas y 4 columnas matriz = tf.zeros([3,4],tf.int32,name=\u0026#39;matriz\u0026#39;) # Atributo print(matriz) # Se inicializa las variables de nuestra aplicación # Esto con TF2.0 ya no es necesario #inicializar = tf.global_variables_initializer() # Inicializa la sesión de tensorflow. Con TF2.0 no es necesario #sess = tf.Session() # Lo que hace es informar nuestras variables en las sesion de TF # Este paso no es necesario usarlo en TF2.ProcessLookupError0 #sess.run(tf.global_variables_initializer()) # No hace usar sar sess.run para cualquier cosa en TF20 #print(sess.run(constante)) # Se hace la multiplicacion entre apartado y constante multiplicacion=apartado*constante # No habrá resultado porque apartado esta vacio print(\u0026#34;Valor de multiplicacion:\u0026#34;, multiplicacion) # Se rellena los valores de apartado apartado = [[15,10,5]] # Ahora si que devuelve datos. Cada columna se multipla por la constante # que es: 2.0, 3 y 4 * 15, 10 y 5. Queda como resultado 30, 30 y 20 print(\u0026#34;Valor de multiplicacion:\u0026#34;, multiplicacion) # Ejemplo de multiplicación de matrices necesita la forma (M,N)X(N,M)=M,M # Primera matriz: 2 filas y 2 columnas a = tf.keras.Input(shape=(2,2), dtype=tf.float32) # Segunda matriz: 2 filas y 3 columnas b = tf.keras.Input(shape=(2,3), dtype=tf.float32) # Datos de la primera matriz a = [[1,2],[2,2]] # Datos de la segunda matriz b = [[12,21,4],[3,2,4]] # Multiplicación de ambas matrices: (2,2) * (2,3) mult = tf.matmul(a,b) # Resultado print(\u0026#34;Resultado de mult: \u0026#34;, mult) # Ejemplo de producto . entre dos vectores # Primer vector: 3 columnas c = tf.keras.Input(shape=(3), dtype=tf.float32) # Segundo vector: 3 columnas d = tf.keras.Input(shape=(3), dtype=tf.float32) # Datos para los vectores # Primer vector c = [1,2,3] # Segundo vector d = [3,2,1] # Calculo del producto punto punto = tf.tensordot(c,d,1) print(\u0026#34;Resultado: \u0026#34;, punto) "});index.add({'id':120,'href':'/docs/python/machine_learning/vectorizacion/','title':"Vectorización",'content':"Introducción\r#\r\rLa vectorización es la que convertimos un grupo de textos a números. El motivo es que nuestros algoritmos procedan números, con lo que son más eficiente procesando una matriz o vector de números que a ún montón de palabras.\nTodo sobre la vectorización.\nSecciones\r#\r\rLas secciones son las siguientes:\n\r\rEjemplo de conversión de palabras a números\r\r\rEjemplo de conversión de palabras a números\r\r\r"});index.add({'id':121,'href':'/docs/sap/abap/bopf/tablas/ventas/','title':"Ventas",'content':"Objetivo\r#\r\rTablas de ventas\nLista\r#\r\rGenerales\r#\r\r   Función Descripción     VAPMA Tabla que contiene los datos de VBAK y VBAP pero con muchos menos campos. Y lo más importantes son claves.    "});index.add({'id':122,'href':'/docs/sap/abap/funciones/ventas/','title':"Ventas",'content':"Objetivo\r#\r\rFunciones del módulo de ventas(SD).\nAl final hay una sección de ejemplos para algunas funciones. No todas las funciones tienen ejemplo.\nLista\r#\r\r   Función Descripción     RV_DOCUMENT_FLOW Sacar el flujo de documentos en ventas   SD_OBJECT_TYPE_DETERMINE Se le pasa el tipo de documento de SD y devuelve el objeto workflow asociado.    Ejemplos\r#\r\rRV_DOCUMENT_FLOW\r#\r\rFORM ir_flujo USING pe_datos TYPE LINE OF zzc_sd49501=\u0026gt;ty_t_alv. DATA ls_vbco6 TYPE vbco6. MOVE-CORRESPONDING pe_datos TO ls_vbco6. CALL DIALOG \u0026#39;RV_DOCUMENT_FLOW\u0026#39; EXPORTING vbco6 FROM ls_vbco6 makt-maktx FROM space kna1-kunnr FROM pe_datos-kunnr kna1-name1 FROM pe_datos-txt_kunnr makt-matnr FROM space ivkorg FROM pe_datos-vkorg ivtweg FROM pe_datos-vtweg. ENDFORM. \u0026#34; IR_FLUJO "});index.add({'id':123,'href':'/docs/sap/ventas/','title':"Ventas",'content':"Objetivo\r#\r\rPoner información más funcional sobre el módulo de ventas\nPublicaciones\r#\r\r\r\rInformación general\r\r\rInformación general\r\r\r"});index.add({'id':124,'href':'/docs/python/sentencias/visualizar_datos/','title':"Visualizar datos",'content':"Visualizar datos\r#\r\rEn todos los ejemplos se hace uso de la sentencia print a la hora de visualizar los datos. En esta página se recopila usos que se hace de ella y otras posibles formar de visualizar en pantalla\nprint\r#\r\rEl print muestra los datos en la consola doned se esta ejecutando el programa. Aquí se recopilan formas de usarlo\nHacer operaciones\r#\r\rEs posible mostrar el resultado de una operación que se pone directamente en la sentencia:\nh=12 a=12 print(\u0026#34;Suma: \u0026#34;,a+h) Resultado\nSuma: 24\rMostrar contenido de variables\r#\r\rOpción más simple:\nn=20 print(\u0026#34;Valor de n: \u0026#34;,n) Resultado:\nValor de n: 10\rO también poner directamente el valor del variables:\nn=20 print(n) Resultado:\n10\rPero la opción que más me gusta por ser la más simple es estA::\nn=20 print(f\u0026#34;Valor de n: {n}\u0026#34;) Resultado:\nValor de n: 10\rTan solo hay que poner el carácter f delante de las comillas dobles o simples, al gusto, y poner entre los carácteres {} la variable.\n"});})();